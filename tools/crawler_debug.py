#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ë””ë²„ê¹… ë„êµ¬
íŠ¹ì • ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  í¬ë¡¤ë§ ë¬¸ì œë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤.
"""

import sys
import os
import requests
from bs4 import BeautifulSoup, Tag
from pathlib import Path
from typing import Optional, List, Any
from datetime import date
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from app.database import SessionLocal
from app.crud import upsert_update

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def analyze_site_structure(url: str):
    """ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    print(f"ğŸ” {url} ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ (í¬ê¸°: {len(response.text)} bytes)")
        
        # 1. ê¸°ë³¸ ì •ë³´
        print("\nğŸ“‹ ê¸°ë³¸ ì •ë³´:")
        print(f"   ì œëª©: {soup.title.string if soup.title else 'ì—†ìŒ'}")
        
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc and isinstance(meta_desc, Tag) and meta_desc.get('content'):
            print(f"   ë©”íƒ€ ì„¤ëª…: {meta_desc['content']}")
        else:
            print("   ë©”íƒ€ ì„¤ëª…: ì—†ìŒ")
        
        # 2. ë³¸ë¬¸ ê´€ë ¨ í´ë˜ìŠ¤/ID ê²€ìƒ‰
        print("\nğŸ” ë³¸ë¬¸ ê´€ë ¨ ìš”ì†Œ ê²€ìƒ‰:")
        content_selectors = [
            'article', 'main', '[role="main"]',
            '.content', '.post-content', '.entry-content', '.article-content',
            '.main-content', '#content', '#main', '.post', '.entry', '.article'
        ]
        
        found_elements = []
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                found_elements.append((selector, len(elements)))
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
        
        if not found_elements:
            print("   âŒ ì¼ë°˜ì ì¸ ë³¸ë¬¸ ì„ íƒìì—ì„œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. ëª¨ë“  í´ë˜ìŠ¤ ë¶„ì„
        print("\nğŸ·ï¸  í˜ì´ì§€ì˜ ëª¨ë“  í´ë˜ìŠ¤ ë¶„ì„:")
        all_classes = set()
        for tag in soup.find_all(class_=True):
            if isinstance(tag, Tag):
                class_attr = tag.get('class')
                if class_attr:
                    if isinstance(class_attr, list):
                        all_classes.update(class_attr)
                    else:
                        all_classes.add(str(class_attr))
        
        content_related_classes = []
        for class_name in sorted(all_classes):
            if any(keyword in class_name.lower() for keyword in ['content', 'post', 'entry', 'article', 'main', 'body']):
                content_related_classes.append(class_name)
                print(f"   ğŸ“ {class_name}")
        
        # 4. í…ìŠ¤íŠ¸ ë¸”ë¡ ë¶„ì„
        print("\nğŸ“„ í…ìŠ¤íŠ¸ ë¸”ë¡ ë¶„ì„:")
        text_blocks = []
        for tag in soup.find_all(['p', 'div', 'article', 'section']):
            if isinstance(tag, Tag):
                text = tag.get_text(strip=True)
                if len(text) > 100:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ë¸”ë¡
                    text_blocks.append((tag.name, len(text), text[:100] + "..."))
        
        text_blocks.sort(key=lambda x: x[1], reverse=True)
        for i, (tag_name, length, preview) in enumerate(text_blocks[:5]):
            print(f"   {i+1}. <{tag_name}> ({length}ì): {preview}")
        
        # 5. ì¶”ì²œ ì„ íƒì ìƒì„±
        print("\nğŸ’¡ ì¶”ì²œ ì„ íƒì:")
        if content_related_classes:
            print("   ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í…€ ì„ íƒì:")
            for class_name in content_related_classes[:5]:
                print(f"     '.{class_name}'")
        
        # 6. ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸:")
        test_selectors = content_related_classes[:3] + ['article', 'main', '.content']
        
        for selector in test_selectors:
            try:
                if selector.startswith('.'):
                    elements = soup.select(selector)
                else:
                    elements = soup.find_all(selector)
                
                if elements:
                    total_text = ""
                    for element in elements:
                        if isinstance(element, Tag):
                            total_text += element.get_text(strip=True) + " "
                    
                    if len(total_text.strip()) > 500:
                        print(f"   âœ… {selector}: {len(total_text)}ì í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                        print(f"      ë¯¸ë¦¬ë³´ê¸°: {total_text[:200]}...")
                        break
                    else:
                        print(f"   âš ï¸  {selector}: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(total_text)}ì)")
                else:
                    print(f"   âŒ {selector}: ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            except Exception as e:
                print(f"   âŒ {selector}: ì˜¤ë¥˜ ë°œìƒ - {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def generate_custom_crawler(url: str):
    """ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í…€ í¬ë¡¤ëŸ¬ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"\nğŸ”§ {url} ì‚¬ì´íŠ¸ìš© ì»¤ìŠ¤í…€ í¬ë¡¤ëŸ¬ ìƒì„±...")
    
    domain = url.split('/')[2]
    
    custom_code = f'''
# {domain} ì‚¬ì´íŠ¸ìš© ì»¤ìŠ¤í…€ í¬ë¡¤ëŸ¬
def crawl_{domain.replace('.', '_').replace('-', '_')}(soup):
    """{domain} ì‚¬ì´íŠ¸ ì „ìš© í¬ë¡¤ë§ í•¨ìˆ˜"""
    main_content = None
    
    # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” ì„ íƒìë“¤
    selectors = [
        # ì—¬ê¸°ì— ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì„ íƒìë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”
    ]
    
    for selector in selectors:
        main_content = soup.select_one(selector)
        if main_content:
            logger.info(f"{domain} ì‚¬ì´íŠ¸ ë³¸ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {{selector}}")
            break
    
    if not main_content:
        # í´ë°±: ì¼ë°˜ì ì¸ ì„ íƒìë“¤
        fallback_selectors = ['article', 'main', '.content', '.post-content']
        for selector in fallback_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
    
    if not main_content:
        main_content = soup.body
    
    return main_content
'''
    
    print("ìƒì„±ëœ ì½”ë“œ:")
    print(custom_code)
    
    # íŒŒì¼ë¡œ ì €ì¥
    output_file = f"custom_crawlers/{domain.replace('.', '_').replace('-', '_')}.py"
    os.makedirs("custom_crawlers", exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(custom_code)
    
    print(f"\nğŸ’¾ ì»¤ìŠ¤í…€ í¬ë¡¤ëŸ¬ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main_debug():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python tools/crawler_debug.py <URL>")
        print("ì˜ˆì‹œ: python tools/crawler_debug.py https://example.com/article")
        return
    
    url = sys.argv[1]
    
    print("=" * 60)
    print("ğŸ”§ í¬ë¡¤ë§ ë””ë²„ê¹… ë„êµ¬")
    print("=" * 60)
    
    if analyze_site_structure(url):
        generate_custom_crawler(url)
        print("\nâœ… ë¶„ì„ ì™„ë£Œ! ìœ„ì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬ë¡¤ëŸ¬ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
    else:
        print("\nâŒ ë¶„ì„ ì‹¤íŒ¨. URLì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

def merge_feature_updates():
    db = SessionLocal()
    # (1) ë‚ ì§œ ì´ë™
    move_map = {
        "2024-06-01": "2025-06-24",
        "2024-05-01": "2025-06-23",
        "2024-04-01": "2025-06-22",
    }
    # (2) ë³‘í•© ëŒ€ìƒ(2024-03-01 ì´í•˜)
    merge_target_dates = [
        "2024-03-01", "2024-02-01", "2024-01-01", "2023-12-01"
    ]
    # (2-1) ë³‘í•© ë‚´ìš© ìˆ˜ì§‘
    from app.models import FeatureUpdate
    merged_content = ""
    for d in merge_target_dates:
        d_obj = date.fromisoformat(d)
        obj = db.query(FeatureUpdate).filter(FeatureUpdate.date == d_obj).first()
        if obj:
            merged_content += f"[{d}]\n" + obj.content + "\n\n"
    # (2-2) ë³‘í•© ë“±ë¡ (2025-06-22)
    if merged_content:
        upsert_update(db, date.fromisoformat("2025-06-22"), merged_content.strip())
    # (1-2) ë‚ ì§œ ì´ë™(upsert)
    for old, new in move_map.items():
        obj = db.query(FeatureUpdate).filter(FeatureUpdate.date == date.fromisoformat(old)).first()
        if obj:
            upsert_update(db, date.fromisoformat(new), obj.content)
    db.commit()
    db.close()
    print("ë‚ ì§œ ì´ë™ ë° ë³‘í•© ì™„ë£Œ!")

if __name__ == "__main__":
    merge_feature_updates() 