#!/usr/bin/env python3
"""
AI SEO Blogger ì‹œìŠ¤í…œ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
ë¶ˆí•„ìš”í•œ íŒŒì¼ë“¤ì„ ì •ë¦¬í•˜ê³  ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""

import os
import shutil
import glob
import json
from pathlib import Path
from datetime import datetime, timedelta

def cleanup_pycache():
    """Python ìºì‹œ íŒŒì¼ ì •ë¦¬"""
    print("ğŸ§¹ Python ìºì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    # __pycache__ ë””ë ‰í† ë¦¬ ì‚­ì œ
    for root, dirs, files in os.walk('.'):
        for dir_name in dirs:
            if dir_name == '__pycache__':
                cache_path = os.path.join(root, dir_name)
                try:
                    shutil.rmtree(cache_path)
                    print(f"  ì‚­ì œë¨: {cache_path}")
                except Exception as e:
                    print(f"  ì˜¤ë¥˜: {cache_path} - {e}")
    
    # .pyc íŒŒì¼ ì‚­ì œ
    for pyc_file in glob.glob('**/*.pyc', recursive=True):
        try:
            os.remove(pyc_file)
            print(f"  ì‚­ì œë¨: {pyc_file}")
        except Exception as e:
            print(f"  ì˜¤ë¥˜: {pyc_file} - {e}")

def cleanup_test_files():
    """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬"""
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    # í…ŒìŠ¤íŠ¸ JSON íŒŒì¼ íŒ¨í„´
    test_patterns = [
        '*test*.json',
        '*performance*.json',
        '*optimization*.json',
        '*stress*.json',
        '*complete*.json',
        '*gemini*.json',
        '*seo*.json',
        '*system*.json',
        '*crawler*.json',
        '*content*.json',
        '*enhanced*.json',
        '*geo*.json',
        '*report*.json',
        '*results*.json'
    ]
    
    for pattern in test_patterns:
        for file_path in glob.glob(pattern):
            try:
                os.remove(file_path)
                print(f"  ì‚­ì œë¨: {file_path}")
            except Exception as e:
                print(f"  ì˜¤ë¥˜: {file_path} - {e}")

def cleanup_debug_files():
    """ë””ë²„ê·¸ íŒŒì¼ ì •ë¦¬"""
    print("ğŸ› ë””ë²„ê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    debug_patterns = [
        'debug_*.html',
        'debug_*.json',
        'debug_*.txt',
        'test_*.html',
        'test_*.txt'
    ]
    
    for pattern in debug_patterns:
        for file_path in glob.glob(pattern):
            try:
                os.remove(file_path)
                print(f"  ì‚­ì œë¨: {file_path}")
            except Exception as e:
                print(f"  ì˜¤ë¥˜: {file_path} - {e}")

def cleanup_temp_files():
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    print("ğŸ“ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    temp_patterns = [
        '*.tmp',
        '*.bak',
        '*.backup',
        '*.swp',
        '*.swo',
        '*~',
        '.DS_Store'
    ]
    
    for pattern in temp_patterns:
        for file_path in glob.glob(pattern):
            try:
                os.remove(file_path)
                print(f"  ì‚­ì œë¨: {file_path}")
            except Exception as e:
                print(f"  ì˜¤ë¥˜: {file_path} - {e}")

def cleanup_old_logs():
    """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
    print("ğŸ“ ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    logs_dir = Path('logs')
    if logs_dir.exists():
        # 7ì¼ ì´ìƒ ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for log_file in logs_dir.glob('*.log'):
            if log_file.stat().st_mtime < cutoff_date.timestamp():
                try:
                    log_file.unlink()
                    print(f"  ì‚­ì œë¨: {log_file}")
                except Exception as e:
                    print(f"  ì˜¤ë¥˜: {log_file} - {e}")

def optimize_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"""
    print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì¤‘...")
    
    import sqlite3
    
    db_files = ['app/blog.db', 'app/news_archive.db']
    
    for db_file in db_files:
        if os.path.exists(db_file):
            try:
                conn = sqlite3.connect(db_file)
                cursor = conn.cursor()
                
                # VACUUMìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
                cursor.execute("VACUUM")
                
                # ì¸ë±ìŠ¤ ì¬êµ¬ì„±
                cursor.execute("REINDEX")
                
                conn.close()
                print(f"  ìµœì í™”ë¨: {db_file}")
            except Exception as e:
                print(f"  ì˜¤ë¥˜: {db_file} - {e}")

def create_optimization_report():
    """ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
    print("ğŸ“Š ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
    total_size = 0
    file_count = 0
    
    for root, dirs, files in os.walk('.'):
        for file in files:
            file_path = os.path.join(root, file)
            try:
                total_size += os.path.getsize(file_path)
                file_count += 1
            except:
                pass
    
    report = {
        'optimization_date': datetime.now().isoformat(),
        'total_size_mb': round(total_size / (1024 * 1024), 2),
        'file_count': file_count,
        'optimization_actions': [
            'Python ìºì‹œ íŒŒì¼ ì •ë¦¬',
            'í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬',
            'ë””ë²„ê·¸ íŒŒì¼ ì •ë¦¬',
            'ì„ì‹œ íŒŒì¼ ì •ë¦¬',
            'ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬',
            'ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”'
        ]
    }
    
    with open('system_optimization_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"  ë¦¬í¬íŠ¸ ìƒì„±ë¨: system_optimization_report.json")
    print(f"  ì´ í¬ê¸°: {report['total_size_mb']}MB")
    print(f"  íŒŒì¼ ê°œìˆ˜: {report['file_count']}ê°œ")

def main():
    """ë©”ì¸ ìµœì í™” í•¨ìˆ˜"""
    print("ğŸš€ AI SEO Blogger ì‹œìŠ¤í…œ ìµœì í™” ì‹œì‘")
    print("=" * 50)
    
    # ìµœì í™” ì „ ìƒíƒœ
    print("ğŸ“ˆ ìµœì í™” ì „ ìƒíƒœ:")
    total_size_before = sum(
        os.path.getsize(os.path.join(root, file))
        for root, dirs, files in os.walk('.')
        for file in files
        if os.path.isfile(os.path.join(root, file))
    )
    print(f"  ì´ í¬ê¸°: {round(total_size_before / (1024 * 1024), 2)}MB")
    
    # ìµœì í™” ì‹¤í–‰
    cleanup_pycache()
    cleanup_test_files()
    cleanup_debug_files()
    cleanup_temp_files()
    cleanup_old_logs()
    optimize_database()
    
    # ìµœì í™” í›„ ìƒíƒœ
    print("\nğŸ“‰ ìµœì í™” í›„ ìƒíƒœ:")
    total_size_after = sum(
        os.path.getsize(os.path.join(root, file))
        for root, dirs, files in os.walk('.')
        for file in files
        if os.path.isfile(os.path.join(root, file))
    )
    print(f"  ì´ í¬ê¸°: {round(total_size_after / (1024 * 1024), 2)}MB")
    
    # ì ˆì•½ëœ ê³µê°„ ê³„ì‚°
    saved_space = total_size_before - total_size_after
    saved_percentage = (saved_space / total_size_before) * 100 if total_size_before > 0 else 0
    
    print(f"  ì ˆì•½ëœ ê³µê°„: {round(saved_space / (1024 * 1024), 2)}MB ({saved_percentage:.1f}%)")
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    create_optimization_report()
    
    print("\nâœ… ì‹œìŠ¤í…œ ìµœì í™” ì™„ë£Œ!")

if __name__ == "__main__":
    main() 