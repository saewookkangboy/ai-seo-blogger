#!/usr/bin/env python3
"""
í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import time
import requests
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_crawler_direct():
    """í¬ë¡¤ëŸ¬ë¥¼ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from app.services.crawler import crawl_url, get_text_from_url
        import asyncio
        
        # í…ŒìŠ¤íŠ¸ URLë“¤
        test_urls = [
            "https://www.example.com/",
            "https://httpbin.org/html",
            "https://jsonplaceholder.typicode.com/posts/1"
        ]
        
        print("\n1. ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸:")
        for url in test_urls:
            print(f"\n   í…ŒìŠ¤íŠ¸ URL: {url}")
            start_time = time.time()
            
            try:
                content = crawl_url(url, use_google_style=True)
                end_time = time.time()
                
                if content:
                    print(f"   âœ… ì„±ê³µ: {len(content)}ì ì¶”ì¶œ")
                    print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
                    print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:100]}...")
                else:
                    print(f"   âŒ ì‹¤íŒ¨: ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
        
        print("\n2. ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸:")
        async def test_async_crawler():
            for url in test_urls:
                print(f"\n   í…ŒìŠ¤íŠ¸ URL: {url}")
                start_time = time.time()
                
                try:
                    content = await get_text_from_url(url)
                    end_time = time.time()
                    
                    if content:
                        print(f"   âœ… ì„±ê³µ: {len(content)}ì ì¶”ì¶œ")
                        print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
                        print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:100]}...")
                    else:
                        print(f"   âŒ ì‹¤íŒ¨: ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âŒ ì˜¤ë¥˜: {e}")
        
        # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(test_async_crawler())
        
        return True
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return False

def test_crawler_with_requests():
    """requestsë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ê¸°ë³¸ requests í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
    
    test_urls = [
        "https://www.example.com/",
        "https://httpbin.org/html",
        "https://jsonplaceholder.typicode.com/posts/1"
    ]
    
    for url in test_urls:
        print(f"\n   í…ŒìŠ¤íŠ¸ URL: {url}")
        start_time = time.time()
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # body í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if soup.body:
                content = soup.body.get_text(strip=True)
                end_time = time.time()
                
                print(f"   âœ… ì„±ê³µ: {len(content)}ì ì¶”ì¶œ")
                print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
                print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:100]}...")
            else:
                print(f"   âŒ ì‹¤íŒ¨: body íƒœê·¸ ì—†ìŒ")
                
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
    
    return True

def test_api_endpoint():
    """API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸...")
    
    test_data = {
        "url": "https://www.example.com/",
        "ai_mode": "gemini_2_0_flash",
        "content_length": "1000"
    }
    
    try:
        print(f"   í…ŒìŠ¤íŠ¸ URL: {test_data['url']}")
        start_time = time.time()
        
        response = requests.post(
            "http://localhost:8000/api/v1/generate-post-gemini-2-flash",
            json=test_data,
            timeout=60
        )
        
        end_time = time.time()
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                print(f"   âœ… API í˜¸ì¶œ ì„±ê³µ")
                print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
                
                # í¬ë¡¤ë§ ê²°ê³¼ í™•ì¸
                if 'crawled_content' in data:
                    content = data['crawled_content']
                    print(f"   ğŸ“ í¬ë¡¤ë§ ê²°ê³¼: {len(content)}ì")
                    print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:100]}...")
                else:
                    print(f"   âš ï¸  í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")
                    
            else:
                print(f"   âŒ API ì‘ë‹µ ì‹¤íŒ¨: {data.get('error', 'Unknown error')}")
        else:
            print(f"   âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
            print(f"   ì‘ë‹µ: {response.text}")
            
    except Exception as e:
        print(f"   âŒ API í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
    
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì¢…í•© í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("="*60)
    
    # 1. ê¸°ë³¸ requests í…ŒìŠ¤íŠ¸
    test_crawler_with_requests()
    
    # 2. ì§ì ‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    test_crawler_direct()
    
    # 3. API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
    test_api_endpoint()
    
    print("\n" + "="*60)
    print("âœ… í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*60)

if __name__ == "__main__":
    main() 