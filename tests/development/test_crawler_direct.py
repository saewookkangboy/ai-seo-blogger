#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ì§ì ‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import asyncio

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.crawler import get_text_from_url, crawl_url

async def test_crawler():
    """í¬ë¡¤ëŸ¬ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í¬ë¡¤ëŸ¬ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    test_urls = [
        "https://www.example.com/",
        "https://httpbin.org/html",
        "https://jsonplaceholder.typicode.com/posts/1"
    ]
    
    for url in test_urls:
        print(f"\nğŸ“„ í…ŒìŠ¤íŠ¸ URL: {url}")
        
        try:
            # ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ (ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë¨)
            print("   ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸...")
            content = await crawl_url(url, use_google_style=True)
            if content:
                print(f"   âœ… ë™ê¸° í¬ë¡¤ëŸ¬ ì„±ê³µ: {len(content)}ì")
                print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
            else:
                print("   âŒ ë™ê¸° í¬ë¡¤ëŸ¬ ì‹¤íŒ¨")
            
            # ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
            print("   ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸...")
            content_async = await get_text_from_url(url)
            if content_async:
                print(f"   âœ… ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì„±ê³µ: {len(content_async)}ì")
                print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_async[:200]}...")
            else:
                print("   âŒ ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"   âŒ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(test_crawler()) 