#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ì„±ê³µë¥  ë¦¬í¬íŠ¸ ìƒì„± ë„êµ¬
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def generate_crawling_report():
    """í¬ë¡¤ë§ ì„±ê³µë¥  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        from app.services.crawler_monitor import crawling_monitor
        
        print("=" * 60)
        print("ğŸ“Š í¬ë¡¤ë§ ì„±ê³µë¥  ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        report = crawling_monitor.generate_report()
        print(report)
        
        # ì¶”ê°€ í†µê³„
        print("\nğŸ“ˆ ìƒì„¸ í†µê³„:")
        overall = crawling_monitor.get_overall_stats()
        
        if overall['total_attempts'] > 0:
            print(f"   â€¢ í‰ê·  ì„±ê³µë¥ : {overall['success_rate']:.1%}")
            
            # ì‚¬ì´íŠ¸ë³„ ì„±ê³µë¥ 
            problem_sites = crawling_monitor.get_problem_sites()
            if problem_sites:
                print(f"\nğŸš¨ ë¬¸ì œ ì‚¬ì´íŠ¸ ìƒì„¸:")
                for site in problem_sites[:5]:
                    print(f"   â€¢ {site['domain']}")
                    print(f"     - ì„±ê³µë¥ : {site['success_rate']:.1%}")
                    print(f"     - ì‹¤íŒ¨ ì›ì¸: {', '.join(site['common_errors'].keys())}")
            
            # ìµœê·¼ ì‹œë„
            recent = crawling_monitor.get_recent_attempts(10)
            if recent:
                print(f"\nğŸ•’ ìµœê·¼ ì‹œë„ (ìµœê·¼ 10ê°œ):")
                for attempt in recent:
                    status = "âœ…" if attempt['success'] else "âŒ"
                    print(f"   {status} {attempt['domain']} - {attempt['timestamp'][:19]}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return False
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def test_specific_site(url: str):
    """íŠ¹ì • ì‚¬ì´íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        from app.services.crawler import EnhancedCrawler
        
        print(f"\nğŸ§ª {url} ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        crawler = EnhancedCrawler()
        content = crawler.crawl_url(url)
        
        if content:
            print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"   â€¢ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content)}ì")
            print(f"   â€¢ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
        else:
            print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            
        return content is not None
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  python tools/crawler_report.py report     # ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±")
        print("  python tools/crawler_report.py test <URL> # íŠ¹ì • ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸")
        return
    
    command = sys.argv[1]
    
    if command == "report":
        generate_crawling_report()
    elif command == "test":
        if len(sys.argv) < 3:
            print("âŒ í…ŒìŠ¤íŠ¸í•  URLì„ ì…ë ¥í•˜ì„¸ìš”")
            return
        url = sys.argv[2]
        test_specific_site(url)
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}")

if __name__ == "__main__":
    main() 