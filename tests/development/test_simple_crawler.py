#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import re

def test_simple_crawler():
    """ê°„ë‹¨í•œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ê°„ë‹¨í•œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    test_url = "https://www.example.com/"
    
    try:
        print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ URL: {test_url}")
        
        # ì§ì ‘ requestsë¡œ í…ŒìŠ¤íŠ¸
        response = requests.get(test_url, timeout=10)
        response.raise_for_status()
        
        print(f"âœ… HTTP ìš”ì²­ ì„±ê³µ: {response.status_code}")
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = soup.find('title')
        if title:
            print(f"ğŸ“ ì œëª©: {title.get_text()}")
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
        content_selectors = [
            'body',
            'div',
            'p',
            'article',
            'main'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # ê°€ì¥ í° í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
                best_content = None
                max_length = 0
                
                for element in elements:
                    text = element.get_text(strip=True)
                    if len(text) > max_length:
                        max_length = len(text)
                        best_content = text
                
                if best_content and len(best_content) > 50:
                    print(f"âœ… {selector} ì„ íƒìë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì„±ê³µ: {len(best_content)}ì")
                    print(f"ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {best_content[:200]}...")
                    return True
        
        print("âŒ ëª¨ë“  ì„ íƒìë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨")
        return False
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    test_simple_crawler() 