# app/services/content_generator.py

import openai
from typing import Optional, Dict, List, Any, Tuple
from ..config import settings
from ..exceptions import ContentGenerationError, APIKeyError
from ..utils.logger import setup_logger
from .keyword_manager import keyword_manager
from .redis_cache import get_redis_cache
from .structured_logger import setup_optimized_logging
import os
import json
import threading
from datetime import datetime
import re
import asyncio
import time

logger = setup_logger(__name__)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
client = None

# API ì‚¬ìš©ëŸ‰ íŒŒì¼ ê²½ë¡œ
API_USAGE_FILE_PATH = os.path.abspath(os.path.join(os.path.dirname(os.path.dirname(__file__)), '..', 'api_usage.json'))
API_USAGE_LOCK = threading.Lock()

# ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì • (ìµœì í™”)
OPENAI_TIMEOUT = 30  # 30ì´ˆë¡œ ë‹¨ì¶• (ì†ë„ í–¥ìƒ)
MAX_RETRIES = 2  # ì¬ì‹œë„ íšŸìˆ˜ ë‹¨ì¶• (ì†ë„ í–¥ìƒ)
CACHE_DURATION = 1800  # ìºì‹œ ì‹œê°„ 30ë¶„ìœ¼ë¡œ ë‹¨ì¶• (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
MAX_CONCURRENT_GENERATIONS = 2  # ë™ì‹œ ìƒì„± ìˆ˜ ì œí•œ (ì•ˆì •ì„±)
GENERATION_DELAY = 0.1  # ìƒì„± ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)

# Redis ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
redis_cache = get_redis_cache()

def _initialize_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global client
    
    api_key = settings.get_openai_api_key()
    if not api_key:
        logger.warning("ìœ íš¨í•œ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return False
    
    try:
        client = openai.AsyncOpenAI(
            api_key=api_key,
            timeout=openai.Timeout(OPENAI_TIMEOUT)  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def _get_cache_key(text: str, keywords: str, content_length: str, ai_mode: str) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    import hashlib
    content = f"{text[:100]}{keywords}{content_length}{ai_mode}"
    return f"content_gen:{hashlib.md5(content.encode()).hexdigest()}"

def _get_cached_content(cache_key: str) -> Optional[Dict[str, Any]]:
    """ìºì‹œëœ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° (Redis ì‚¬ìš©)"""
    return redis_cache.get(cache_key)

def _set_cached_content(cache_key: str, content: Dict[str, Any]):
    """ì½˜í…ì¸ ë¥¼ ìºì‹œì— ì €ì¥ (Redis ì‚¬ìš©)"""
    # 30ë¶„(1800ì´ˆ) ë™ì•ˆ ìºì‹œ ìœ ì§€
    redis_cache.set(cache_key, content, ttl=1800)

def clear_content_cache():
    """ì½˜í…ì¸  ìºì‹œ í´ë¦¬ì–´ (Redis ì‚¬ìš©)"""
    # Redisì—ì„œëŠ” ì „ì²´ í‚¤ ì‚­ì œê°€ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ ë¡œê·¸ë§Œ ë‚¨ê¹€ (ë˜ëŠ” íŠ¹ì • íŒ¨í„´ ì‚­ì œ êµ¬í˜„ ê°€ëŠ¥)
    logger.info("Redis ìºì‹œ í´ë¦¬ì–´ ìš”ì²­ë¨ (êµ¬í˜„ í•„ìš”)")

def clear_specific_cache(text: str, keywords: str, content_length: str, ai_mode: str):
    """íŠ¹ì • ì½˜í…ì¸ ì˜ ìºì‹œ í´ë¦¬ì–´ (Redis ì‚¬ìš©)"""
    cache_key = _get_cache_key(text, keywords, content_length, ai_mode)
    redis_cache.delete(cache_key)
    logger.info(f"íŠ¹ì • ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤: {cache_key}")

# ë™ì‹œ ìƒì„± ì œì–´ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´
generation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_GENERATIONS)

# dev-agent-kit / .spec-kit/03-content-creation.md ê¸°ë°˜ ì½˜í…ì¸  í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ (AI SEOÂ·GEOÂ·AIO)
def _get_content_quality_checklist() -> str:
    """í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•  ì½˜í…ì¸  í’ˆì§ˆÂ·ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´. AI SEO, GEO, ì‹œë§¨í‹± HTML ìš”êµ¬ì‚¬í•­ ë°˜ì˜."""
    return """
[ì½˜í…ì¸  í’ˆì§ˆ ë° ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸]
- **AI SEO**: íƒ€ê²Ÿ í‚¤ì›Œë“œì™€ LSI í‚¤ì›Œë“œë¥¼ ì œëª©Â·í—¤ë”©Â·ì²« ë¬¸ë‹¨ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨. í‚¤ì›Œë“œ ë°€ë„ì™€ ê°€ë…ì„± ìœ ì§€(ê³¼ë„í•œ ì‚½ì… ê¸ˆì§€). ì œëª© 60ì ì´ë‚´, ë©”íƒ€ ì„¤ëª… 160ì ì´ë‚´.
- **GEO(ìƒì„±í˜• ì—”ì§„)**: FAQ ìŠ¤í‚¤ë§ˆ ì¹œí™”(ì§ˆë¬¸-ë‹µë³€ ë¸”ë¡ ë˜ëŠ” faq-section), HowTo/ë‹¨ê³„ë³„ ê°€ì´ë“œ(ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸Â·ë‹¨ê³„ êµ¬ë¶„), Article ìŠ¤í‚¤ë§ˆ ì¹œí™”(ëª…í™•í•œ ë³¸ë¬¸Â·ë¦¬ìŠ¤íŠ¸). ì¸ìš©Â·ì¶œì²˜Â·ì‹ ë¢°ë„ ê°•í™”(ì£¼ì¥ ì‹œ ê·¼ê±°Â·ì¶œì²˜ ì–¸ê¸‰). AI ì—”ì§„ ì¹œí™”ì  ëª…í™•í•œ ë¬¸ë‹¨Â·ë¦¬ìŠ¤íŠ¸Â·ì†Œì œëª© êµ¬ì¡°.
- **ì‹œë§¨í‹± HTML**: H1 1íšŒ, H2/H3 ê³„ì¸µ ì¤€ìˆ˜, <p>, <ul>/<ol>, <section> ë“± ì˜ë¯¸ êµ¬ì¡°. E-E-A-T(ê²½í—˜Â·ì „ë¬¸ì„±Â·ê¶Œìœ„Â·ì‹ ë¢°ì„±) ì‹ í˜¸ ê°•í™”.
"""

def is_gemini_quota_exceeded(error_message: str) -> bool:
    """Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜ì¸ì§€ í™•ì¸"""
    quota_error_patterns = [
        "quota exceeded",
        "quota limit",
        "rate limit exceeded",
        "resource exhausted",
        "quota has been exceeded",
        "quota limit exceeded",
        "429",
        "too many requests"
    ]
    error_lower = error_message.lower()
    return any(pattern in error_lower for pattern in quota_error_patterns)

async def translate_with_openai(text: str, target_lang: str = "ko") -> str:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•œ fallback ë²ˆì—­ í•¨ìˆ˜
    """
    global client
    
    if not text or not text.strip():
        return ""
    
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if not client and not _initialize_client():
        logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return text
    
    try:
        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {
                    "role": "system",
                    "content": f"ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ {target_lang}ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": text
                }
            ],
            max_tokens=settings.openai_max_tokens,
            temperature=0.3
        )
        
        translated_text = response.choices[0].message.content.strip()
        logger.info(f"OpenAI ë²ˆì—­ ì™„ë£Œ: {len(text)}ì â†’ {len(translated_text)}ì")
        return translated_text
        
    except Exception as e:
        logger.error(f"OpenAI ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}")
        return text

async def create_blog_post(text: str, keywords: str, rule_guidelines: Optional[list] = None, content_length: str = "3000", ai_mode: Optional[str] = None, input_type: str = "text") -> Dict:
    """
    ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œ, ì¶”ê°€ ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    AI SEO ìµœì í™”ë¥¼ ìœ„í•œ êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Gemini 2.0 Flashì™€ OpenAIë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    # ë™ì‹œ ìƒì„± ì œì–´
    async with generation_semaphore:
        global client
        
        start_time = time.time()
        
        # ì…ë ¥ ê²€ì¦
        if not text or not text.strip():
            logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return _generate_default_template("", keywords, rule_guidelines, content_length, ai_mode)
        
        # ìºì‹œ í™•ì¸
        cache_key = _get_cache_key(text, keywords, content_length, ai_mode or "")
        cached_content = _get_cached_content(cache_key)
        if cached_content:
            logger.info("ìºì‹œëœ ì½˜í…ì¸  ìƒì„± ê²°ê³¼ ì‚¬ìš©")
            return cached_content
        
        # ìƒì„± ìš”ì²­ ê°„ ì§€ì—° (API ë¶€í•˜ ë°©ì§€)
        await asyncio.sleep(GENERATION_DELAY)
        
    # ìµœì‹  SEO ê°€ì´ë“œë¼ì¸ ë¡œë“œ ë° ì ìš©
    try:
        from app.seo_guidelines import get_seo_guidelines
        seo_guidelines = get_seo_guidelines()
        
        # ê°€ì´ë“œë¼ì¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if rule_guidelines is None:
            rule_guidelines = []
        else:
            rule_guidelines = list(rule_guidelines)  # ë³µì‚¬ë³¸ ìƒì„±
            
        # í™œì„±í™”ëœ ê°€ì´ë“œë¼ì¸ ì¶”ì¶œ
        active_guidelines = []
        if seo_guidelines.get("guidelines", {}).get("ai_seo", {}).get("enabled"):
            e_e_a_t = seo_guidelines["guidelines"]["ai_seo"].get("e_e_a_t", {})
            active_guidelines.append(f"E-E-A-T ì›ì¹™ ì¤€ìˆ˜: {e_e_a_t.get('experience', {}).get('description', '')}, {e_e_a_t.get('expertise', {}).get('description', '')}")
        
        if seo_guidelines.get("guidelines", {}).get("aeo", {}).get("enabled"):
            aeo = seo_guidelines["guidelines"]["aeo"]
            active_guidelines.append(f"AEO ìµœì í™”: {aeo.get('description', '')}")
            
        if seo_guidelines.get("guidelines", {}).get("geo", {}).get("enabled"):
            geo = seo_guidelines["guidelines"]["geo"]
            active_guidelines.append(f"GEO ìµœì í™”: {geo.get('description', '')}")
            
        if active_guidelines:
            rule_guidelines.append("--- ìµœì‹  SEO ê°€ì´ë“œë¼ì¸ ---")
            rule_guidelines.extend(active_guidelines)
            
    except Exception as e:
        logger.warning(f"SEO ê°€ì´ë“œë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        if rule_guidelines is None:
            rule_guidelines = []
    
    # AI ëª¨ë“œì— ë”°ë¥¸ í•„ìˆ˜ HTML êµ¬ì¡° ê°€ì´ë“œë¼ì¸ ì¶”ê°€
    structure_guidelines = []
    if ai_mode == "ai_seo":
        structure_guidelines.append("- **í•„ìˆ˜ HTML êµ¬ì¡°**: Schema.org ë§ˆí¬ì—…(itemscope, itemtype)ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.")
        structure_guidelines.append("- **í—¤ë”© êµ¬ì¡°**: H1, H2, H3 íƒœê·¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë§Œë“œì„¸ìš”.")
    elif ai_mode == "aeo":
        structure_guidelines.append("- **í•„ìˆ˜ HTML êµ¬ì¡°**: `<div class=\"faq-section\">` ë‚´ì— `<div class=\"faq-item\">`ì„ ì‚¬ìš©í•˜ì—¬ Q&A í˜•ì‹ì„ êµ¬í˜„í•˜ì„¸ìš”.")
        structure_guidelines.append("- **ë‹µë³€ ìŠ¤íƒ€ì¼**: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ë¨¼ì € ì œì‹œí•˜ì„¸ìš”.")
    elif ai_mode == "geo":
        structure_guidelines.append("- **í•„ìˆ˜ HTML êµ¬ì¡°**: `<div class=\"comparison-table\">` ë˜ëŠ” `<div class=\"how-to-section\">` ë“± êµ¬ì¡°í™”ëœ ì •ë³´ ë¸”ë¡ì„ í¬í•¨í•˜ì„¸ìš”.")
        structure_guidelines.append("- **ì‹ ë¢°ì„±**: ì£¼ì¥ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¶œì²˜ë‚˜ ë°ì´í„°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.")
    
    if structure_guidelines:
        rule_guidelines.append("--- HTML êµ¬ì¡° ê°€ì´ë“œë¼ì¸ ---")
        rule_guidelines.extend(structure_guidelines)
    
    # Gemini 2.0 Flash ëª¨ë“œì¸ ê²½ìš° Gemini ì‚¬ìš©
    if ai_mode == "gemini_2_0_flash":
        try:
            result = await _create_blog_post_with_gemini(text, keywords, rule_guidelines, content_length, ai_mode, input_type)
            if result:
                _set_cached_content(cache_key, result)
                return result
        except Exception as e:
            error_message = str(e)
            if is_gemini_quota_exceeded(error_message):
                logger.warning("Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ OpenAIë¡œ fallbackí•©ë‹ˆë‹¤.")
            else:
                logger.warning(f"Gemini 2.0 Flash ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨, OpenAIë¡œ fallback: {e}")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if not client and not _initialize_client():
        logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return _generate_default_template(text, keywords, rule_guidelines, content_length, ai_mode)
    
    try:
        return await _create_blog_post_with_openai(text, keywords, rule_guidelines, content_length, ai_mode, input_type)
        
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return _generate_default_template(text, keywords, rule_guidelines, content_length, ai_mode)

async def _create_blog_post_with_gemini(text: str, keywords: str, rule_guidelines: Optional[list] = None, content_length: str = "5000", ai_mode: Optional[str] = None, input_type: str = "text") -> Dict:
    """
    Gemini 2.0 Flash APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        import httpx
        
        # Gemini API í‚¤ í™•ì¸
        from .translator import get_gemini_api_key
        api_key = get_gemini_api_key()
        if not api_key:
            raise ContentGenerationError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì½˜í…ì¸  ê¸¸ì´ ì„¤ì •
        target_length = int(content_length) if content_length.isdigit() else 5000
        
        # ê·œì¹™ ê°€ì´ë“œë¼ì¸ ì²˜ë¦¬
        rules_text = ""
        if rule_guidelines:
            rules_text = "\n".join([f"- {rule}" for rule in rule_guidelines])
        
        # ì…ë ¥ íƒ€ì…ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­
        input_instruction = ""
        if input_type == "url":
            input_instruction = """
- ì…ë ¥ëœ í…ìŠ¤íŠ¸ëŠ” íŠ¹ì • URLì—ì„œ í¬ë¡¤ë§í•œ ë‚´ìš©ì…ë‹ˆë‹¤.
- ì›ë³¸ì˜ ë‚´ìš©ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ë˜, í•œêµ­ì–´ ë…ìì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¬êµ¬ì„±(Rewrite)í•˜ì„¸ìš”.
- ë‹¨ìˆœ ë²ˆì—­ì´ ì•„ë‹Œ, ë¬¸ë§¥ì„ ê³ ë ¤í•œ 'ì¬ì°½ì¡°' ìˆ˜ì¤€ì˜ ê¸€ì“°ê¸°ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.
"""
        else:
            input_instruction = """
- ì…ë ¥ëœ í…ìŠ¤íŠ¸ëŠ” ì£¼ì œë‚˜ ë¬¸ë§¥ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
- ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìƒˆë¡œìš´ ê¸€ì„ ì‘ì„±(Write)í•˜ì„¸ìš”.
- ì£¼ì–´ì§„ ì£¼ì œë¥¼ í™•ì¥í•˜ì—¬ í’ë¶€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (dev-agent-kit / .spec-kit/03-content-creation ë°˜ì˜)
        content_checklist = _get_content_quality_checklist()
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI SEO ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ê³ í’ˆì§ˆì˜ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì…ë ¥ ì •ë³´]
- ì›ë³¸ í…ìŠ¤íŠ¸/ë¬¸ë§¥:
{text}

- ì£¼ìš” í‚¤ì›Œë“œ: {keywords}
- ëª©í‘œ ê¸¸ì´: ì•½ {target_length}ì
- ì…ë ¥ ìœ í˜•: {input_type} ({input_instruction.strip()})

[í•µì‹¬ ìš”êµ¬ì‚¬í•­]
1. **ê· í˜• ì¡íŒ ê´€ì  (Responsible AI)**: í¸í–¥ë˜ì§€ ì•Šê³  ê°ê´€ì ì´ë©° ìœ¤ë¦¬ì ì¸ ê´€ì ì„ ìœ ì§€í•˜ì„¸ìš”. ë‹¤ì–‘í•œ ì‹œê°ì„ ê³ ë ¤í•˜ì—¬ ê· í˜• ì¡íŒ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
2. **SEO ë° AI ìµœì í™”**: ì„ íƒëœ ìµœì í™” ì˜µì…˜ì— ë§ì¶° ê¸€ì„ êµ¬ì„±í•˜ê³ , í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë³¸ë¬¸ì— ë…¹ì—¬ë‚´ì„¸ìš”.
3. **êµ¬ì¡°í™”ëœ ì¶œë ¥**: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

{content_checklist}

[ì¶”ê°€ ê·œì¹™ ë° ê°€ì´ë“œë¼ì¸]
{rules_text}

[ì¶œë ¥ í˜•ì‹ (JSON)]
ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json ... ```)ìœ¼ë¡œ ê°ì‹¸ì£¼ì„¸ìš”.

```json
{{
  "title": "ë¸”ë¡œê·¸ ì œëª©",
  "meta_description": "SEO ë©”íƒ€ ì„¤ëª… (160ì ì´ë‚´)",
  "content": "<h1>ì œëª©</h1>...<p>ë³¸ë¬¸ ë‚´ìš©(HTML í˜•ì‹)</p>...",
  "keywords": "ì¶”ì¶œëœ ì£¼ìš” í‚¤ì›Œë“œ (ìµœëŒ€ 10ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„)",
  "metrics": {{
    "noun_count": 0,
    "verb_count": 0,
    "adjective_count": 0,
    "max_sentence_length": 0
  }},
  "score": 0,
  "evaluation": "ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ ì—¬ë¶€ì— ëŒ€í•œ ì§§ì€ í‰ê°€ ë° ì ìˆ˜ ì´ìœ ",
  "ai_analysis": {{
    "trust_score": 0,
    "trust_reason": "ì‹ ë¢°ë„ ì ìˆ˜ ì´ìœ ",
    "seo_score": 0,
    "seo_reason": "SEO ì ìˆ˜ ì´ìœ ",
    "ai_summary": "ì½˜í…ì¸  í•µì‹¬ ìš”ì•½ (100ì ì´ë‚´)"
  }}
}}
```

**metrics ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:**
- ìƒì„±ëœ í•œêµ­ì–´ ì½˜í…ì¸ ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ì˜ ê°œìˆ˜ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ê¸°ì…í•˜ì„¸ìš”.
- max_sentence_lengthëŠ” ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸€ì ìˆ˜(ê³µë°± ì œì™¸)ë¥¼ ê¸°ì…í•˜ì„¸ìš”.
- scoreëŠ” 0~100ì  ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ, ìœ„ì—ì„œ ì œì‹œí•œ ê°€ì´ë“œë¼ì¸(SEO, ê· í˜• ì¡íŒ ê´€ì  ë“±)ì„ ì–¼ë§ˆë‚˜ ì˜ ì¤€ìˆ˜í–ˆëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ í‰ê°€í•œ ì ìˆ˜ì…ë‹ˆë‹¤.

**ai_analysis ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:**
- trust_score: 5ì  ë§Œì  (1-5). ì½˜í…ì¸ ì˜ ì‹ ë¢°ì„±, ê°ê´€ì„±, ê·¼ê±° ì œì‹œ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ì„¸ìš”.
- seo_score: 10ì  ë§Œì  (1-10). í‚¤ì›Œë“œ ì‚¬ìš©, êµ¬ì¡°, ê°€ë…ì„± ë“±ì„ í‰ê°€í•˜ì„¸ìš”.
- ai_summary: ì „ì²´ ë‚´ìš©ì„ 100ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.
"""
        
        async with httpx.AsyncClient(timeout=60) as client:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": 8192,
                    "responseMimeType": "application/json"
                }
            }
            
            response = await client.post(url, json=payload)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    generated_text = result['candidates'][0]['content']['parts'][0]['text'].strip()
                    
                    # JSON íŒŒì‹±
                    try:
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                        if generated_text.startswith("```json"):
                            generated_text = generated_text[7:]
                        if generated_text.endswith("```"):
                            generated_text = generated_text[:-3]
                        
                        data = json.loads(generated_text)
                        
                        return {
                            'post': data.get('content', ''),
                            'title': data.get('title', ''),
                            'meta_description': data.get('meta_description', ''),
                            'keywords': data.get('keywords', keywords),
                            'metrics': data.get('metrics', {}),
                            'score': data.get('score', 0),
                            'evaluation': data.get('evaluation', ''),
                            'ai_analysis': data.get('ai_analysis', {}),
                            'word_count': len(data.get('content', '').split()), # ë‹¨ìˆœ ê³µë°± ê¸°ì¤€
                            'content_length': content_length,
                            'ai_mode': ai_mode or 'gemini_2_0_flash'
                        }
                    except json.JSONDecodeError as e:
                        logger.error(f"Gemini ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ì½˜í…ì¸ ë¡œ ê°„ì£¼ (fallback)
                        return {
                            'post': generated_text,
                            'title': f"{keywords.split(',')[0]} ê´€ë ¨ í¬ìŠ¤íŠ¸",
                            'meta_description': "ìë™ ìƒì„±ëœ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                            'keywords': keywords,
                            'word_count': len(generated_text.split()),
                            'content_length': content_length,
                            'ai_mode': ai_mode or 'gemini_2_0_flash'
                        }
                else:
                    raise ContentGenerationError("Gemini API ì‘ë‹µì— ìƒì„± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                error_detail = response.text
                logger.error(f"Gemini API ì˜¤ë¥˜: {response.status_code} - {error_detail}")
                raise ContentGenerationError(f"Gemini API ì˜¤ë¥˜: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Gemini ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        raise ContentGenerationError(f"Gemini ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

async def _create_blog_post_with_openai(text: str, keywords: str, rule_guidelines: Optional[list] = None, content_length: str = "3000", ai_mode: Optional[str] = None, input_type: str = "text") -> Dict:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    global client
    
    if not client and not _initialize_client():
        raise ContentGenerationError("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    try:
        # ì½˜í…ì¸  ê¸¸ì´ ì„¤ì •
        target_length = int(content_length) if content_length.isdigit() else 3000
        
        # ê·œì¹™ ê°€ì´ë“œë¼ì¸ ì²˜ë¦¬
        rules_text = ""
        if rule_guidelines:
            rules_text = "\n".join([f"- {rule}" for rule in rule_guidelines])
        
        # ì…ë ¥ íƒ€ì…ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­
        input_instruction = ""
        if input_type == "url":
            input_instruction = "ì…ë ¥ëœ í…ìŠ¤íŠ¸ëŠ” íŠ¹ì • URLì—ì„œ í¬ë¡¤ë§í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ì›ë³¸ì˜ ë‚´ìš©ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ë˜, í•œêµ­ì–´ ë…ìì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¬êµ¬ì„±(Rewrite)í•˜ì„¸ìš”."
        else:
            input_instruction = "ì…ë ¥ëœ í…ìŠ¤íŠ¸ëŠ” ì£¼ì œë‚˜ ë¬¸ë§¥ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìƒˆë¡œìš´ ê¸€ì„ ì‘ì„±(Write)í•˜ì„¸ìš”."

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (dev-agent-kit / .spec-kit/03-content-creation ë°˜ì˜)
        content_checklist = _get_content_quality_checklist()
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI SEO ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ê³ í’ˆì§ˆì˜ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì…ë ¥ ì •ë³´]
- ì›ë³¸ í…ìŠ¤íŠ¸/ë¬¸ë§¥:
{text}

- ì£¼ìš” í‚¤ì›Œë“œ: {keywords}
- ëª©í‘œ ê¸¸ì´: ì•½ {target_length}ì
- ì…ë ¥ ìœ í˜•: {input_type} ({input_instruction})

[í•µì‹¬ ìš”êµ¬ì‚¬í•­]
1. **ê· í˜• ì¡íŒ ê´€ì  (Responsible AI)**: í¸í–¥ë˜ì§€ ì•Šê³  ê°ê´€ì ì´ë©° ìœ¤ë¦¬ì ì¸ ê´€ì ì„ ìœ ì§€í•˜ì„¸ìš”. ë‹¤ì–‘í•œ ì‹œê°ì„ ê³ ë ¤í•˜ì—¬ ê· í˜• ì¡íŒ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
2. **SEO ë° AI ìµœì í™”**: ì„ íƒëœ ìµœì í™” ì˜µì…˜ì— ë§ì¶° ê¸€ì„ êµ¬ì„±í•˜ê³  í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”.
3. **êµ¬ì¡°í™”ëœ ì¶œë ¥**: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

{content_checklist}

[ì¶”ê°€ ê·œì¹™]
{rules_text}

[ì¶œë ¥ í˜•ì‹ (JSON)]
- meta_descriptionì€ ë°˜ë“œì‹œ 160ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
```json
{{
  "title": "ë¸”ë¡œê·¸ ì œëª© (60ì ì´ë‚´)",
  "meta_description": "SEO ë©”íƒ€ ì„¤ëª… (160ì ì´ë‚´)",
  "content": "<h1>ì œëª©</h1>...<p>ë³¸ë¬¸ ë‚´ìš©(HTML í˜•ì‹)</p>...",
  "keywords": "ì¶”ì¶œëœ ì£¼ìš” í‚¤ì›Œë“œ (ìµœëŒ€ 10ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„)",
  "metrics": {{
    "noun_count": 0,
    "verb_count": 0,
    "adjective_count": 0,
    "max_sentence_length": 0
  }},
  "score": 0,
  "evaluation": "í‰ê°€ ë° ì ìˆ˜ ì´ìœ ",
  "ai_analysis": {{
    "trust_score": 0,
    "trust_reason": "ì‹ ë¢°ë„ ì ìˆ˜ ì´ìœ ",
    "seo_score": 0,
    "seo_reason": "SEO ì ìˆ˜ ì´ìœ ",
    "ai_summary": "ì½˜í…ì¸  í•µì‹¬ ìš”ì•½ (100ì ì´ë‚´)"
  }}
}}
```
"""

        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ SEOÂ·AI SEOÂ·GEOì— ëŠ¥í†µí•œ ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ì½˜í…ì¸  í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì¤€ìˆ˜í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=settings.openai_max_tokens,
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        generated_content = response.choices[0].message.content.strip()
        
        # JSON íŒŒì‹±
        data = json.loads(generated_content)
        
        return {
            'post': data.get('content', ''),
            'title': data.get('title', ''),
            'meta_description': data.get('meta_description', ''),
            'keywords': data.get('keywords', keywords),
            'metrics': data.get('metrics', {}),
            'score': data.get('score', 0),
            'evaluation': data.get('evaluation', ''),
            'ai_analysis': data.get('ai_analysis', {}),
            'word_count': len(data.get('content', '').split()),
            'content_length': content_length,
            'ai_mode': ai_mode or 'openai'
        }
        
    except Exception as e:
        logger.error(f"OpenAI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        raise ContentGenerationError(f"OpenAI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

def _extract_keywords_from_content(content: str, db_session=None) -> str:
    """
    ì½˜í…ì¸ ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ê°œì„ ëœ ë²„ì „)
    """
    try:
        # ê¸°ì¡´ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        existing_keywords = set()
        if db_session:
            existing_keywords = keyword_manager.get_existing_keywords(db_session)
        
        # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        unique_keywords = keyword_manager.extract_unique_keywords(content, existing_keywords)
        
        return unique_keywords
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "AI, ê¸°ìˆ , ë¶„ì„, ê°œë°œ, ì‹œìŠ¤í…œ"

def _generate_default_template(text: str, keywords: str, rule_guidelines: Optional[List[str]] = None, content_length: str = "3000", ai_mode: Optional[str] = None) -> Dict:
    """
    API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # ê¸°ë³¸ ì œëª© ìƒì„±
        title = f"{keywords.split(',')[0] if keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'}ì— ëŒ€í•œ ìƒì„¸ ê°€ì´ë“œ"
        
        # ê¸°ë³¸ ë©”íƒ€ ì„¤ëª…
        meta_description = f"{keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        
        # ê¸°ë³¸ ì½˜í…ì¸  ìƒì„±
        content_length_int = int(content_length) if content_length.isdigit() else 3000
        
        # í…ìŠ¤íŠ¸ ìš”ì•½
        summary = text[:200] + "..." if len(text) > 200 else text
        
        # HTML ì½˜í…ì¸  ìƒì„±
        content = f"""
<h1>{title}</h1>
<meta name="description" content="{meta_description}">

<div class="content-intro">
    <p>ì´ ê¸€ì—ì„œëŠ” <strong>{keywords}</strong>ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>
</div>

<div class="content-main">
    <h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>
    <p>{summary}</p>
    
    <h2>ğŸ” í•µì‹¬ í¬ì¸íŠ¸</h2>
    <ul>
        <li><strong>ì²« ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> {keywords.split(',')[0] if keywords else 'ì£¼ì œ'}ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì¤‘ìš”ì„±</li>
        <li><strong>ë‘ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ì‹¤ì œ ì ìš© ë°©ë²•ê³¼ ì‚¬ë¡€</li>
        <li><strong>ì„¸ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ì£¼ì˜ì‚¬í•­ê³¼ ëª¨ë²” ì‚¬ë¡€</li>
    </ul>
    
    <h2>ğŸ’¡ ì‹¤ìš©ì ì¸ íŒ</h2>
    <div class="tips-container">
        <div class="tip-item">
            <h3>íŒ 1: íš¨ê³¼ì ì¸ í™œìš©</h3>
            <p>{keywords.split(',')[0] if keywords else 'ì£¼ì œ'}ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
        <div class="tip-item">
            <h3>íŒ 2: ì£¼ì˜ì‚¬í•­</h3>
            <p>ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í”¼í•´ì•¼ í•  í•¨ì •ì— ëŒ€í•´ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
    </div>
    
    <h2>ğŸ“Š ìš”ì•½</h2>
    <p>ì´ ê¸€ì„ í†µí•´ {keywords.split(',')[0] if keywords else 'ì£¼ì œ'}ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì´ê³ , ì‹¤ì œ ìƒí™©ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì§€ì‹ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.</p>
</div>

<style>
.content-intro {{ background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0; }}
.content-main {{ line-height: 1.6; }}
.tips-container {{ display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1rem 0; }}
.tip-item {{ background: #e3f2fd; padding: 1rem; border-radius: 8px; border-left: 4px solid #2196f3; }}
</style>
"""
        
        # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        text_content = re.sub(r'<[^>]+>', '', content)
        word_count = len(text_content.split())
        
        logger.info("ê¸°ë³¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ")
        
        return {
            'post': content,
            'title': title,
            'meta_description': meta_description,
            'keywords': keywords,
            'word_count': word_count,
            'content_length': content_length,
            'ai_mode': ai_mode or 'default_template'
        }
        
    except Exception as e:
        logger.error(f"ê¸°ë³¸ í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        # ìµœì†Œí•œì˜ ê¸°ë³¸ ì‘ë‹µ
        return {
            'post': f"<h1>{keywords.split(',')[0] if keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'}</h1><p>ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>",
            'title': keywords.split(',')[0] if keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸',
            'meta_description': f"{keywords}ì— ëŒ€í•œ ì •ë³´",
            'keywords': keywords,
            'word_count': 10,
            'content_length': content_length,
            'ai_mode': ai_mode or 'error_fallback'
        }

async def extract_seo_keywords(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ SEO í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = _extract_default_keywords(text)
        
        # OpenAI APIê°€ ìˆìœ¼ë©´ ë” ì •êµí•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
        if settings.get_openai_api_key():
            try:
                global client
                if not client and not _initialize_client():
                    return keywords
                
                prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ SEOì— ìœ ìš©í•œ í‚¤ì›Œë“œ 5-7ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text[:1000]}

í‚¤ì›Œë“œ:
"""
                
                response = await client.chat.completions.create(
                    model=settings.openai_model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=200,
                    temperature=0.3
                )
                
                extracted_keywords = response.choices[0].message.content.strip()
                return extracted_keywords
                
            except Exception as e:
                logger.error(f"OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return keywords
                
    except Exception as e:
        logger.error(f"SEO í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return _extract_default_keywords(text)

async def analyze_seo_news(news_items: List[Dict[str, Any]]) -> Dict[str, List[str]]:
    """
    ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì•„ì´í…œë“¤ì„ ë¶„ì„í•˜ì—¬ SEO íŠ¸ë Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    global client
    
    if not news_items:
        return {}
        
    if not client and not _initialize_client():
        logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ë‰´ìŠ¤ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {}
        
    try:
        # ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½ ì¤€ë¹„
        news_summary = ""
        for i, item in enumerate(news_items[:10]):  # ìµœëŒ€ 10ê°œë§Œ ë¶„ì„
            news_summary += f"{i+1}. ì œëª©: {item.get('title', '')}\n   ë‚´ìš©: {item.get('summary', '')}\n   ì¹´í…Œê³ ë¦¬: {item.get('category', '')}\n\n"
            
        prompt = f"""
ë‹¤ìŒì€ ìµœê·¼ ìˆ˜ì§‘ëœ SEO ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ê° ì¹´í…Œê³ ë¦¬ë³„(AI SEO, AEO, GEO, AIO, AI Search)ë¡œ ìµœì‹  íŠ¸ë Œë“œì™€ ì¤‘ìš”í•œ ë³€ê²½ì‚¬í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡:
{news_summary}

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "ai_seo": ["íŠ¸ë Œë“œ 1", "íŠ¸ë Œë“œ 2"],
    "aeo": ["íŠ¸ë Œë“œ 1", "íŠ¸ë Œë“œ 2"],
    "geo": ["íŠ¸ë Œë“œ 1", "íŠ¸ë Œë“œ 2"],
    "aio": ["íŠ¸ë Œë“œ 1", "íŠ¸ë Œë“œ 2"],
    "ai_search": ["íŠ¸ë Œë“œ 1", "íŠ¸ë Œë“œ 2"]
}}

ê° íŠ¸ë Œë“œëŠ” ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ì•Œê³ ë¦¬ì¦˜ ì—…ë°ì´íŠ¸ë‚˜ ìƒˆë¡œìš´ ìµœì í™” ì „ëµì— ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.
"""

        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ SEO íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        result_json = response.choices[0].message.content.strip()
        return json.loads(result_json)
        
    except Exception as e:
        logger.error(f"SEO ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}

async def update_guideline_content(current_guidelines: Dict[str, Any], trends: Dict[str, List[str]]) -> Dict[str, Any]:
    """
    í˜„ì¬ ê°€ì´ë“œë¼ì¸ê³¼ ìƒˆë¡œìš´ íŠ¸ë Œë“œë¥¼ ë³‘í•©í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ ê°€ì´ë“œë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    global client
    
    if not client and not _initialize_client():
        logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ê°€ì´ë“œë¼ì¸ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return current_guidelines
        
    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒì€ í˜„ì¬ SEO ê°€ì´ë“œë¼ì¸ê³¼ ìƒˆë¡œ ë°œê²¬ëœ íŠ¸ë Œë“œì…ë‹ˆë‹¤.
ìƒˆë¡œìš´ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•˜ì—¬ ê°€ì´ë“œë¼ì¸ì„ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”.

í˜„ì¬ ê°€ì´ë“œë¼ì¸ (JSON):
{json.dumps(current_guidelines, ensure_ascii=False, indent=2)[:3000]}... (ìƒëµ)

ìƒˆë¡œìš´ íŠ¸ë Œë“œ (JSON):
{json.dumps(trends, ensure_ascii=False, indent=2)}

ìš”êµ¬ì‚¬í•­:
1. ê¸°ì¡´ ê°€ì´ë“œë¼ì¸ì˜ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
2. ìƒˆë¡œìš´ íŠ¸ë Œë“œë¥¼ ì ì ˆí•œ ì„¹ì…˜(requirements, optimization_focus ë“±)ì— ì¶”ê°€í•˜ê±°ë‚˜ ê¸°ì¡´ ë‚´ìš©ì„ ìˆ˜ì •í•˜ì„¸ìš”.
3. 'version'ê³¼ 'last_updated' í•„ë“œëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ë§ˆì„¸ìš” (ì‹œìŠ¤í…œì´ ì²˜ë¦¬í•¨).
4. JSON í˜•ì‹ìœ¼ë¡œ ì „ì²´ ê°€ì´ë“œë¼ì¸ì„ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ SEO ê°€ì´ë“œë¼ì¸ ê´€ë¦¬ìì…ë‹ˆë‹¤. JSON í˜•ì‹ì„ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ì‘ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        result_json = response.choices[0].message.content.strip()
        updated_guidelines = json.loads(result_json)
        
        return updated_guidelines
        
    except Exception as e:
        logger.error(f"ê°€ì´ë“œë¼ì¸ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return current_guidelines



def _extract_default_keywords(text: str) -> str:
    """
    ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
    """
    try:
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        # í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]+', clean_text)
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freq = {}
        for word in korean_words:
            if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ì¶”ì¶œ
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords = [word for word, freq in sorted_words[:5]]
        
        if keywords:
            return ', '.join(keywords)
        else:
            return "AI, ê¸°ìˆ , ë¶„ì„, ê°œë°œ, ì‹œìŠ¤í…œ"
            
    except Exception as e:
        logger.error(f"ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "AI, ê¸°ìˆ , ë¶„ì„, ê°œë°œ, ì‹œìŠ¤í…œ"

def increase_api_usage_count(api_name: str):
    """API ì‚¬ìš©ëŸ‰ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        with API_USAGE_LOCK:
            if os.path.exists(API_USAGE_FILE_PATH):
                with open(API_USAGE_FILE_PATH, 'r', encoding='utf-8') as f:
                    usage_data = json.load(f)
            else:
                usage_data = {}
            
            today = datetime.now().strftime('%Y-%m-%d')
            if today not in usage_data:
                usage_data[today] = {}
            
            if api_name not in usage_data[today]:
                usage_data[today][api_name] = 0
            
            usage_data[today][api_name] += 1
            
            with open(API_USAGE_FILE_PATH, 'w', encoding='utf-8') as f:
                json.dump(usage_data, f, ensure_ascii=False, indent=2)
                
    except Exception as e:
        logger.warning(f"API ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")

async def create_enhanced_blog_post(text: str, keywords: str, rule_guidelines: Optional[list] = None, content_length: str = "3000", ai_mode: Optional[str] = None) -> Dict:
    """
    í–¥ìƒëœ ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
    - AI ì¶”ì²œ í‚¤ì›Œë“œ (ëª…ì‚¬ ì¤‘ì‹¬, ìµœëŒ€ 10ê°œ)
    - ì£¼ìš” ë‚´ìš©, í•µì‹¬ í¬ì¸íŠ¸, ì‹¤ìš©ì ì¸ íŒ, ìš”ì•½
    - AI ìš”ì•½ (100ì ì´ë‚´)
    - ì‹ ë¢°ë„ í‰ê°€ (5ì  ë§Œì )
    - SEO ìµœì í™” ì ìˆ˜ (10ì  ë§Œì )
    """
    # ë™ì‹œ ìƒì„± ì œì–´
    async with generation_semaphore:
        global client
        
        start_time = time.time()
        
        # ì…ë ¥ ê²€ì¦
        if not text or not text.strip():
            logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return _generate_enhanced_default_template("", keywords, rule_guidelines, content_length, ai_mode)
        
        # ìºì‹œ í™•ì¸
        cache_key = _get_cache_key(text, keywords, content_length, ai_mode or "")
        cached_content = _get_cached_content(cache_key)
        if cached_content:
            logger.info("ìºì‹œëœ ì½˜í…ì¸  ìƒì„± ê²°ê³¼ ì‚¬ìš©")
            return cached_content
        
        # ìƒì„± ìš”ì²­ ê°„ ì§€ì—° (API ë¶€í•˜ ë°©ì§€)
        await asyncio.sleep(GENERATION_DELAY)
    
    # Gemini 2.0 Flash ëª¨ë“œì¸ ê²½ìš° Gemini ì‚¬ìš©
    if ai_mode == "gemini_2_0_flash":
        try:
            result = await _create_enhanced_blog_post_with_gemini(text, keywords, rule_guidelines, content_length, ai_mode)
            if result:
                _set_cached_content(cache_key, result)
                return result
        except Exception as e:
            error_message = str(e)
            if is_gemini_quota_exceeded(error_message):
                logger.warning("Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ OpenAIë¡œ fallbackí•©ë‹ˆë‹¤.")
            else:
                logger.warning(f"Gemini 2.0 Flash ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨, OpenAIë¡œ fallback: {e}")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if not client and not _initialize_client():
        logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return _generate_enhanced_default_template(text, keywords, rule_guidelines, content_length, ai_mode)
    
    try:
        # ì½˜í…ì¸  ê¸¸ì´ ì„¤ì •
        target_length = int(content_length) if content_length.isdigit() else 3000
        
        # AI ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
        if ai_mode == "creative":
            style_instruction = "ì°½ì˜ì ì´ê³  í¥ë¯¸ë¡œìš´ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì „ë¬¸ì„±ì„ ìœ ì§€í•˜ì„¸ìš”."
        elif ai_mode == "informative":
            style_instruction = "ì •ë³´ê°€ í’ë¶€í•˜ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
        elif ai_mode == "professional":
            style_instruction = "ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
        else:
            style_instruction = "ê· í˜• ì¡íŒ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
        
        # ê·œì¹™ ê°€ì´ë“œë¼ì¸ ì²˜ë¦¬
        rules_text = ""
        if rule_guidelines:
            rules_text = "\n".join([f"- {rule}" for rule in rule_guidelines])
        
        # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SEO ìµœì í™”ëœ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ í…ìŠ¤íŠ¸:
{text}

ì£¼ìš” í‚¤ì›Œë“œ: {keywords}

ìš”êµ¬ì‚¬í•­:
- ëª©í‘œ ê¸¸ì´: ì•½ {target_length}ì
- ìŠ¤íƒ€ì¼: {style_instruction}
- SEO ìµœì í™”: í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- êµ¬ì¡°: ì œëª©, ì†Œì œëª©, ë³¸ë¬¸, ê²°ë¡  í¬í•¨
- HTML í˜•ì‹ìœ¼ë¡œ ì‘ì„±

ì¶”ê°€ ê·œì¹™:
{rules_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

```html
<h1>ì œëª©</h1>
<meta name="description" content="ë©”íƒ€ ì„¤ëª…">

<div class="content-intro">
    <p>ì†Œê°œ ë‚´ìš©...</p>
</div>

<div class="ai-keywords">
    <h2>ğŸ¤– AI ì¶”ì²œ í‚¤ì›Œë“œ</h2>
    <ul>
        <li>í‚¤ì›Œë“œ1</li>
        <li>í‚¤ì›Œë“œ2</li>
        ...
    </ul>
</div>

<div class="main-content">
    <h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>
    <p>ì£¼ìš” ë‚´ìš©...</p>
    
    <h2>ğŸ” í•µì‹¬ í¬ì¸íŠ¸</h2>
    <ul>
        <li>í•µì‹¬ í¬ì¸íŠ¸ 1</li>
        <li>í•µì‹¬ í¬ì¸íŠ¸ 2</li>
        ...
    </ul>
    
    <h2>ğŸ’¡ ì‹¤ìš©ì ì¸ íŒ</h2>
    <div class="tips-container">
        <div class="tip-item">
            <h3>íŒ 1</h3>
            <p>íŒ ë‚´ìš©...</p>
        </div>
        ...
    </div>
    
    <h2>ğŸ“Š ìš”ì•½</h2>
    <p>ìš”ì•½ ë‚´ìš©...</p>
</div>

<div class="ai-analysis">
    <h2>ğŸ¤– AI ë¶„ì„</h2>
    <div class="analysis-item">
        <h3>AI ìš”ì•½ (100ì ì´ë‚´)</h3>
        <p>ìš”ì•½ ë‚´ìš©...</p>
    </div>
    <div class="analysis-item">
        <h3>ì‹ ë¢°ë„ í‰ê°€ (5ì  ë§Œì )</h3>
        <p>í‰ì : â­â­â­â­â­ (5/5)</p>
        <p>í‰ê°€ ê·¼ê±°: ...</p>
    </div>
    <div class="analysis-item">
        <h3>SEO ìµœì í™” ì ìˆ˜ (10ì  ë§Œì )</h3>
        <p>í‰ì : ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ (10/10)</p>
        <p>ìµœì í™” ê·¼ê±°: ...</p>
    </div>
</div>
```

ì œëª©ê³¼ ë©”íƒ€ ì„¤ëª…ë„ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”.
"""

        # API í˜¸ì¶œ
        for attempt in range(MAX_RETRIES):
            try:
                response = await client.chat.completions.create(
                    model=settings.openai_model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ SEO ì „ë¬¸ê°€ì´ì ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ê³ í’ˆì§ˆì˜ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=settings.openai_max_tokens,
                    temperature=0.7
                )
                
                generated_content = response.choices[0].message.content.strip()
                
                # HTML íƒœê·¸ ì¶”ì¶œ
                title_match = re.search(r'<h1[^>]*>(.*?)</h1>', generated_content, re.IGNORECASE | re.DOTALL)
                meta_match = re.search(r'<meta[^>]*name="description"[^>]*content="([^"]*)"', generated_content, re.IGNORECASE)
                
                title = title_match.group(1).strip() if title_match else f"{keywords.split(',')[0] if keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'}"
                meta_description = meta_match.group(1).strip() if meta_match else f"{keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                
                # HTML íƒœê·¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                clean_content = re.sub(r'<[^>]+>', '', generated_content)
                word_count = len(clean_content.split())
                
                # AI ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                ai_analysis = _extract_ai_analysis(generated_content)
                
                # ê°€ì´ë“œë¼ì¸ ì ìš© ë¶„ì„
                guidelines_analysis = _analyze_guidelines_application(generated_content, rule_guidelines)
                
                result = {
                    'post': generated_content,
                    'title': title,
                    'meta_description': meta_description,
                    'keywords': keywords,
                    'word_count': word_count,
                    'content_length': content_length,
                    'ai_mode': ai_mode or 'openai',
                    'ai_analysis': ai_analysis,
                    'guidelines_analysis': guidelines_analysis
                }
                
                # ì„±ëŠ¥ ë¡œê¹…
                end_time = time.time()
                logger.info(f"í–¥ìƒëœ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: {end_time - start_time:.2f}ì´ˆ")
                
                return result
                
            except Exception as e:
                logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt == MAX_RETRIES - 1:
                    raise ContentGenerationError(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                await asyncio.sleep(1)
                
    except Exception as e:
        logger.error(f"í–¥ìƒëœ ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        raise ContentGenerationError(f"í–¥ìƒëœ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {str(e)}")

def _extract_ai_analysis(content: str) -> Dict:
    """AI ë¶„ì„ ê²°ê³¼ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    analysis = {
        'ai_summary': '',
        'trust_score': 0,
        'seo_score': 0,
        'trust_reason': '',
        'seo_reason': ''
    }
    
    try:
        # AI ìš”ì•½ ì¶”ì¶œ (ë” ì •í™•í•œ íŒ¨í„´ ë§¤ì¹­)
        summary_patterns = [
            r'<h3>AI ìš”ì•½.*?</h3>\s*<p>(.*?)</p>',
            r'<h3>AI ìš”ì•½.*?</h3>\s*<p>(.*?)(?=<h3>|<div|</div>)',
            r'AI ìš”ì•½.*?<p>(.*?)</p>',
            r'AI ìš”ì•½.*?<p>(.*?)(?=<h3>|<div|</div>)'
        ]
        
        for pattern in summary_patterns:
            summary_match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
            if summary_match:
                summary_text = summary_match.group(1).strip()
                # HTML íƒœê·¸ ì œê±°
                summary_text = re.sub(r'<[^>]+>', '', summary_text)
                analysis['ai_summary'] = summary_text
                break
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ì¶œ
        trust_patterns = [
            r'í‰ì :\s*â­+.*?\((\d+)/5\)',
            r'ì‹ ë¢°ë„.*?\((\d+)/5\)',
            r'ì‹ ë¢°ë„.*?(\d+)/5'
        ]
        
        for pattern in trust_patterns:
            trust_match = re.search(pattern, content, re.IGNORECASE)
            if trust_match:
                analysis['trust_score'] = int(trust_match.group(1))
                break
        
        # SEO ì ìˆ˜ ì¶”ì¶œ
        seo_patterns = [
            r'í‰ì :\s*ğŸ”¥+.*?\((\d+)/10\)',
            r'SEO.*?\((\d+)/10\)',
            r'SEO.*?(\d+)/10'
        ]
        
        for pattern in seo_patterns:
            seo_match = re.search(pattern, content, re.IGNORECASE)
            if seo_match:
                analysis['seo_score'] = int(seo_match.group(1))
                break
        
        # í‰ê°€ ê·¼ê±° ì¶”ì¶œ (ë” ì •í™•í•œ íŒ¨í„´)
        trust_reason_patterns = [
            r'í‰ê°€ ê·¼ê±°:\s*(.*?)(?=<h3>|<div|</div>|$)',
            r'ì‹ ë¢°ë„.*?ê·¼ê±°:\s*(.*?)(?=<h3>|<div|</div>|$)'
        ]
        
        for pattern in trust_reason_patterns:
            trust_reason_match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
            if trust_reason_match:
                reason_text = trust_reason_match.group(1).strip()
                reason_text = re.sub(r'<[^>]+>', '', reason_text)
                analysis['trust_reason'] = reason_text
                break
        
        seo_reason_patterns = [
            r'ìµœì í™” ê·¼ê±°:\s*(.*?)(?=<h3>|<div|</div>|$)',
            r'SEO.*?ê·¼ê±°:\s*(.*?)(?=<h3>|<div|</div>|$)'
        ]
        
        for pattern in seo_reason_patterns:
            seo_reason_match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
            if seo_reason_match:
                reason_text = seo_reason_match.group(1).strip()
                reason_text = re.sub(r'<[^>]+>', '', reason_text)
                analysis['seo_reason'] = reason_text
                break
            
    except Exception as e:
        logger.error(f"AI ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return analysis

async def _create_enhanced_blog_post_with_gemini(text: str, keywords: str, rule_guidelines: Optional[list] = None, content_length: str = "5000", ai_mode: Optional[str] = None) -> Dict:
    """
    Gemini 2.0 Flash APIë¥¼ ì‚¬ìš©í•˜ì—¬ í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        import httpx
        
        # Gemini API í‚¤ í™•ì¸
        from .translator import get_gemini_api_key
        api_key = get_gemini_api_key()
        if not api_key:
            raise ContentGenerationError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì½˜í…ì¸  ê¸¸ì´ ì„¤ì •
        target_length = int(content_length) if content_length.isdigit() else 5000
        
        # ê·œì¹™ ê°€ì´ë“œë¼ì¸ ì²˜ë¦¬
        rules_text = ""
        if rule_guidelines:
            rules_text = "\n".join([f"- {rule}" for rule in rule_guidelines])
        
        # ì½˜í…ì¸  ìŠ¤íƒ€ì¼ë³„ ê°€ì´ë“œë¼ì¸ ì¶”ê°€
        style_guidelines = ""
        if ai_mode:
            if ai_mode == "ë‰´ìŠ¤":
                style_guidelines = """
ë‰´ìŠ¤ ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¼ì¸:
- ê°ê´€ì ì´ê³  ì‚¬ì‹¤ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±
- 5W1H (ëˆ„ê°€, ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„, ì™œ, ì–´ë–»ê²Œ) í¬í•¨
- ìµœì‹ ì„±ê³¼ ì‹œì˜ì„± ê°•ì¡°
- ìš”ì•½: í•µì‹¬ ì‚¬ì‹¤ë§Œ ê°„ê²°í•˜ê²Œ (300ì ì´ë‚´)
- ì£¼ìš” ë‚´ìš©: ì‹œê°„ìˆœ ë˜ëŠ” ì¤‘ìš”ë„ìˆœìœ¼ë¡œ êµ¬ì„±
- ê°œì¸ì  ì˜ê²¬ ë°°ì œ, ê°ê´€ì  ì„œìˆ 
"""
            elif ai_mode == "ë¦¬ë·°":
                style_guidelines = """
ë¦¬ë·° ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¼ì¸:
- ì¥ë‹¨ì ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì„œìˆ 
- ê°œì¸ì  ê²½í—˜ê³¼ ì²´í—˜ ì¤‘ì‹¬
- êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ê·¼ê±° ì œì‹œ
- ìš”ì•½: í•µì‹¬ í‰ê°€ì™€ ì¶”ì²œ ì—¬ë¶€ (400ì ì´ë‚´)
- ì£¼ìš” ë‚´ìš©: ì¥ì , ë‹¨ì , ê²°ë¡  ìˆœìœ¼ë¡œ êµ¬ì„±
- ì†”ì§í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í†¤ ìœ ì§€
"""
            elif ai_mode == "ê°€ì´ë“œ":
                style_guidelines = """
ê°€ì´ë“œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¼ì¸:
- ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…
- ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ ì œê³µ
- ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìƒì„¸íˆ ì„¤ëª…
- ìš”ì•½: í•µì‹¬ ë‹¨ê³„ì™€ ëª©í‘œ (350ì ì´ë‚´)
- ì£¼ìš” ë‚´ìš©: ë‹¨ê³„ë³„ ìˆœì„œëŒ€ë¡œ êµ¬ì„±
- ì²´í¬ë¦¬ìŠ¤íŠ¸ë‚˜ íŒ í¬í•¨
"""
            elif ai_mode == "ìš”ì•½":
                style_guidelines = """
ìš”ì•½ ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¼ì¸:
- í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬
- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì¥
- ìš”ì•½: í•µì‹¬ í¬ì¸íŠ¸ë§Œ ê°„ê²°í•˜ê²Œ (250ì ì´ë‚´)
- ì£¼ìš” ë‚´ìš©: ì¤‘ìš”ë„ìˆœìœ¼ë¡œ í•µì‹¬ë§Œ ë‚˜ì—´
- ë¹ ë¥¸ ì •ë³´ ì „ë‹¬ì— ìµœì í™”
"""
            elif ai_mode == "ë¸”ë¡œê·¸":
                style_guidelines = """
ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¼ì¸:
- ê°œì¸ì  ì¸ì‚¬ì´íŠ¸ì™€ ê²½í—˜ ì¤‘ì‹¬
- ë…ìì™€ì˜ ì†Œí†µí•˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±
- ì‹¤ìš©ì  ì¡°ì–¸ê³¼ ê°œì¸ì  ê²¬í•´ í¬í•¨
- ìš”ì•½: ê°œì¸ì  ê´€ì ê³¼ í•µì‹¬ ë©”ì‹œì§€ (400ì ì´ë‚´)
- ì£¼ìš” ë‚´ìš©: ê²½í—˜ë‹´, ì¸ì‚¬ì´íŠ¸, ì¡°ì–¸ ìˆœìœ¼ë¡œ êµ¬ì„±
- ì¹œê·¼í•˜ê³  ì†”ì§í•œ ì–´ì¡° ìœ ì§€
"""
            elif ai_mode == "enhanced":
                style_guidelines = """
í–¥ìƒëœ ëª¨ë“œ ê°€ì´ë“œë¼ì¸ (2025 Advanced SEO):
- AEO (ë‹µë³€ ì—”ì§„ ìµœì í™”): ê° ì„¹ì…˜ì˜ ì‹œì‘ì€ ì§ˆë¬¸ì— ëŒ€í•œ 40-60ì ë‚´ì™¸ì˜ ëª…í™•í•œ ì§ì ‘ ë‹µë³€(Direct Answer)ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
- AIO (AI Overviews): ëŒ€í™”ì²´ í†¤ì„ ìœ ì§€í•˜ê³ , 'ì œë¡œ í´ë¦­' ê²€ìƒ‰ì„ ìœ„í•´ í•µì‹¬ ì •ë³´ë¥¼ ì „ë©´ì— ë°°ì¹˜í•˜ì„¸ìš”.
- GEO (ìƒì„±í˜• ì—”ì§„ ìµœì í™”): ì£¼ì œë¥¼ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¨ê³ , ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¥¼ ì¸ìš©í•˜ê±°ë‚˜ ì–¸ê¸‰í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì´ì„¸ìš”.
- êµ¬ì¡°: ì§ˆë¬¸(H2/H3) -> ì§ì ‘ ë‹µë³€ -> ìƒì„¸ ì„¤ëª… -> ì˜ˆì‹œ/ë°ì´í„° ìˆœì„œë¡œ êµ¬ì„±í•˜ì„¸ìš”.
- AI ë¶„ì„ê³¼ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ í¬í•¨
- ì‹¬ì¸µì  ë¶„ì„ê³¼ ì „ë¬¸ì  ê´€ì 
- ë‹¤ì–‘í•œ ê´€ì ì—ì„œì˜ ì¢…í•©ì  ë¶„ì„
- ìš”ì•½: AI ë¶„ì„ ê²°ê³¼ì™€ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (500ì ì´ë‚´)
- ì£¼ìš” ë‚´ìš©: ë¶„ì„, ì¸ì‚¬ì´íŠ¸, ì „ë§ ìˆœìœ¼ë¡œ êµ¬ì„±
- ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
"""
        
        # ìš”ì•½ ë° ì£¼ìš” ë‚´ìš© ê°€ì´ë“œë¼ì¸
        summary_guidelines = """
ìš”ì•½ ë° ì£¼ìš” ë‚´ìš© ê°€ì´ë“œë¼ì¸:
- ìš”ì•½: ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±, 500ì ì´ë‚´
- ì£¼ìš” ë‚´ìš©: í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„
- í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- ë…ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°
- ê° ì„¹ì…˜ë³„ë¡œ ëª…í™•í•œ ì œëª© ì‚¬ìš©
"""
        
        # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SEO ìµœì í™”ëœ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ í…ìŠ¤íŠ¸:
{text}

ì£¼ìš” í‚¤ì›Œë“œ: {keywords}

ìš”êµ¬ì‚¬í•­:
- ëª©í‘œ ê¸¸ì´: ì•½ {target_length}ì
- SEO ìµœì í™”: í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- êµ¬ì¡°: ì œëª©, ì†Œì œëª©, ë³¸ë¬¸, ê²°ë¡  í¬í•¨
- HTML í˜•ì‹ìœ¼ë¡œ ì‘ì„±

ì¶”ê°€ ê·œì¹™:
{rules_text}

{style_guidelines}

{summary_guidelines}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

```html
<h1>ì œëª©</h1>
<meta name="description" content="ë©”íƒ€ ì„¤ëª…">

<div class="content-intro">
    <p>ì†Œê°œ ë‚´ìš©...</p>
</div>

<div class="ai-keywords">
    <h2>ğŸ¤– AI ì¶”ì²œ í‚¤ì›Œë“œ</h2>
    <ul>
        <li>í‚¤ì›Œë“œ1</li>
        <li>í‚¤ì›Œë“œ2</li>
        ...
    </ul>
</div>

<div class="main-content">
    <h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>
    <p>ì£¼ìš” ë‚´ìš©...</p>
    
    <h2>ğŸ” í•µì‹¬ í¬ì¸íŠ¸</h2>
    <ul>
        <li>í•µì‹¬ í¬ì¸íŠ¸ 1</li>
        <li>í•µì‹¬ í¬ì¸íŠ¸ 2</li>
        ...
    </ul>
    
    <h2>ğŸ’¡ ì‹¤ìš©ì ì¸ íŒ</h2>
    <div class="tips-container">
        <div class="tip-item">
            <h3>íŒ 1</h3>
            <p>íŒ ë‚´ìš©...</p>
        </div>
        ...
    </div>
    
    <h2>ğŸ“Š ìš”ì•½</h2>
    <p>ìš”ì•½ ë‚´ìš©...</p>
</div>

<div class="ai-analysis">
    <h2>ğŸ¤– AI ë¶„ì„</h2>
    <div class="analysis-item">
        <h3>AI ìš”ì•½ (100ì ì´ë‚´)</h3>
        <p>ìš”ì•½ ë‚´ìš©...</p>
    </div>
    <div class="analysis-item">
        <h3>ì‹ ë¢°ë„ í‰ê°€ (5ì  ë§Œì )</h3>
        <p>í‰ì : â­â­â­â­â­ (5/5)</p>
        <p>í‰ê°€ ê·¼ê±°: ...</p>
    </div>
    <div class="analysis-item">
        <h3>SEO ìµœì í™” ì ìˆ˜ (10ì  ë§Œì )</h3>
        <p>í‰ì : ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ (10/10)</p>
        <p>ìµœì í™” ê·¼ê±°: ...</p>
    </div>
</div>
```

ì œëª©ê³¼ ë©”íƒ€ ì„¤ëª…ë„ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        async with httpx.AsyncClient(timeout=30) as client:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": 8192
                }
            }
            
            response = await client.post(url, json=payload)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    generated_content = result['candidates'][0]['content']['parts'][0]['text'].strip()
                    
                    # HTML íƒœê·¸ ì¶”ì¶œ
                    title_match = re.search(r'<h1[^>]*>(.*?)</h1>', generated_content, re.IGNORECASE | re.DOTALL)
                    meta_match = re.search(r'<meta[^>]*name="description"[^>]*content="([^"]*)"', generated_content, re.IGNORECASE)
                    
                    title = title_match.group(1).strip() if title_match else f"{keywords.split(',')[0] if keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'}"
                    meta_description = meta_match.group(1).strip() if meta_match else f"{keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                    
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    clean_content = re.sub(r'<[^>]+>', '', generated_content)
                    word_count = len(clean_content.split())
                    
                    # AI ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                    ai_analysis = _extract_ai_analysis(generated_content)
                    
                    # ê°€ì´ë“œë¼ì¸ ì ìš© ë¶„ì„
                    guidelines_analysis = _analyze_guidelines_application(generated_content, rule_guidelines)
                    
                    return {
                        'post': generated_content,
                        'title': title,
                        'meta_description': meta_description,
                        'keywords': keywords,
                        'word_count': word_count,
                        'content_length': content_length,
                        'ai_mode': ai_mode or 'gemini_2_0_flash',
                        'ai_analysis': ai_analysis,
                        'guidelines_analysis': guidelines_analysis
                    }
                else:
                    raise ContentGenerationError("Gemini API ì‘ë‹µì— ìƒì„± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                error_detail = response.text
                logger.error(f"Gemini API ì˜¤ë¥˜: {response.status_code} - {error_detail}")
                raise ContentGenerationError(f"Gemini API ì˜¤ë¥˜: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Gemini í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        raise ContentGenerationError(f"Gemini í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

def _generate_enhanced_default_template(text: str, keywords: str, rule_guidelines: Optional[List[str]] = None, content_length: str = "3000", ai_mode: Optional[str] = None) -> Dict:
    """
    í–¥ìƒëœ ê¸°ëŠ¥ì„ í¬í•¨í•œ ê¸°ë³¸ í…œí”Œë¦¿ ìƒì„±
    """
    # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    if not keywords:
        keywords = _extract_default_keywords(text)
    
    # ì œëª© ìƒì„±
    title = f"{keywords.split(',')[0].strip() if keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'}ì— ëŒ€í•œ ìƒì„¸ ê°€ì´ë“œ"
    
    # í…ìŠ¤íŠ¸ ìš”ì•½ (1000ì ì´ë‚´ë¡œ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬)
    def create_summary(text_content, max_length=1000):
        """ì£¼ìš” ë‚´ìš©ì˜ ì „ì²´ í…ìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì••ì¶•í•˜ì—¬ 1000ì ì´ë‚´ë¡œ ì •ë¦¬"""
        if len(text_content) <= max_length:
            return text_content
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text_content)
        summary_sentences = []
        current_length = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ ì¶”ê°€
            if not sentence.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                sentence += '.'
            
            if current_length + len(sentence) <= max_length:
                summary_sentences.append(sentence)
                current_length += len(sentence)
            else:
                break
        
        return ' '.join(summary_sentences)
    
    summary = create_summary(text, 1000)
    
    # ì£¼ìš” ë‚´ìš© ìƒì„± í•¨ìˆ˜
    def create_main_content(text_content, target_length):
        """URL ì…ë ¥ ë˜ëŠ” ì…ë ¥ í…ŒìŠ¤íŠ¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ 'ì½˜í…ì¸  ë¶„ëŸ‰'ì— ë”°ë¼ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¬¸ì¥í˜•ìœ¼ë¡œ ìƒì„±"""
        try:
            target_length_int = int(target_length) if target_length.isdigit() else 3000
            
            # ì½˜í…ì¸  ê¸¸ì´ì— ë”°ë¥¸ ì£¼ìš” ë‚´ìš© ìƒì„±
            if len(text_content) < 500:
                # ì§§ì€ ì½˜í…ì¸ : ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ë¬¸ì¥í˜•ìœ¼ë¡œ ì •ë¦¬
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text_content)
                clean_sentences = []
                for sentence in sentences:
                    sentence = sentence.strip()
                    if sentence and not sentence.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                        sentence += '.'
                    if sentence:
                        clean_sentences.append(sentence)
                return ' '.join(clean_sentences)
            elif len(text_content) < 1500:
                # ì¤‘ê°„ ê¸¸ì´: ìš”ì•½ + ì¶”ê°€ ì„¤ëª…
                return f"{summary} ì´ ë‚´ìš©ì€ {keywords.split(',')[0].strip() if keywords else 'ì£¼ì œ'}ì— ëŒ€í•œ í•µì‹¬ ì •ë³´ë¥¼ ë‹´ê³  ìˆìœ¼ë©°, ë” ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ì„¹ì…˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            else:
                # ê¸´ ì½˜í…ì¸ : êµ¬ì¡°í™”ëœ ì£¼ìš” ë‚´ìš©
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text_content)
                main_sentences = []
                current_length = 0
                max_main_length = min(target_length_int // 3, 800)  # ì£¼ìš” ë‚´ìš©ì€ ì „ì²´ì˜ 1/3 ì´í•˜
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    if not sentence.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                        sentence += '.'
                    
                    if current_length + len(sentence) <= max_main_length:
                        main_sentences.append(sentence)
                        current_length += len(sentence)
                    else:
                        break
                
                main_content = ' '.join(main_sentences)
                if len(main_content) < len(text_content):
                    main_content += f" ì´ ì™¸ì—ë„ {keywords.split(',')[0].strip() if keywords else 'ì£¼ì œ'}ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì •ë³´ì™€ ë¶„ì„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                
                return main_content
        except Exception as e:
            logger.error(f"ì£¼ìš” ë‚´ìš© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return summary
    
    # AI ì¶”ì²œ í‚¤ì›Œë“œ (ëª…ì‚¬ ì¤‘ì‹¬, ìµœëŒ€ 10ê°œ)
    ai_keywords = _extract_ai_keywords(text, keywords)
    
    # í–¥ìƒëœ HTML ì½˜í…ì¸  ìƒì„±
    content = f"""
<h1>{title}</h1>
<meta name="description" content="{keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.">

<div class="content-intro">
    <p>ì´ ê¸€ì—ì„œëŠ” <strong>{keywords}</strong>ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>
</div>

<div class="ai-keywords">
    <h2>ğŸ¤– AI ì¶”ì²œ í‚¤ì›Œë“œ</h2>
    <ul>
        {''.join([f'<li>{keyword.strip()}</li>' for keyword in ai_keywords[:10]])}
    </ul>
</div>

    <div class="main-content">
        <h2>ğŸ“ ìš”ì•½</h2>
        <p>{summary}</p>
        
        <h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>
        <p>{create_main_content(text, content_length)}</p>
    
    <h2>ğŸ” í•µì‹¬ í¬ì¸íŠ¸</h2>
    <ul>
        <li><strong>ì²« ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> {keywords.split(',')[0].strip()}ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì¤‘ìš”ì„±</li>
        <li><strong>ë‘ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ì‹¤ì œ ì ìš© ë°©ë²•ê³¼ ì‚¬ë¡€</li>
        <li><strong>ì„¸ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ì£¼ì˜ì‚¬í•­ê³¼ ëª¨ë²” ì‚¬ë¡€</li>
        <li><strong>ë„¤ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ë¯¸ë˜ ì „ë§ê³¼ ë°œì „ ë°©í–¥</li>
    </ul>
    
    <h2>ğŸ’¡ ì‹¤ìš©ì ì¸ íŒ</h2>
    <div class="tips-container">
        <div class="tip-item">
            <h3>íŒ 1: íš¨ê³¼ì ì¸ í™œìš©</h3>
            <p>{keywords.split(',')[0].strip()}ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
        <div class="tip-item">
            <h3>íŒ 2: ì£¼ì˜ì‚¬í•­</h3>
            <p>ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í”¼í•´ì•¼ í•  í•¨ì •ì— ëŒ€í•´ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
        <div class="tip-item">
            <h3>íŒ 3: ìµœì í™” ë°©ë²•</h3>
            <p>ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
    </div>
    
    <h2>ğŸ“Š ìš”ì•½</h2>
    <p>ì´ ê¸€ì„ í†µí•´ {keywords.split(',')[0].strip()}ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì´ê³ , ì‹¤ì œ ìƒí™©ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì§€ì‹ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.</p>
</div>

    <div class="ai-analysis">
        <h2>ğŸ¤– AI ë¶„ì„</h2>
        <div class="analysis-item">
            <h3>AI ìš”ì•½ (1000ì ì´ë‚´)</h3>
            <p>{summary[:1000]}{'...' if len(summary) > 1000 else ''}</p>
        </div>
    <div class="analysis-item">
        <h3>ì‹ ë¢°ë„ í‰ê°€ (5ì  ë§Œì )</h3>
        <p>í‰ì : â­â­â­â­â­ (5/5)</p>
        <p>í‰ê°€ ê·¼ê±°: ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì™€ ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.</p>
    </div>
    <div class="analysis-item">
        <h3>SEO ìµœì í™” ì ìˆ˜ (10ì  ë§Œì )</h3>
        <p>í‰ì : ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ (10/10)</p>
        <p>ìµœì í™” ê·¼ê±°: í‚¤ì›Œë“œ ìµœì í™”, êµ¬ì¡°í™”ëœ ì½˜í…ì¸ , ë©”íƒ€ ì„¤ëª…, ì‚¬ìš©ì ì¹œí™”ì  ë ˆì´ì•„ì›ƒì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.</p>
    </div>
</div>

<style>
.content-intro {{ background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0; }}
.ai-keywords {{ background: #e8f5e8; padding: 1rem; border-radius: 8px; margin: 1rem 0; }}
.main-content {{ line-height: 1.6; }}
.tips-container {{ display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 1rem; margin: 1rem 0; }}
.tip-item {{ background: #e3f2fd; padding: 1rem; border-radius: 8px; border-left: 4px solid #2196f3; }}
.ai-analysis {{ background: #fff3e0; padding: 1rem; border-radius: 8px; margin: 1rem 0; }}
.analysis-item {{ margin: 1rem 0; padding: 0.5rem; background: white; border-radius: 4px; }}
</style>
"""
    
    # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
    text_content = re.sub(r'<[^>]+>', '', content)
    word_count = len(text_content.split())
    
    # AI ë¶„ì„ ê²°ê³¼
    ai_analysis = {
        'ai_summary': summary[:1000] + ('...' if len(summary) > 1000 else ''),
        'trust_score': 5,
        'seo_score': 10,
        'trust_reason': 'ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì™€ ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.',
        'seo_reason': 'í‚¤ì›Œë“œ ìµœì í™”, êµ¬ì¡°í™”ëœ ì½˜í…ì¸ , ë©”íƒ€ ì„¤ëª…, ì‚¬ìš©ì ì¹œí™”ì  ë ˆì´ì•„ì›ƒì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.'
    }
    
    # ê°€ì´ë“œë¼ì¸ ì ìš© ë¶„ì„
    guidelines_analysis = _analyze_guidelines_application(content, rule_guidelines)
    
    return {
        'post': content,
        'title': title,
        'meta_description': f"{keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        'keywords': keywords,
        'word_count': word_count,
        'content_length': content_length,
        'ai_mode': ai_mode or 'default',
        'ai_analysis': ai_analysis,
        'guidelines_analysis': guidelines_analysis
    }

def _extract_ai_keywords(text: str, existing_keywords: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ AI ì¶”ì²œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ëª…ì‚¬ ì¤‘ì‹¬, ìµœëŒ€ 10ê°œ)
    """
    try:
        # ê¸°ì¡´ í‚¤ì›Œë“œì™€ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©
        combined_text = f"{existing_keywords} {text}"
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ (ëª…ì‚¬ ì¤‘ì‹¬)
        import re
        
        # í•œêµ­ì–´ ëª…ì‚¬ íŒ¨í„´ (ê°„ë‹¨í•œ êµ¬í˜„)
        korean_nouns = re.findall(r'[ê°€-í£]+', combined_text)
        
        # ì˜ì–´ ëª…ì‚¬ íŒ¨í„´
        english_words = re.findall(r'\b[a-zA-Z]{3,}\b', combined_text)
        
        # í‚¤ì›Œë“œ í›„ë³´ ìƒì„±
        candidates = []
        
        # ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ê°€
        if existing_keywords:
            candidates.extend([kw.strip() for kw in existing_keywords.split(',')])
        
        # í•œêµ­ì–´ ëª…ì‚¬ ì¶”ê°€ (ê¸¸ì´ê°€ 2ì ì´ìƒì¸ ê²ƒë§Œ)
        candidates.extend([noun for noun in korean_nouns if len(noun) >= 2])
        
        # ì˜ì–´ ë‹¨ì–´ ì¶”ê°€
        candidates.extend(english_words)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_candidates = list(set(candidates))
        unique_candidates.sort(key=len, reverse=True)  # ê¸´ í‚¤ì›Œë“œ ìš°ì„ 
        
        # ìµœëŒ€ 10ê°œ ë°˜í™˜
        return unique_candidates[:10]
        
    except Exception as e:
        logger.error(f"AI í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
        if existing_keywords:
            return [kw.strip() for kw in existing_keywords.split(',')[:10]]
        return ['í‚¤ì›Œë“œ', 'ë¶„ì„', 'ê°€ì´ë“œ']

def _analyze_guidelines_application(content: str, rules: Optional[List[str]] = None) -> Dict:
    """
    ìƒì„±ëœ ì½˜í…ì¸ ì—ì„œ ê°€ì´ë“œë¼ì¸ ì ìš© ì—¬ë¶€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    analysis = {
        'ai_seo_applied': False,
        'aeo_applied': False,
        'geo_applied': False,
        'aio_applied': False,
        'policy_applied': False,
        'structured_data': False,
        'faq_included': False,
        'keywords_optimized': False,
        'headings_structure': False,
        'links_included': False,
        'images_optimized': False,
        'balance_perspective': False
    }
    
    try:
        # AI SEO ë¶„ì„
        if rules and 'AI_SEO' in rules:
            analysis['ai_seo_applied'] = True
            # êµ¬ì¡°í™”ëœ ë°ì´í„° í™•ì¸
            if 'itemscope' in content or 'schema.org' in content:
                analysis['structured_data'] = True
            # í‚¤ì›Œë“œ ìµœì í™” í™•ì¸ (H1, H2 íƒœê·¸)
            if '<h1' in content and '<h2' in content:
                analysis['keywords_optimized'] = True
            # í—¤ë”© êµ¬ì¡° í™•ì¸
            if '<h1' in content and '<h2' in content and '<h3' in content:
                analysis['headings_structure'] = True
            # ë§í¬ í™•ì¸
            if '<a href' in content or 'http' in content:
                analysis['links_included'] = True
            # ì´ë¯¸ì§€ ìµœì í™” í™•ì¸
            if '<img' in content and 'alt=' in content:
                analysis['images_optimized'] = True
        
        # AEO ë¶„ì„
        if rules and 'AEO' in rules:
            analysis['aeo_applied'] = True
            # FAQ í¬í•¨ í™•ì¸
            if 'FAQ' in content or 'ìì£¼ ë¬»ëŠ” ì§ˆë¬¸' in content or 'Q&A' in content:
                analysis['faq_included'] = True
        
        # GEO ë¶„ì„
        if rules and 'GEO' in rules:
            analysis['geo_applied'] = True
            # êµ¬ì¡°í™”ëœ ë°ì´í„° í™•ì¸
            if 'itemscope' in content or 'schema.org' in content:
                analysis['structured_data'] = True
        
        # AIO ë¶„ì„
        if rules and 'AIO' in rules:
            analysis['aio_applied'] = True
        
        # ì •ì±… ê· í˜• í™•ì¸
        if 'ì¥ë‹¨ì ' in content or 'ê³ ë ¤ì‚¬í•­' in content or 'ì£¼ì˜ì‚¬í•­' in content or 'ë‹¨ì ' in content:
            analysis['balance_perspective'] = True
            analysis['policy_applied'] = True
            
    except Exception as e:
        logger.error(f"ê°€ì´ë“œë¼ì¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return analysis
