"""
í¬ë¡¤ë§ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
í¬ë¡¤ë§ ì„±ê³µë¥ ì„ ì¶”ì í•˜ê³  ë¬¸ì œ ì‚¬ì´íŠ¸ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from pathlib import Path
import logging
from collections import defaultdict, Counter

logger = logging.getLogger(__name__)

class CrawlingMonitor:
    """í¬ë¡¤ë§ ì„±ê³µë¥  ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, stats_file: str = "crawling_stats.json"):
        self.stats_file = Path(stats_file)
        self.stats = self._load_stats()
    
    def _load_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # Counter ê°ì²´ ë³µì›
                    for domain, stats in data.get("site_stats", {}).items():
                        if "common_errors" in stats and isinstance(stats["common_errors"], dict):
                            stats["common_errors"] = Counter(stats["common_errors"])
                    
                    return data
            except Exception as e:
                logger.error(f"í†µê³„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return {
            "total_attempts": 0,
            "successful_crawls": 0,
            "failed_crawls": 0,
            "site_stats": {},
            "recent_attempts": [],
            "problem_sites": [],
            "last_updated": datetime.now().isoformat()
        }
    
    def _save_stats(self):
        """í†µê³„ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # Counter ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            save_data = self.stats.copy()
            for domain, stats in save_data.get("site_stats", {}).items():
                if "common_errors" in stats and isinstance(stats["common_errors"], Counter):
                    stats["common_errors"] = dict(stats["common_errors"])
            
            save_data["last_updated"] = datetime.now().isoformat()
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"í†µê³„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def record_attempt(self, url: str, success: bool, content_length: int = 0, error: str = ""):
        """í¬ë¡¤ë§ ì‹œë„ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
        from urllib.parse import urlparse
        
        domain = urlparse(url).netloc.lower()
        timestamp = datetime.now().isoformat()
        
        # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
        self.stats["total_attempts"] += 1
        if success:
            self.stats["successful_crawls"] += 1
        else:
            self.stats["failed_crawls"] += 1
        
        # ì‚¬ì´íŠ¸ë³„ í†µê³„ ì—…ë°ì´íŠ¸
        if domain not in self.stats["site_stats"]:
            self.stats["site_stats"][domain] = {
                "total_attempts": 0,
                "successful_crawls": 0,
                "failed_crawls": 0,
                "avg_content_length": 0,
                "last_success": None,
                "last_failure": None,
                "common_errors": Counter()
            }
        
        site_stat = self.stats["site_stats"][domain]
        site_stat["total_attempts"] += 1
        
        if success:
            site_stat["successful_crawls"] += 1
            site_stat["last_success"] = timestamp
            if content_length > 0:
                # í‰ê·  ì½˜í…ì¸  ê¸¸ì´ ì—…ë°ì´íŠ¸
                total_length = site_stat["avg_content_length"] * (site_stat["successful_crawls"] - 1) + content_length
                site_stat["avg_content_length"] = total_length / site_stat["successful_crawls"]
        else:
            site_stat["failed_crawls"] += 1
            site_stat["last_failure"] = timestamp
            if error:
                # Counter ê°ì²´ê°€ ì•„ë‹Œ ê²½ìš° Counterë¡œ ë³€í™˜
                if not isinstance(site_stat["common_errors"], Counter):
                    site_stat["common_errors"] = Counter(site_stat["common_errors"])
                site_stat["common_errors"][error] += 1
        
        # ìµœê·¼ ì‹œë„ ê¸°ë¡ (ìµœëŒ€ 100ê°œ)
        recent_attempt = {
            "url": url,
            "domain": domain,
            "success": success,
            "content_length": content_length,
            "error": error,
            "timestamp": timestamp
        }
        
        self.stats["recent_attempts"].append(recent_attempt)
        if len(self.stats["recent_attempts"]) > 100:
            self.stats["recent_attempts"] = self.stats["recent_attempts"][-100:]
        
        # ë¬¸ì œ ì‚¬ì´íŠ¸ ì‹ë³„
        self._identify_problem_sites()
        
        # í†µê³„ ì €ì¥
        self._save_stats()
    
    def _identify_problem_sites(self):
        """ë¬¸ì œê°€ ìˆëŠ” ì‚¬ì´íŠ¸ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
        problem_sites = []
        
        for domain, stats in self.stats["site_stats"].items():
            if stats["total_attempts"] >= 3:  # ìµœì†Œ 3ë²ˆ ì‹œë„
                success_rate = stats["successful_crawls"] / stats["total_attempts"]
                
                if success_rate < 0.5:  # 50% ë¯¸ë§Œ ì„±ê³µë¥ 
                    # Counter ê°ì²´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    common_errors = stats.get("common_errors", {})
                    if isinstance(common_errors, Counter):
                        error_dict = dict(common_errors.most_common(3))
                    else:
                        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ìƒìœ„ 3ê°œ ì„ íƒ
                        sorted_errors = sorted(common_errors.items(), key=lambda x: x[1], reverse=True)
                        error_dict = dict(sorted_errors[:3])
                    
                    problem_sites.append({
                        "domain": domain,
                        "success_rate": success_rate,
                        "total_attempts": stats["total_attempts"],
                        "failed_attempts": stats["failed_crawls"],
                        "last_failure": stats["last_failure"],
                        "common_errors": error_dict
                    })
        
        # ì„±ê³µë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        problem_sites.sort(key=lambda x: x["success_rate"])
        self.stats["problem_sites"] = problem_sites
    
    def get_overall_stats(self) -> Dict[str, Any]:
        """ì „ì²´ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        total = self.stats["total_attempts"]
        if total == 0:
            return {"success_rate": 0, "total_attempts": 0}
        
        success_rate = self.stats["successful_crawls"] / total
        return {
            "success_rate": success_rate,
            "total_attempts": total,
            "successful_crawls": self.stats["successful_crawls"],
            "failed_crawls": self.stats["failed_crawls"]
        }
    
    def get_site_stats(self, domain: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ì‚¬ì´íŠ¸ì˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.stats["site_stats"].get(domain)
    
    def get_problem_sites(self) -> List[Dict[str, Any]]:
        """ë¬¸ì œê°€ ìˆëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.stats["problem_sites"]
    
    def get_recent_attempts(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ìµœê·¼ í¬ë¡¤ë§ ì‹œë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.stats["recent_attempts"][-limit:]
    
    def generate_report(self) -> str:
        """í¬ë¡¤ë§ ì„±ê³µë¥  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        overall = self.get_overall_stats()
        problem_sites = self.get_problem_sites()
        
        report = f"""
ğŸ“Š í¬ë¡¤ë§ ì„±ê³µë¥  ë¦¬í¬íŠ¸
{'='*50}

ğŸ“ˆ ì „ì²´ í†µê³„:
   â€¢ ì´ ì‹œë„: {overall['total_attempts']:,}íšŒ
   â€¢ ì„±ê³µ: {overall['successful_crawls']:,}íšŒ
   â€¢ ì‹¤íŒ¨: {overall['failed_crawls']:,}íšŒ
   â€¢ ì„±ê³µë¥ : {overall['success_rate']:.1%}

ğŸš¨ ë¬¸ì œ ì‚¬ì´íŠ¸ ({len(problem_sites)}ê°œ):
"""
        
        for site in problem_sites[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            report += f"""
   â€¢ {site['domain']}
     - ì„±ê³µë¥ : {site['success_rate']:.1%}
     - ì´ ì‹œë„: {site['total_attempts']}íšŒ
     - ì‹¤íŒ¨: {site['failed_attempts']}íšŒ
     - ì£¼ìš” ì˜¤ë¥˜: {', '.join(site['common_errors'].keys())}
"""
        
        if not problem_sites:
            report += "   âœ… ë¬¸ì œ ì‚¬ì´íŠ¸ ì—†ìŒ\n"
        
        report += f"\nğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {self.stats['last_updated']}"
        
        return report
    
    def cleanup_old_data(self, days: int = 30):
        """ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        # ìµœê·¼ ì‹œë„ì—ì„œ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
        self.stats["recent_attempts"] = [
            attempt for attempt in self.stats["recent_attempts"]
            if datetime.fromisoformat(attempt["timestamp"]) > cutoff_date
        ]
        
        # ì‚¬ì´íŠ¸ë³„ í†µê³„ì—ì„œ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        for domain in list(self.stats["site_stats"].keys()):
            stats = self.stats["site_stats"][domain]
            last_activity = None
            
            if stats["last_success"]:
                last_activity = datetime.fromisoformat(stats["last_success"])
            elif stats["last_failure"]:
                last_activity = datetime.fromisoformat(stats["last_failure"])
            
            if last_activity and last_activity < cutoff_date:
                del self.stats["site_stats"][domain]
        
        self._save_stats()
        logger.info(f"{days}ì¼ ì´ìƒ ëœ ë°ì´í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")

# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
crawling_monitor = CrawlingMonitor() 