"""
í¬ë¡¤ë§ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
í¬ë¡¤ë§ ì„±ê³µë¥ ì„ ì¶”ì í•˜ê³  ë¬¸ì œ ì‚¬ì´íŠ¸ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from pathlib import Path
import logging
from collections import defaultdict, Counter
import threading

logger = logging.getLogger(__name__)

class CrawlingMonitor:
    """í¬ë¡¤ë§ ì„±ê³µë¥  ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, stats_file: str = "crawling_stats.json"):
        self.stats_file = Path(stats_file)
        self.stats = self._load_stats()
        self.lock = threading.Lock()
        self.monitoring = False
        self.monitor_thread = None
    
    def _load_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # Counter ê°ì²´ ë³µì›
                    for domain, stats in data.get("site_stats", {}).items():
                        if "common_errors" in stats and isinstance(stats["common_errors"], dict):
                            stats["common_errors"] = Counter(stats["common_errors"])
                    
                    return data
            except Exception as e:
                logger.error(f"í†µê³„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return {
            "total_attempts": 0,
            "successful_crawls": 0,
            "failed_crawls": 0,
            "site_stats": {},
            "recent_attempts": [],
            "problem_sites": [],
            "performance_metrics": {
                "avg_response_time": 0,
                "success_rate_trend": [],
                "top_performing_sites": [],
                "worst_performing_sites": []
            },
            "last_updated": datetime.now().isoformat()
        }
    
    def _save_stats(self):
        """í†µê³„ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with self.lock:
                # Counter ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                save_data = self.stats.copy()
                for domain, stats in save_data.get("site_stats", {}).items():
                    if "common_errors" in stats and isinstance(stats["common_errors"], Counter):
                        stats["common_errors"] = dict(stats["common_errors"])
                
                save_data["last_updated"] = datetime.now().isoformat()
                with open(self.stats_file, 'w', encoding='utf-8') as f:
                    json.dump(save_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"í†µê³„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def record_attempt(self, url: str, success: bool, content_length: int = 0, error: str = "", response_time: float = 0):
        """í¬ë¡¤ë§ ì‹œë„ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
        from urllib.parse import urlparse
        
        domain = urlparse(url).netloc.lower()
        timestamp = datetime.now().isoformat()
        
        with self.lock:
            # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["total_attempts"] += 1
            if success:
                self.stats["successful_crawls"] += 1
            else:
                self.stats["failed_crawls"] += 1
            
            # ì‚¬ì´íŠ¸ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            if domain not in self.stats["site_stats"]:
                self.stats["site_stats"][domain] = {
                    "total_attempts": 0,
                    "successful_crawls": 0,
                    "failed_crawls": 0,
                    "avg_content_length": 0,
                    "avg_response_time": 0,
                    "last_success": None,
                    "last_failure": None,
                    "common_errors": Counter(),
                    "response_times": [],
                    "success_rate": 0.0
                }
            
            site_stat = self.stats["site_stats"][domain]
            
            # ê¸°ì¡´ ë°ì´í„°ì— response_times í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ì¶”ê°€
            if "response_times" not in site_stat:
                site_stat["response_times"] = []
            if "success_rate" not in site_stat:
                site_stat["success_rate"] = 0.0
            
            site_stat["total_attempts"] += 1
            
            if success:
                site_stat["successful_crawls"] += 1
                site_stat["last_success"] = timestamp
                if content_length > 0:
                    # í‰ê·  ì½˜í…ì¸  ê¸¸ì´ ì—…ë°ì´íŠ¸
                    total_length = site_stat["avg_content_length"] * (site_stat["successful_crawls"] - 1) + content_length
                    site_stat["avg_content_length"] = total_length / site_stat["successful_crawls"]
            else:
                site_stat["failed_crawls"] += 1
                site_stat["last_failure"] = timestamp
                if error:
                    site_stat["common_errors"][error] += 1
            
            # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
            if response_time > 0:
                site_stat["response_times"].append(response_time)
                if len(site_stat["response_times"]) > 100:  # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
                    site_stat["response_times"] = site_stat["response_times"][-100:]
                site_stat["avg_response_time"] = sum(site_stat["response_times"]) / len(site_stat["response_times"])
            
            # ì„±ê³µë¥  ê³„ì‚°
            site_stat["success_rate"] = site_stat["successful_crawls"] / site_stat["total_attempts"]
            
            # ìµœê·¼ ì‹œë„ ê¸°ë¡
            attempt_record = {
                "url": url,
                "domain": domain,
                "success": success,
                "content_length": content_length,
                "error": error,
                "response_time": response_time,
                "timestamp": timestamp
            }
            
            self.stats["recent_attempts"].append(attempt_record)
            
            # ìµœê·¼ ì‹œë„ëŠ” ìµœëŒ€ 1000ê°œë§Œ ìœ ì§€
            if len(self.stats["recent_attempts"]) > 1000:
                self.stats["recent_attempts"] = self.stats["recent_attempts"][-1000:]
            
            # ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
            try:
                self._update_performance_metrics()
            except Exception as e:
                logger.error(f"ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            
            # ë¬¸ì œ ì‚¬ì´íŠ¸ ì‹ë³„
            try:
                self._identify_problem_sites()
            except Exception as e:
                logger.error(f"ë¬¸ì œ ì‚¬ì´íŠ¸ ì‹ë³„ ì˜¤ë¥˜: {e}")
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
            if self.stats["total_attempts"] % 10 == 0:  # 10ë²ˆë§ˆë‹¤ ì €ì¥
                self._save_stats()
    
    def _update_performance_metrics(self):
        """ì„±ëŠ¥ ì§€í‘œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if not self.stats["site_stats"]:
            return
        
        # ì „ì²´ í‰ê·  ì‘ë‹µ ì‹œê°„
        all_response_times = []
        for site_stat in self.stats["site_stats"].values():
            if "response_times" in site_stat and site_stat["response_times"]:
                all_response_times.extend(site_stat["response_times"])
        
        if all_response_times:
            self.stats["performance_metrics"]["avg_response_time"] = sum(all_response_times) / len(all_response_times)
        
        # ì„±ê³µë¥  ê¸°ì¤€ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì •ë ¬
        sites_with_stats = []
        for domain, stats in self.stats["site_stats"].items():
            if stats["total_attempts"] >= 3:  # ìµœì†Œ 3ë²ˆ ì‹œë„í•œ ì‚¬ì´íŠ¸ë§Œ
                sites_with_stats.append({
                    "domain": domain,
                    "success_rate": stats.get("success_rate", 0.0),
                    "total_attempts": stats["total_attempts"],
                    "avg_response_time": stats.get("avg_response_time", 0.0)
                })
        
        # ì„±ê³µë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        sites_with_stats.sort(key=lambda x: x["success_rate"], reverse=True)
        
        # ìƒìœ„/í•˜ìœ„ ì„±ëŠ¥ ì‚¬ì´íŠ¸
        self.stats["performance_metrics"]["top_performing_sites"] = sites_with_stats[:5]
        self.stats["performance_metrics"]["worst_performing_sites"] = sites_with_stats[-5:] if len(sites_with_stats) >= 5 else sites_with_stats
        
        # ì„±ê³µë¥  íŠ¸ë Œë“œ (ìµœê·¼ 10ë²ˆ ì‹œë„ ê¸°ì¤€)
        recent_attempts = self.stats["recent_attempts"][-10:]
        if recent_attempts:
            recent_success = sum(1 for attempt in recent_attempts if attempt["success"])
            recent_success_rate = recent_success / len(recent_attempts)
            self.stats["performance_metrics"]["success_rate_trend"].append({
                "timestamp": datetime.now().isoformat(),
                "success_rate": recent_success_rate,
                "attempts": len(recent_attempts)
            })
            
            # íŠ¸ë Œë“œëŠ” ìµœê·¼ 50ê°œë§Œ ìœ ì§€
            if len(self.stats["performance_metrics"]["success_rate_trend"]) > 50:
                self.stats["performance_metrics"]["success_rate_trend"] = self.stats["performance_metrics"]["success_rate_trend"][-50:]
    
    def _identify_problem_sites(self):
        """ë¬¸ì œ ì‚¬ì´íŠ¸ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
        problem_sites = []
        
        for domain, stats in self.stats["site_stats"].items():
            if stats["total_attempts"] >= 3:  # ìµœì†Œ 3ë²ˆ ì‹œë„í•œ ì‚¬ì´íŠ¸ë§Œ
                success_rate = stats.get("success_rate", 0.0)
                
                # ì„±ê³µë¥ ì´ 50% ë¯¸ë§Œì´ê±°ë‚˜ ìµœê·¼ ì‹¤íŒ¨ê°€ ë§ì€ ì‚¬ì´íŠ¸
                if success_rate < 0.5 or (stats["failed_crawls"] >= 3 and success_rate < 0.7):
                    problem_sites.append({
                        "domain": domain,
                        "success_rate": success_rate,
                        "total_attempts": stats["total_attempts"],
                        "failed_attempts": stats["failed_crawls"],
                        "last_failure": stats["last_failure"],
                        "common_errors": dict(stats["common_errors"].most_common(3))
                    })
        
        # ì‹¤íŒ¨ íšŸìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        problem_sites.sort(key=lambda x: x["failed_attempts"], reverse=True)
        self.stats["problem_sites"] = problem_sites
    
    def get_overall_stats(self) -> Dict[str, Any]:
        """ì „ì²´ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with self.lock:
            total_attempts = self.stats["total_attempts"]
            successful_crawls = self.stats["successful_crawls"]
            failed_crawls = self.stats["failed_crawls"]
            
            success_rate = successful_crawls / total_attempts if total_attempts > 0 else 0
            
            return {
                "success_rate": round(success_rate, 4),
                "total_attempts": total_attempts,
                "successful_crawls": successful_crawls,
                "failed_crawls": failed_crawls,
                "avg_response_time": self.stats["performance_metrics"]["avg_response_time"],
                "last_updated": self.stats["last_updated"]
            }
    
    def get_site_stats(self, domain: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ì‚¬ì´íŠ¸ì˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with self.lock:
            return self.stats["site_stats"].get(domain)
    
    def get_problem_sites(self) -> List[Dict[str, Any]]:
        """ë¬¸ì œ ì‚¬ì´íŠ¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with self.lock:
            return self.stats["problem_sites"]
    
    def get_recent_attempts(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì‹œë„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with self.lock:
            return self.stats["recent_attempts"][-limit:]
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with self.lock:
            return self.stats["performance_metrics"]
    
    def generate_report(self) -> str:
        """í¬ë¡¤ë§ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        with self.lock:
            overall_stats = self.get_overall_stats()
            problem_sites = self.get_problem_sites()
            performance_metrics = self.get_performance_metrics()
            
            report = f"""
ğŸ“Š í¬ë¡¤ë§ ì„±ê³µë¥  ë¦¬í¬íŠ¸
==================================================

ğŸ“ˆ ì „ì²´ í†µê³„:
  â€¢ ì´ ì‹œë„: {overall_stats['total_attempts']}íšŒ
  â€¢ ì„±ê³µ: {overall_stats['successful_crawls']}íšŒ
  â€¢ ì‹¤íŒ¨: {overall_stats['failed_crawls']}íšŒ
  â€¢ ì„±ê³µë¥ : {overall_stats['success_rate']:.1%}
  â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: {overall_stats['avg_response_time']:.2f}ì´ˆ

ğŸš¨ ë¬¸ì œ ì‚¬ì´íŠ¸ ({len(problem_sites)}ê°œ):
"""
            
            for site in problem_sites:
                report += f"""
  â€¢ {site['domain']}
    - ì„±ê³µë¥ : {site['success_rate']:.1%}
    - ì´ ì‹œë„: {site['total_attempts']}íšŒ
    - ì‹¤íŒ¨: {site['failed_attempts']}íšŒ
    - ì£¼ìš” ì˜¤ë¥˜: {', '.join(site['common_errors'].keys())}
"""
            
            if performance_metrics["top_performing_sites"]:
                report += f"""
ğŸ† ìƒìœ„ ì„±ëŠ¥ ì‚¬ì´íŠ¸:
"""
                for site in performance_metrics["top_performing_sites"]:
                    report += f"  â€¢ {site['domain']}: {site['success_rate']:.1%} ì„±ê³µë¥ \n"
            
            report += f"""
ğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {self.stats['last_updated']}
"""
            
            return report
    
    def cleanup_old_data(self, days: int = 30):
        """ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        with self.lock:
            # ìµœê·¼ ì‹œë„ì—ì„œ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
            self.stats["recent_attempts"] = [
                attempt for attempt in self.stats["recent_attempts"]
                if datetime.fromisoformat(attempt["timestamp"]) > cutoff_date
            ]
            
            # ì„±ëŠ¥ ì§€í‘œ íŠ¸ë Œë“œì—ì„œ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
            self.stats["performance_metrics"]["success_rate_trend"] = [
                trend for trend in self.stats["performance_metrics"]["success_rate_trend"]
                if datetime.fromisoformat(trend["timestamp"]) > cutoff_date
            ]
            
            # ì‘ë‹µ ì‹œê°„ ë°ì´í„° ì •ë¦¬ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            for site_stat in self.stats["site_stats"].values():
                if "response_times" in site_stat and len(site_stat["response_times"]) > 100:
                    site_stat["response_times"] = site_stat["response_times"][-100:]
            
            self._save_stats()
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self.monitor_thread.start()
            logger.info("í¬ë¡¤ë§ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("í¬ë¡¤ë§ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            try:
                # ì£¼ê¸°ì ìœ¼ë¡œ ë°ì´í„° ì •ë¦¬
                self.cleanup_old_data()
                
                # ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                self._update_performance_metrics()
                
                # í†µê³„ ì €ì¥
                self._save_stats()
                
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
crawling_monitor = CrawlingMonitor() 