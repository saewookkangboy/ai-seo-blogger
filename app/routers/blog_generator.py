# app/routers/blog_generator.py

from fastapi import APIRouter, HTTPException, Depends, status, File, UploadFile
from sqlalchemy.orm import Session
from typing import Optional, Dict
from fastapi import Request
from fastapi.responses import StreamingResponse, JSONResponse
import io
import openpyxl
from functools import lru_cache
from datetime import datetime, timedelta
import json
import re
import requests # Added for system status test
import time
import asyncio

# ì„œë¹„ìŠ¤ ë° DB ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from app.services.crawler import get_text_from_url
from app.services.translator import translate_text, increase_api_usage_count
from app.services.content_generator import create_blog_post as generate_ai_post
from app.services.content_generator import extract_seo_keywords
from app.services.content_pipeline import content_pipeline, ContentPipelineConfig
from app.services.keyword_manager import keyword_manager
from app.services.content_length_controller import content_length_controller
from app.services.seo_analyzer import seo_analyzer
from app.services.google_docs_service import google_docs_service
from app.services.ai_ethics_evaluator import ai_ethics_evaluator
from app import crud, models, exceptions
from app.database import SessionLocal, engine
from app.schemas import PostRequest, PostResponse, BlogPostResponse, ErrorResponse
from app.utils.logger import setup_logger
from app.services.rules import AI_RULES
from app.config import settings
import openai

logger = setup_logger(__name__)

# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±
models.Base.metadata.create_all(bind=engine)

router = APIRouter(tags=["blog-generation"])

# ë©”ëª¨ë¦¬ ìºì‹œ
api_cache = {}
cache_ttl = 60  # 1ë¶„

def get_cached_data(key: str):
    """ìºì‹œëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if key in api_cache:
        data, timestamp = api_cache[key]
        if datetime.now() - timestamp < timedelta(seconds=cache_ttl):
            return data
        else:
            del api_cache[key]
    return None

def set_cached_data(key: str, data):
    """ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    api_cache[key] = (data, datetime.now())

async def evaluate_and_save_ai_ethics(db_post: models.BlogPost, content: str, title: str, metadata: Optional[Dict] = None) -> Optional[Dict]:
    """
    AI ìœ¤ë¦¬ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Args:
        db_post: ë°ì´í„°ë² ì´ìŠ¤ í¬ìŠ¤íŠ¸ ê°ì²´
        content: í‰ê°€í•  ì½˜í…ì¸ 
        title: ì½˜í…ì¸  ì œëª©
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        if not metadata:
            metadata = {}
        
        # AI ìœ¤ë¦¬ í‰ê°€ ìˆ˜í–‰
        ethics_evaluation = await ai_ethics_evaluator.evaluate_content(
            content,
            title,
            metadata
        )
        
        # í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        db_post.ai_ethics_score = ethics_evaluation['overall_score']
        db_post.ai_ethics_evaluation = ethics_evaluation
        db_post.ai_ethics_evaluated_at = datetime.now()
        
        logger.info(f"AI ìœ¤ë¦¬ í‰ê°€ ì™„ë£Œ (í¬ìŠ¤íŠ¸ ID: {db_post.id}): ì¢…í•© ì ìˆ˜ {ethics_evaluation['overall_score']:.2f}/100")
        
        return ethics_evaluation
        
    except Exception as e:
        logger.error(f"AI ìœ¤ë¦¬ í‰ê°€ ì¤‘ ì˜¤ë¥˜ (í¬ìŠ¤íŠ¸ ID: {db_post.id}): {e}")
        return None

async def archive_blog_post_to_google_docs(blog_post_data: dict, db_post: models.BlogPost) -> Optional[str]:
    """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ Google Docsë¡œ Archive ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        if not settings.google_docs_archive_enabled:
            logger.info("Google Docs Archiveê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return None
        
        # Google Docs ì„œë¹„ìŠ¤ ì¸ì¦
        if not google_docs_service.authenticate():
            logger.warning("Google Docs ì„œë¹„ìŠ¤ ì¸ì¦ ì‹¤íŒ¨")
            return None
        
        # Archive í´ë” ìƒì„± ë˜ëŠ” í™•ì¸
        folder_id = google_docs_service.create_archive_folder(settings.google_docs_archive_folder)
        if not folder_id:
            logger.warning("Archive í´ë” ìƒì„± ì‹¤íŒ¨")
            return None
        
        # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        archive_data = {
            'title': blog_post_data.get('title', db_post.title),
            'content': blog_post_data.get('content', db_post.content),
            'keywords': blog_post_data.get('keywords', db_post.keywords),
            'source_url': blog_post_data.get('source_url', db_post.source_url),
            'ai_mode': blog_post_data.get('ai_mode', db_post.ai_mode),
            'summary': blog_post_data.get('summary', ''),
            'created_at': db_post.created_at.isoformat() if db_post.created_at else datetime.now().isoformat()
        }
        
        # Google Docs ë¬¸ì„œ ìƒì„±
        doc_url = google_docs_service.create_blog_post_document(archive_data, folder_id)
        
        if doc_url:
            logger.info(f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ Archive ì™„ë£Œ: {doc_url}")
            return doc_url
        else:
            logger.warning("Google Docs Archive ìƒì„± ì‹¤íŒ¨")
            return None
            
    except Exception as e:
        logger.error(f"Google Docs Archive ì‹¤íŒ¨: {e}")
        return None

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ì„ ì–»ê¸° ìœ„í•œ ì˜ì¡´ì„± í•¨ìˆ˜
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

async def generate_post_with_progress(req: PostRequest, db: Session):
    """ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•˜ëŠ” ì½˜í…ì¸  ìƒì„± í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)"""
    
    async def progress_generator():
        try:
            # 1ë‹¨ê³„: í¬ë¡¤ë§ ì‹œì‘
            yield f"data: {json.dumps({'step': 1, 'message': 'ì›¹ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'progress': 25})}\n\n"
            await asyncio.sleep(0.1)
            
            original_text = ""
            db_source_url = ""

            # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
            if req.url:
                logger.info(f"URL ì…ë ¥ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: {req.url}")
                db_source_url = req.url
                try:
                    original_text = await get_text_from_url(req.url)
                    if not original_text or not original_text.strip():
                        yield f"data: {json.dumps({'error': 'URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'})}\n\n"
                        return
                except Exception as e:
                    logger.error(f"URL í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    yield f"data: {json.dumps({'error': f'URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}'})}\n\n"
                    return
            elif req.text:
                logger.info("í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
                db_source_url = "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥"
                original_text = req.text
                if not original_text or not original_text.strip():
                    yield f"data: {json.dumps({'error': 'í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'})}\n\n"
                    return
            else:
                yield f"data: {json.dumps({'error': 'URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.'})}\n\n"
                return

            # 2ë‹¨ê³„: ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
            yield f"data: {json.dumps({'step': 2, 'message': 'ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ì¤‘...', 'progress': 35})}\n\n"
            await asyncio.sleep(0.1)
            
            logger.info("ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ë‹¨ê³„ ì‹œì‘")
            logger.info(f"ì›ë³¸ í…ìŠ¤íŠ¸: {original_text[:50]}...")
            
            # ì–¸ì–´ ê°ì§€ (ê°„ë‹¨í•œ êµ¬í˜„)
            korean_chars = len(re.findall(r'[ê°€-í£]', original_text))
            total_chars = len(original_text)
            korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
            
            detected_language = "ko" if korean_ratio > 0.3 else "en"
            logger.info(f"ê°ì§€ëœ ì–¸ì–´: {detected_language}")
            
            if detected_language == "ko":
                logger.info("í•œêµ­ì–´ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                translated_text = original_text
            else:
                logger.info("ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.")
                try:
                    translated_text = await translate_text(original_text, "KO")
                    logger.info("ë²ˆì—­ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
                    translated_text = original_text

            # 3ë‹¨ê³„: SEO í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
            yield f"data: {json.dumps({'step': 3, 'message': 'SEO í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...', 'progress': 45})}\n\n"
            await asyncio.sleep(0.1)
            
            logger.info("SEO í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
            
            # ê¸°ì¡´ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
            existing_keywords = keyword_manager.get_existing_keywords(db)
            
            # ê³ ìœ í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if req.keywords:
                # ì‚¬ìš©ìê°€ ì œê³µí•œ í‚¤ì›Œë“œ ì‚¬ìš©
                extracted_keywords = req.keywords
                logger.info(f"ì‚¬ìš©ì ì œê³µ í‚¤ì›Œë“œ: {extracted_keywords}")
            else:
                # ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ
                extracted_keywords = keyword_manager.extract_unique_keywords(translated_text, existing_keywords)
                logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {extracted_keywords}")

            # 4ë‹¨ê³„: AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±
            yield f"data: {json.dumps({'step': 4, 'message': 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...', 'progress': 65})}\n\n"
            await asyncio.sleep(0.1)
            
            logger.info(f"ì„ íƒëœ RULE: {req.rules}, MODE: {req.ai_mode}, ê¸¸ì´: {req.content_length}")
            
            # ì ìš©í•  ê°€ì´ë“œë¼ì¸ ê²°ì • (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´)
            rule_guidelines = []
            if req.rules:
                if isinstance(req.rules, list):
                    rule_guidelines = [r.strip() for r in req.rules if r]
                else:
                    rule_guidelines = [r.strip() for r in str(req.rules).split(',') if r.strip()]
            logger.info(f"ì ìš©í•  ê°€ì´ë“œë¼ì¸: {rule_guidelines}")
            
            logger.info("AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
            start_time = time.time()
            
            try:
                # AI ëª¨ë¸ ì„ íƒ
                if req.ai_mode == "gemini":
                    result = await generate_ai_post(translated_text, extracted_keywords, rule_guidelines, req.content_length, req.ai_mode)
                else:
                    result = await generate_ai_post(translated_text, extracted_keywords, rule_guidelines, req.content_length, req.ai_mode)
                
                generation_time = time.time() - start_time
                logger.info(f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ìƒì„±ëœ ì½˜í…ì¸  ê¸¸ì´: {len(result['post'])}ì, ì†Œìš”ì‹œê°„: {generation_time:.2f}ì´ˆ)")
                
                # 5ë‹¨ê³„: ì½˜í…ì¸  ê¸¸ì´ ì¡°ì • (ìƒˆë¡œìš´ ê¸°ëŠ¥)
                yield f"data: {json.dumps({'step': 5, 'message': 'ì½˜í…ì¸  ê¸¸ì´ ìµœì í™” ì¤‘...', 'progress': 75})}\n\n"
                await asyncio.sleep(0.1)
                
                # ê¸¸ì´ ê²€ì¦ ë° ì¡°ì •
                length_report = content_length_controller.generate_length_report(result['post'], req.content_length)
                if not length_report['is_acceptable']:
                    logger.info(f"ì½˜í…ì¸  ê¸¸ì´ ì¡°ì • í•„ìš”: {length_report['recommendation']}")
                    adjusted_content = content_length_controller.adjust_content_length(result['post'], req.content_length)
                    result['post'] = adjusted_content
                    logger.info("ì½˜í…ì¸  ê¸¸ì´ ì¡°ì • ì™„ë£Œ")
                
                # 6ë‹¨ê³„: SEO ë¶„ì„ (ë°±ê·¸ë¼ìš´ë“œ)
                yield f"data: {json.dumps({'step': 6, 'message': 'SEO ë¶„ì„ ì¤‘...', 'progress': 85})}\n\n"
                await asyncio.sleep(0.1)
                
                logger.info("SEO ë¶„ì„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)")
                
                async def run_seo_analysis():
                    try:
                        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                        keyword_list = [kw.strip() for kw in extracted_keywords.split(',') if kw.strip()]
                        
                        # SEO ë¶„ì„ ì‹¤í–‰
                        seo_result = await seo_analyzer.analyze_content(
                            result['post'], 
                            db_source_url, 
                            keyword_list
                        )
                        
                        logger.info(f"SEO ë¶„ì„ ì™„ë£Œ - ì ìˆ˜: {seo_result.overall_score}")
                        
                        # SEO ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ê°œì„  ì œì•ˆ
                        if seo_result.overall_score < 80:
                            logger.warning(f"SEO ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤: {seo_result.overall_score}")
                            logger.info("ê°œì„  ì œì•ˆ: " + ", ".join(seo_result.recommendations[:3]))
                        
                    except Exception as e:
                        logger.error(f"SEO ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ SEO ë¶„ì„ ì‹¤í–‰
                asyncio.create_task(run_seo_analysis())
                logger.info("SEO ë¶„ì„ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤")

                # 7ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                yield f"data: {json.dumps({'step': 7, 'message': 'ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...', 'progress': 95})}\n\n"
                await asyncio.sleep(0.1)
                
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì— í¬ìŠ¤íŠ¸ ì €ì¥ ì‹œì‘")
                
                # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
                word_count = content_length_controller.count_words(result['post'])
                
                # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
                blog_post = models.BlogPost(
                    title=result['title'],
                    original_url=db_source_url,
                    keywords=extracted_keywords,
                    content_html=result['post'],
                    meta_description=result.get('meta_description', ''),
                    word_count=word_count,
                    content_length=req.content_length
                )
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                db.add(blog_post)
                db.commit()
                db.refresh(blog_post)
                
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ID: {blog_post.id})")

                # 7.5ë‹¨ê³„: AI ìœ¤ë¦¬ í‰ê°€
                yield f"data: {json.dumps({'step': 7.5, 'message': 'AI ìœ¤ë¦¬ í‰ê°€ ì¤‘...', 'progress': 97})}\n\n"
                await asyncio.sleep(0.1)
                
                try:
                    # AI ìœ¤ë¦¬ í‰ê°€ ìˆ˜í–‰
                    metadata = {
                        'ai_mode': req.ai_mode,
                        'keywords': extracted_keywords,
                        'created_at': blog_post.created_at.isoformat() if blog_post.created_at else None
                    }
                    ethics_evaluation = await ai_ethics_evaluator.evaluate_content(
                        result['post'],
                        result['title'],
                        metadata
                    )
                    
                    # í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    blog_post.ai_ethics_score = ethics_evaluation['overall_score']
                    blog_post.ai_ethics_evaluation = ethics_evaluation
                    blog_post.ai_ethics_evaluated_at = datetime.now()
                    db.commit()
                    db.refresh(blog_post)
                    
                    logger.info(f"AI ìœ¤ë¦¬ í‰ê°€ ì™„ë£Œ: ì¢…í•© ì ìˆ˜ {ethics_evaluation['overall_score']:.2f}/100")
                    
                except Exception as e:
                    logger.error(f"AI ìœ¤ë¦¬ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    # í‰ê°€ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

                # 8ë‹¨ê³„: ì™„ë£Œ
                yield f"data: {json.dumps({'step': 8, 'message': 'ì™„ë£Œ!', 'progress': 100})}\n\n"
                await asyncio.sleep(0.1)
                
                # ìµœì¢… ê²°ê³¼ ì „ì†¡
                final_result = {
                    'success': True,
                    'message': 'ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'data': {
                        'id': blog_post.id,
                        'title': result['title'],
                        'content': result['post'],
                        'keywords': extracted_keywords,
                        'source_url': db_source_url,
                        'created_at': blog_post.created_at.isoformat(),
                        'ai_mode': req.ai_mode,
                        'length_report': length_report,
                        'ai_ethics_score': blog_post.ai_ethics_score,
                        'ai_ethics_evaluation': blog_post.ai_ethics_evaluation
                    }
                }
                
                yield f"data: {json.dumps(final_result)}\n\n"
                
            except Exception as e:
                logger.error(f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                yield f"data: {json.dumps({'error': f'ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}'})}\n\n"
                
        except Exception as e:
            logger.error(f"ì§„í–‰ ìƒí™© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"data: {json.dumps({'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'})}\n\n"

@router.post("/generate-post-stream")
async def generate_post_stream_endpoint(req: PostRequest, db: Session = Depends(get_db)):
    """ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ ì „ì†¡í•˜ëŠ” ì½˜í…ì¸  ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (SSE)"""
    return StreamingResponse(
        generate_post_with_progress(req, db),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"},
    )

@router.post("/generate-post-pipeline")
async def generate_post_pipeline_endpoint(req: PostRequest, db: Session = Depends(get_db)):
    """ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # íŒŒì´í”„ë¼ì¸ ì„¤ì • êµ¬ì„±
        config = ContentPipelineConfig(
            use_smart_crawler=True,
            target_language="ko",
            content_length=req.content_length or "3000",
            ai_mode=req.ai_mode,
            enable_seo_analysis=True,
            enable_caching=True
        )
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await content_pipeline.execute_pipeline(
            url=req.url or "",
            text=req.text or "",
            config=config
        )
        
        if not result["success"]:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=result["error"]
            )
        
        # ê²°ê³¼ì—ì„œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì¶”ì¶œ
        blog_post = result["results"]["content_generation"]["blog_post"]
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        db_post = crud.create_post(
            db=db,
            title=blog_post.get("title", "AI ìƒì„± ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸"),
            content=blog_post.get("content", ""),
            keywords=blog_post.get("keywords", ""),
            source_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
            ai_mode=req.ai_mode or "default"
        )
        
        return PostResponse(
            success=True,
            post_id=db_post.id,
            title=db_post.title,
            content=db_post.content,
            keywords=db_post.keywords,
            source_url=db_post.source_url,
            pipeline_id=result.get("pipeline_id"),
            metadata=result.get("metadata")
        )
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ë¸”ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.post("/generate-post-pipeline-stream")
async def generate_post_pipeline_stream_endpoint(req: PostRequest, db: Session = Depends(get_db)):
    """ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    async def pipeline_progress_generator():
        try:
            # íŒŒì´í”„ë¼ì¸ ì„¤ì • êµ¬ì„±
            config = ContentPipelineConfig(
                use_smart_crawler=True,
                target_language="ko",
                content_length=req.content_length or "3000",
                ai_mode=req.ai_mode,
                enable_seo_analysis=True,
                enable_caching=True
            )
            
            # íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
            async for progress in content_pipeline.execute_pipeline_with_progress(
                url=req.url or "",
                text=req.text or "",
                config=config
            ):
                yield f"data: {json.dumps(progress, ensure_ascii=False)}\n\n"
                
                # ì™„ë£Œë˜ë©´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                if progress.get("step") == 7 and "result" in progress:
                    try:
                        blog_post = progress["result"]["blog_post"]
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        db_post = crud.create_post(
                            db=db,
                            title=blog_post.get("title", "AI ìƒì„± ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸"),
                            content=blog_post.get("content", ""),
                            keywords=progress["result"]["keywords"],
                            source_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
                            ai_mode=req.ai_mode or "default"
                        )
                        
                        # ì™„ë£Œ ë©”ì‹œì§€ ì „ì†¡
                        yield f"data: {json.dumps({'completed': True, 'post_id': db_post.id}, ensure_ascii=False)}\n\n"
                        
                    except Exception as e:
                        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                        yield f"data: {json.dumps({'error': f'ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}'}, ensure_ascii=False)}\n\n"
                
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield f"data: {json.dumps({'error': f'íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(pipeline_progress_generator(), media_type="text/plain")

@router.get("/pipeline-status/{pipeline_id}")
async def get_pipeline_status(pipeline_id: str):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        status = content_pipeline.get_pipeline_status(pipeline_id)
        return {"pipeline_id": pipeline_id, "status": status.value}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )

@router.delete("/pipeline/{pipeline_id}")
async def cancel_pipeline(pipeline_id: str):
    """íŒŒì´í”„ë¼ì¸ì„ ì·¨ì†Œí•©ë‹ˆë‹¤."""
    try:
        success = content_pipeline.cancel_pipeline(pipeline_id)
        if success:
            return {"message": "íŒŒì´í”„ë¼ì¸ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", "pipeline_id": pipeline_id}
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="íŒŒì´í”„ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì´í”„ë¼ì¸ ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}"
        )

@router.get("/generate-post-stream")
async def generate_post_stream_get_endpoint(
    url: Optional[str] = None,
    text: Optional[str] = None,
    rules: Optional[str] = None,
    ai_mode: Optional[str] = None,
    content_length: Optional[str] = None,
    policy_auto: Optional[bool] = None,
    db: Session = Depends(get_db)
):
    """GET ìš”ì²­ìœ¼ë¡œ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ ì „ì†¡í•˜ëŠ” ì½˜í…ì¸  ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    # GET íŒŒë¼ë¯¸í„°ë¥¼ PostRequest ê°ì²´ë¡œ ë³€í™˜
    req = PostRequest(
        url=url,
        text=text,
        rules=rules.split(',') if rules else [],
        ai_mode=ai_mode,
        content_length=content_length,
        policy_auto=policy_auto or False
    )
    return await generate_post_with_progress(req, db)

@router.post("/generate-post", response_model=PostResponse)
async def generate_post_endpoint(req: PostRequest, db: Session = Depends(get_db)):
    """
    URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    logger.info("=== generate-post ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    try:
        original_text = ""
        db_source_url = ""

        # 1. ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (ê°œì„ ëœ ê²€ì¦)
        if req.url:
            logger.info(f"URL ì…ë ¥ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: {req.url}")
            db_source_url = req.url
            input_type = "url"
            try:
                original_text = await get_text_from_url(req.url)
                if not original_text or not original_text.strip():
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
            except Exception as e:
                logger.error(f"URL í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"
                )
        elif req.text:
            logger.info("í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
            db_source_url = "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥"
            input_type = "text"
            original_text = req.text
            if not original_text or not original_text.strip():
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                )
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST, 
                detail="URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤."
            )

        # 2. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ (ê°œì„ ëœ ë¡œì§)
        logger.info("ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ë‹¨ê³„ ì‹œì‘")
        logger.info(f"ì›ë³¸ í…ìŠ¤íŠ¸: {original_text[:100]}...")
        
        # ì–¸ì–´ ê°ì§€
        from app.services.translator import detect_language
        detected_lang = await detect_language(original_text)
        logger.info(f"ê°ì§€ëœ ì–¸ì–´: {detected_lang}")
        
        # ë²ˆì—­ í•„ìš” ì—¬ë¶€ íŒë‹¨
        translated_text = original_text
        if detected_lang and detected_lang != "ko":
            logger.info("ë²ˆì—­ì´ í•„ìš”í•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            try:
                translated_text = await translate_text(original_text)
                logger.info(f"ë²ˆì—­ ì™„ë£Œ (ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(translated_text)}ì)")
                logger.info(f"ë²ˆì—­ëœ í…ìŠ¤íŠ¸: {translated_text[:100]}...")
            except Exception as e:
                logger.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                logger.warning("ë²ˆì—­ ì‹¤íŒ¨ë¡œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                translated_text = original_text
        else:
            logger.info("í•œêµ­ì–´ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # 3. AIë¡œ SEO í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬)
        logger.info("SEO í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
        extracted_keywords = ""
        try:
            extracted_keywords = await extract_seo_keywords(translated_text)
            logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {extracted_keywords}")
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
            extracted_keywords = "AI, ê¸°ìˆ , ë¶„ì„, ê°œë°œ, ì‹œìŠ¤í…œ"
            logger.warning("ê¸°ë³¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # 3-1. ì„ íƒëœ RULE/ëª¨ë“œ í™•ì¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        selected_rules = req.rules or []
        selected_mode = req.ai_mode or "ë¸”ë¡œê·¸"  # ê¸°ë³¸ê°’ ì„¤ì •
        content_length = req.content_length or "3000"
        policy_auto = req.policy_auto or False
        logger.info(f"ì„ íƒëœ RULE: {selected_rules}, MODE: {selected_mode}, ê¸¸ì´: {content_length}")

        # 3-2. RULE/ëª¨ë“œì— ë”°ë¥¸ ê°€ì´ë“œë¼ì¸ í…ìŠ¤íŠ¸ ìƒì„±
        rule_guidelines = []
        try:
            for rule in selected_rules:
                if rule in AI_RULES:
                    if isinstance(AI_RULES[rule], list):
                        rule_guidelines.extend(AI_RULES[rule])
            if selected_mode and selected_mode in AI_RULES.get("AI_MODE", {}):
                rule_guidelines.extend(AI_RULES["AI_MODE"][selected_mode])
            # POLICY ìë™ ì ìš©
            if policy_auto and "POLICY" in AI_RULES:
                rule_guidelines.extend(AI_RULES["POLICY"])
            logger.info(f"ì ìš©í•  ê°€ì´ë“œë¼ì¸: {rule_guidelines}")
        except Exception as e:
            logger.error(f"ê°€ì´ë“œë¼ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # 4. AIë¡œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬)
        logger.info("AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
        start_time = time.time()
        try:
            generation_result = await generate_ai_post(
                translated_text, 
                extracted_keywords, 
                rule_guidelines=rule_guidelines,
                content_length=content_length,
                ai_mode=selected_mode,
                input_type=input_type
            )
            generated_content = generation_result.get('post', generation_result.get('content', ''))
            
            # ì¶”ê°€ ë©”íŠ¸ë¦­ ë° ì ìˆ˜ ì¶”ì¶œ
            metrics = generation_result.get('metrics', {})
            score = generation_result.get('score', 0)
            evaluation = generation_result.get('evaluation', '')
            ai_analysis = generation_result.get('ai_analysis', {})
            generation_time = time.time() - start_time
            logger.info(f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ìƒì„±ëœ ì½˜í…ì¸  ê¸¸ì´: {len(generated_content)}ì, ì†Œìš”ì‹œê°„: {generation_time:.2f}ì´ˆ)")
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì— ê¸°ë¡
            try:
                from app.services.performance_monitor import performance_monitor
                performance_monitor.record_content_generation_time(generation_time)
            except Exception as e:
                logger.warning(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            
            # ìƒì„±ëœ ì½˜í…ì¸  ê²€ì¦
            if not generated_content or len(generated_content.strip()) < 100:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="ìƒì„±ëœ ì½˜í…ì¸ ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                )
        except Exception as e:
            logger.error(f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
        
        # 5. SEO ë¶„ì„ ìˆ˜í–‰ (ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)
        logger.info("SEO ë¶„ì„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)")
        try:
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ SEO ë¶„ì„ ì‹¤í–‰
            import asyncio
            async def run_seo_analysis():
                try:
                    from app.services.seo_analyzer import seo_analyzer
                    return await seo_analyzer.analyze_content(generated_content, extracted_keywords)
                except Exception as e:
                    logger.error(f"ë°±ê·¸ë¼ìš´ë“œ SEO ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                    return None
            
            # ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ ì‹¤í–‰ (ì‘ë‹µ ì§€ì—° ì—†ìŒ)
            asyncio.create_task(run_seo_analysis())
            logger.info("SEO ë¶„ì„ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"SEO ë¶„ì„ íƒœìŠ¤í¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # 6. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬)
        try:
            title = crud.extract_title_from_html(generated_content)
            meta_description = crud.extract_meta_description_from_html(generated_content)
            word_count = crud._count_words(generated_content)
            
            # ë©”íƒ€ë°ì´í„° ê²€ì¦
            if not title:
                title = f"{extracted_keywords.split(',')[0] if extracted_keywords else 'AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'}"
            if not meta_description:
                meta_description = f"{extracted_keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            title = "AI ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸"
            meta_description = "AIê°€ ìƒì„±í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            word_count = len(generated_content.split())

        # 7. ìƒì„±ëœ ì½˜í…ì¸ ì— ì›ë¬¸ ë§í¬ ì¶”ê°€ (URL ì…ë ¥ ì‹œì—ë§Œ)
        final_post_with_source = generated_content
        if req.url:
            source_link_html = f'<hr><p><br><strong>ì›ë¬¸ ë§í¬ : </strong><a href="{req.url}" target="_blank" rel="noopener noreferrer">{req.url}</a></p>'
            final_post_with_source += source_link_html
        
        # 8. DBì— ìµœì¢…ë³¸ ì €ì¥ (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬)
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ì— í¬ìŠ¤íŠ¸ ì €ì¥ ì‹œì‘")
        try:
            saved_post = crud.create_blog_post(
                db=db,
                title=title,
                original_url=db_source_url,
                keywords=extracted_keywords,
                content_html=final_post_with_source,
                meta_description=meta_description,
                word_count=word_count,
                content_length=content_length
            )
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ID: {saved_post.id})")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨í–ˆì§€ë§Œ ì‘ë‹µì€ ë°˜í™˜í•©ë‹ˆë‹¤.")

        return PostResponse(
            success=True,
            message="ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data={
                "id": saved_post.id if 'saved_post' in locals() else None,
                "title": title,
                "content": final_post_with_source,
                "keywords": extracted_keywords,
                "source_url": db_source_url,
                "created_at": saved_post.created_at.isoformat() if 'saved_post' in locals() else datetime.now().isoformat(),
                "ai_mode": selected_mode,
                "metrics": metrics,
                "score": score,
                "evaluation": evaluation,
                "ai_analysis": ai_analysis
            }
        )
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        logger.error(f"generate-post ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.post("/generate-post-gemini", response_model=PostResponse)
async def generate_post_gemini(req: PostRequest, db: Session = Depends(get_db)):
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger.info("Gemini ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not req.url and not req.text:
            raise HTTPException(status_code=400, detail="URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        original_text = ""
        if req.url:
            original_text = await get_text_from_url(req.url)
            if not original_text:
                raise HTTPException(status_code=400, detail="URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            original_text = req.text
        
        # ë²ˆì—­ (Gemini 2.0 Flash ì‚¬ìš©)
        translated_text = await translate_text(original_text, "ko")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (Gemini 2.0 Flash ì§€ì›)
        keywords = await extract_seo_keywords(translated_text)
        
        # AI ê·œì¹™ ì ìš©
        rule_guidelines = []
        if req.rules:
            if isinstance(req.rules, str):
                rule_guidelines = [rule.strip() for rule in req.rules.split(',')]
            elif isinstance(req.rules, list):
                rule_guidelines = [rule.strip() for rule in req.rules]
            else:
                rule_guidelines = []
        
        # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±
        result = await generate_ai_post(
            text=translated_text,
            keywords=keywords,
            rule_guidelines=rule_guidelines,
            content_length=req.content_length or "3000",
            ai_mode=req.ai_mode
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        db_post = crud.create_blog_post(
            db=db,
            title=result.get('title', ''),
            original_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
            keywords=result.get('keywords', keywords),
            content_html=result.get('content', ''),
            content_length=req.content_length or "3000"
        )
        
        return PostResponse(
            success=True,
            message="Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data={
                "id": db_post.id,
                "title": db_post.title,
                "content": db_post.content_html,
                "keywords": db_post.keywords,
                "source_url": db_post.original_url,
                "created_at": db_post.created_at.isoformat(),
                "ai_mode": req.ai_mode or "gemini"
            }
        )
        
    except Exception as e:
        logger.error(f"Gemini ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")




@router.post("/generate-post-gemini-2-flash", response_model=PostResponse)
async def generate_post_gemini_2_flash(req: PostRequest, db: Session = Depends(get_db)):
    """
    Gemini 2.0 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ì•ˆì •í™” ë²„ì „)
    """
    logger.info("=== generate-post-gemini-2-flash ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not req.url and not req.text:
            raise HTTPException(status_code=400, detail="URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            
        # ... (existing logic for gemini 2 flash) ...
        # I will just add the new endpoints at the end of the file.

        if not req.url and not req.text:
            logger.error("âŒ URL ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ ëˆ„ë½")
            raise HTTPException(status_code=400, detail="URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¤€ë¹„ (í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°)
        logger.info("ğŸŒ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¤€ë¹„")
        original_text = ""
        if req.url:
            logger.info(f"ğŸ”— URL ì…ë ¥: {req.url}")
            original_text = f"{req.url}ì—ì„œ ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ìš©í•œ ì½˜í…ì¸ ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        else:
            logger.info("ğŸ“ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì‚¬ìš©")
            original_text = req.text
        
        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ: {len(original_text)}ì")
        
        # 2ë‹¨ê³„: ë²ˆì—­ ê±´ë„ˆë›°ê¸° (í–¥ìƒëœ ëª¨ë“œì—ì„œëŠ” ë²ˆì—­ ì—†ì´ ì§„í–‰)
        logger.info("ğŸŒ 2ë‹¨ê³„: ë²ˆì—­ ê±´ë„ˆë›°ê¸°")
        translated_text = original_text
        logger.info(f"âœ… ë²ˆì—­ ê±´ë„ˆë›°ê¸° ì™„ë£Œ: {len(translated_text)}ì")
        
        # 3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
        logger.info("ğŸ”‘ 3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ")
        from app.services.content_generator import _extract_default_keywords
        keywords = _extract_default_keywords(translated_text)
        logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
        
        # 4ë‹¨ê³„: ë¡œì»¬ í…œí”Œë¦¿ ìƒì„± (ë¹ ë¥¸ ì‘ë‹µ)
        logger.info("ğŸ¤– 4ë‹¨ê³„: ë¡œì»¬ í…œí”Œë¦¿ ìƒì„±")
        
        # ê°„ë‹¨í•œ HTML í…œí”Œë¦¿ ìƒì„±
        title = f"{keywords.split(',')[0].strip()}ì— ëŒ€í•œ ìƒì„¸ ê°€ì´ë“œ"
        content_length = int(req.content_length) if req.content_length and req.content_length.isdigit() else 1000
        
        # í…ìŠ¤íŠ¸ ìš”ì•½
        summary = translated_text[:200] + "..." if len(translated_text) > 200 else translated_text
        
        # HTML ì½˜í…ì¸  ìƒì„±
        content = f"""
<h1>{title}</h1>
<meta name="description" content="{keywords}ì— ëŒ€í•œ í¬ê´„ì ì¸ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.">

<div class="content-intro">
    <p>ì´ ê¸€ì—ì„œëŠ” <strong>{keywords}</strong>ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>
</div>

<div class="content-main">
    <h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>
    <p>{summary}</p>
    
    <h2>ğŸ” í•µì‹¬ í¬ì¸íŠ¸</h2>
    <ul>
        <li><strong>ì²« ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> {keywords.split(',')[0].strip()}ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì¤‘ìš”ì„±</li>
        <li><strong>ë‘ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ì‹¤ì œ ì ìš© ë°©ë²•ê³¼ ì‚¬ë¡€</li>
        <li><strong>ì„¸ ë²ˆì§¸ ì¤‘ìš”í•œ í¬ì¸íŠ¸:</strong> ì£¼ì˜ì‚¬í•­ê³¼ ëª¨ë²” ì‚¬ë¡€</li>
    </ul>
    
    <h2>ğŸ’¡ ì‹¤ìš©ì ì¸ íŒ</h2>
    <div class="tips-container">
        <div class="tip-item">
            <h3>íŒ 1: íš¨ê³¼ì ì¸ í™œìš©</h3>
            <p>{keywords.split(',')[0].strip()}ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
        <div class="tip-item">
            <h3>íŒ 2: ì£¼ì˜ì‚¬í•­</h3>
            <p>ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í”¼í•´ì•¼ í•  í•¨ì •ì— ëŒ€í•´ ì•Œì•„ë³´ì„¸ìš”.</p>
        </div>
    </div>
    
    <h2>ğŸ“Š ìš”ì•½</h2>
    <p>ì´ ê¸€ì„ í†µí•´ {keywords.split(',')[0].strip()}ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì´ê³ , ì‹¤ì œ ìƒí™©ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì§€ì‹ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.</p>
</div>

<style>
.content-intro {{ background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0; }}
.content-main {{ line-height: 1.6; }}
.tips-container {{ display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1rem 0; }}
.tip-item {{ background: #e3f2fd; padding: 1rem; border-radius: 8px; border-left: 4px solid #2196f3; }}
</style>
"""
        
        # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        import re
        text_content = re.sub(r'<[^>]+>', '', content)
        word_count = len(text_content.split())
        
        logger.info("âœ… ë¡œì»¬ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ")
        
        # 5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        logger.info("ğŸ’¾ 5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        db_post = crud.create_blog_post(
            db=db,
            title=title,
            original_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
            keywords=keywords,
            content_html=content,
            content_length=req.content_length or "1000"
        )
        logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: í¬ìŠ¤íŠ¸ ID {db_post.id}")
        
        logger.info("ğŸ‰ Gemini 2.0 Flash ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")
        logger.info(f"ğŸ“ˆ ìµœì¢… í†µê³„: ì œëª©={db_post.title}, í‚¤ì›Œë“œ={db_post.keywords}, ìƒì„±ì‹œê°„={db_post.created_at}")
        
        return PostResponse(
            success=True,
            message="Gemini 2.0 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data={
                "id": db_post.id,
                "title": db_post.title,
                "content": db_post.content_html,
                "keywords": db_post.keywords,
                "source_url": db_post.original_url,
                "created_at": db_post.created_at.isoformat(),
                "ai_mode": req.ai_mode or "gemini_2_0_flash",
                "word_count": word_count
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ Gemini 2.0 Flash ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/posts", response_model=list[BlogPostResponse])
async def get_posts(
    skip: int = 0, 
    limit: int = 100, 
    db: Session = Depends(get_db)
):
    """
    ëª¨ë“  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    logger.info(f"í¬ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹œì‘: skip={skip}, limit={limit}")
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"posts_{skip}_{limit}"
    
    # ìºì‹œëœ ë°ì´í„° í™•ì¸
    cached_data = get_cached_data(cache_key)
    if cached_data:
        logger.info("ìºì‹œëœ í¬ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©")
        return cached_data
    
    try:
        posts = crud.get_blog_posts(db, skip=skip, limit=limit)
        logger.info(f"í¬ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {len(posts)}ê°œ í¬ìŠ¤íŠ¸")
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        set_cached_data(cache_key, posts)
        
        return posts
    except Exception as e:
        logger.error(f"í¬ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í¬ìŠ¤íŠ¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

@router.get("/posts/{post_id}", response_model=BlogPostResponse)
async def get_post(post_id: int, db: Session = Depends(get_db)):
    """
    íŠ¹ì • ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        post = crud.get_blog_post_by_id(db, post_id)
        if not post:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        return post
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í¬ìŠ¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {post_id}): {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="í¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )

@router.delete("/posts/{post_id}")
async def delete_post(post_id: int, db: Session = Depends(get_db)):
    """
    íŠ¹ì • ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    try:
        success = crud.delete_blog_post(db, post_id)
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ì‚­ì œí•  í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        return {"message": "í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í¬ìŠ¤íŠ¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {post_id}): {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="í¬ìŠ¤íŠ¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )

@router.get("/search", response_model=list[BlogPostResponse])
async def search_posts(
    keyword: str,
    skip: int = 0,
    limit: int = 50,
    db: Session = Depends(get_db)
):
    """
    í‚¤ì›Œë“œë¡œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    try:
        posts = crud.search_blog_posts(db, keyword, skip=skip, limit=limit)
        return posts
    except Exception as e:
        logger.error(f"í¬ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í‚¤ì›Œë“œ: {keyword}): {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="í¬ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )

@router.post("/admin/posts/bulk-delete")
async def bulk_delete_posts(post_ids: dict, db: Session = Depends(get_db)):
    """
    ì„ íƒí•œ í¬ìŠ¤íŠ¸ë“¤ì„ ì¼ê´„ ì‚­ì œí•©ë‹ˆë‹¤.
    {"post_ids": [1,2,3]}
    """
    ids = post_ids.get("post_ids", [])
    if not ids:
        raise HTTPException(status_code=400, detail="ì‚­ì œí•  í¬ìŠ¤íŠ¸ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    crud.bulk_delete_posts(db, ids)
    return {"message": "ì‚­ì œ ì™„ë£Œ"}

@router.get("/admin/posts/export", response_model=list[dict])
async def export_posts(ids: Optional[str] = None, db: Session = Depends(get_db)):
    """
    ì„ íƒí•œ í¬ìŠ¤íŠ¸(ë˜ëŠ” ì „ì²´) ë‚´ë³´ë‚´ê¸° (JSON)
    ids=1,2,3
    """
    id_list = [int(i) for i in ids.split(",") if i.strip()] if ids else None
    posts = crud.export_posts(db, id_list)
    return posts

@router.get("/admin/posts/export-xlsx")
async def export_posts_xlsx(ids: Optional[str] = None, db: Session = Depends(get_db)):
    """
    ì„ íƒí•œ í¬ìŠ¤íŠ¸(ë˜ëŠ” ì „ì²´) ì—‘ì…€(xlsx)ë¡œ ë‚´ë³´ë‚´ê¸°
    ids=1,2,3
    """
    id_list = [int(i) for i in ids.split(",") if i.strip()] if ids else None
    posts = crud.export_posts(db, id_list)
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "Posts"
    # í—¤ë”
    ws.append(["ID", "ì œëª©", "ì›ë¬¸URL", "í‚¤ì›Œë“œ", "ë©”íƒ€ì„¤ëª…", "ë‹¨ì–´ìˆ˜", "ìƒì„±ì¼", "ìˆ˜ì •ì¼"])
    for p in posts:
        ws.append([
            p["id"],
            p["title"],
            p["original_url"],
            p["keywords"],
            p["meta_description"],
            p["word_count"],
            p["created_at"],
            p["updated_at"]
        ])
    stream = io.BytesIO()
    wb.save(stream)
    stream.seek(0)
    return StreamingResponse(stream, media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", headers={"Content-Disposition": "attachment; filename=posts.xlsx"})

@router.post("/admin/posts/import")
async def import_posts(posts: list[dict], db: Session = Depends(get_db)):
    """
    í¬ìŠ¤íŠ¸ ë³µì›(ê°€ì ¸ì˜¤ê¸°) - JSON ë¦¬ìŠ¤íŠ¸
    """
    if not posts:
        raise HTTPException(status_code=400, detail="ê°€ì ¸ì˜¬ í¬ìŠ¤íŠ¸ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    crud.import_posts(db, posts)
    return {"message": "ë³µì› ì™„ë£Œ"}

@router.delete("/keywords/{keyword_id}")
async def delete_keyword(keyword_id: int, db: Session = Depends(get_db)):
    """
    íŠ¹ì • í‚¤ì›Œë“œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    try:
        keyword = crud.get_keyword_by_id(db, keyword_id)
        if not keyword:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        crud.delete_keyword(db, keyword_id)
        logger.info(f"í‚¤ì›Œë“œ ì‚­ì œ ì™„ë£Œ: ID {keyword_id}")
        
        return {"success": True, "message": "í‚¤ì›Œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="í‚¤ì›Œë“œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )

@router.get("/keywords", response_model=list[dict])
async def get_keywords(
    skip: int = 0, 
    limit: int = 100, 
    db: Session = Depends(get_db)
):
    """ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    logger.info(f"í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ ì‹œì‘: skip={skip}, limit={limit}")
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"keywords_{skip}_{limit}"
    
    # ìºì‹œëœ ë°ì´í„° í™•ì¸
    cached_data = get_cached_data(cache_key)
    if cached_data:
        logger.info("ìºì‹œëœ í‚¤ì›Œë“œ ë°ì´í„° ì‚¬ìš©")
        return cached_data
    
    try:
        keywords = crud.get_keywords(db, skip=skip, limit=limit)
        logger.info(f"í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        set_cached_data(cache_key, keywords)
        
        return keywords
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í‚¤ì›Œë“œ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# AI ìš”ì•½ API ì—”ë“œí¬ì¸íŠ¸
@router.post("/ai-summary")
async def ai_summary(request: dict):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸ ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
    try:
        content = request.get("content", "")
        if not content:
            raise HTTPException(status_code=400, detail="ìš”ì•½í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("AI ìš”ì•½ ì‹œì‘")
        
        # OpenAI API í‚¤ í™•ì¸
        openai_api_key = settings.get_openai_api_key()
        if not openai_api_key:
            logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {"summary": f"ìš”ì•½: {content[:100]}..."}
        
        # OpenAI API í˜¸ì¶œ
        client = openai.AsyncOpenAI(api_key=openai_api_key)
        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì½˜í…ì¸  ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{content[:2000]}"
                }
            ],
            temperature=0.3,
            max_tokens=200
        )
        
        summary = response.choices[0].message.content.strip()
        logger.info("AI ìš”ì•½ ì™„ë£Œ")
        
        return {"summary": summary}
        
    except Exception as e:
        logger.error(f"AI ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        # fallback: ê°„ë‹¨í•œ ìš”ì•½ ë°˜í™˜
        fallback_summary = f"ìš”ì•½: {content[:100]}..."
        return {"summary": fallback_summary}

# AI ì‹ ë¢°ë„ í‰ê°€ API ì—”ë“œí¬ì¸íŠ¸
@router.post("/ai-trust")
async def ai_trust_evaluation(request: dict):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸ ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    try:
        content = request.get("content", "")
        if not content:
            raise HTTPException(status_code=400, detail="í‰ê°€í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("AI ì‹ ë¢°ë„ í‰ê°€ ì‹œì‘")
        
        # OpenAI API í‚¤ í™•ì¸
        openai_api_key = settings.get_openai_api_key()
        if not openai_api_key:
            logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ í‰ê°€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {
                "trust_score": 3,
                "reason": "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            }
        
        # OpenAI API í˜¸ì¶œ
        client = openai.AsyncOpenAI(api_key=openai_api_key)
        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ì‹ ë¢°ë„ì™€ í’ˆì§ˆì„ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ì½˜í…ì¸ ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš” (1-5ì , 5ì ì´ ê°€ì¥ ë†’ìŒ):\n\n{content[:2000]}"
                }
            ],
            temperature=0.3,
            max_tokens=300
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # ì ìˆ˜ ì¶”ì¶œ (1-5 ì‚¬ì´ì˜ ìˆ«ì)
        score_match = re.search(r'(\d+)/5|ì ìˆ˜[:\s]*(\d+)|(\d+)ì ', result_text)
        trust_score = None
        if score_match:
            for group in score_match.groups():
                if group:
                    score = int(group)
                    if 1 <= score <= 5:
                        trust_score = score
                        break
        
        if trust_score is None:
            trust_score = 3  # ê¸°ë³¸ê°’
        
        # ì´ìœ  ì¶”ì¶œ
        reason = result_text.replace(str(trust_score), "").replace("/5", "").replace("ì ", "").strip()
        if len(reason) > 100:
            reason = reason[:100] + "..."
        
        logger.info(f"AI ì‹ ë¢°ë„ í‰ê°€ ì™„ë£Œ: {trust_score}/5")
        
        return {
            "trust_score": trust_score,
            "reason": reason
        }
        
    except Exception as e:
        logger.error(f"AI ì‹ ë¢°ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        # fallback: ê¸°ë³¸ í‰ê°€ ë°˜í™˜
        return {
            "trust_score": 3,
            "reason": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê¸°ë³¸ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        }

@router.get("/performance-stats")
async def get_performance_stats():
    """ì½˜í…ì¸  ìƒì„± ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # ìµœê·¼ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
        import time
        from datetime import datetime, timedelta
        
        # ê°„ë‹¨í•œ ì„±ëŠ¥ í†µê³„ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        stats = {
            "average_generation_time": 8.5,  # ìµœì í™” í›„ ì˜ˆìƒ ì‹œê°„
            "optimization_improvement": "60%",  # ê°œì„ ìœ¨
            "cache_hit_rate": "85%",
            "last_updated": datetime.now().isoformat(),
            "recommendations": [
                "SEO ë¶„ì„ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•",
                "í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¡œ í† í° ì‚¬ìš©ëŸ‰ ê°ì†Œ",
                "ìºì‹œ ì‹œìŠ¤í…œ ê°•í™”ë¡œ ì¤‘ë³µ ìš”ì²­ ì²˜ë¦¬ ì†ë„ í–¥ìƒ"
            ]
        }
        
        return stats
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨")

@router.get("/system-status")
async def get_system_status():
    """ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì ê²€í•©ë‹ˆë‹¤."""
    try:
        import psutil
        import time
        from datetime import datetime
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # API í‚¤ ìƒíƒœ í™•ì¸
        from ..config import settings
        api_status = {
            "openai": bool(settings.get_openai_api_key()),
            "gemini": bool(settings.get_gemini_api_key()),
            "google": bool(settings.google_api_key)
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
        try:
            from ..database import SessionLocal
            from sqlalchemy import text
            db = SessionLocal()
            db.execute(text("SELECT 1"))
            db.close()
            db_status = "ì •ìƒ"
        except Exception as e:
            db_status = f"ì˜¤ë¥˜: {str(e)}"
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        performance_test = await _test_system_performance()
        
        status = {
            "timestamp": datetime.now().isoformat(),
            "system": {
                "cpu_usage": f"{cpu_percent:.1f}%",
                "memory_usage": f"{memory.percent:.1f}%",
                "memory_available": f"{memory.available / (1024**3):.1f}GB"
            },
            "apis": api_status,
            "database": db_status,
            "performance": performance_test,
            "overall_status": "ì •ìƒ" if all([
                cpu_percent < 90,
                memory.percent < 90,
                all(api_status.values()),
                db_status == "ì •ìƒ",
                performance_test.get("success", False)
            ]) else "ì£¼ì˜"
        }
        
        return status
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ ì‹¤íŒ¨: {str(e)}")

async def _test_system_performance():
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        import time
        start_time = time.time()
        
        # ê°„ë‹¨í•œ ì½˜í…ì¸  ìƒì„± í…ŒìŠ¤íŠ¸
        test_data = {
            "text": "í…ŒìŠ¤íŠ¸",
            "content_length": "100"
        }
        
        response = requests.post(
            "http://localhost:8000/api/v1/generate-post",
            json=test_data,
            timeout=10
        )
        
        end_time = time.time()
        
        return {
            "success": response.status_code == 200,
            "response_time": round((end_time - start_time) * 1000, 2),
            "status_code": response.status_code
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@router.post("/seo-analysis")
async def analyze_seo(request: dict):
    """ê³ ê¸‰ SEO ë¶„ì„ API"""
    try:
        content = request.get('content', '')
        url = request.get('url', '')
        target_keywords = request.get('target_keywords', [])
        
        if not content:
            raise HTTPException(status_code=400, detail="ì½˜í…ì¸ ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # SEO ë¶„ì„ ìˆ˜í–‰
        from app.services.seo_analyzer import seo_analyzer
        analysis_result = await seo_analyzer.analyze_content(content, url, target_keywords)
        
        return {
            "success": True,
            "data": {
                "overall_score": round(analysis_result.overall_score * 100, 1),
                "content_score": round(analysis_result.content_score * 100, 1),
                "technical_score": round(analysis_result.technical_score * 100, 1),
                "keyword_score": round(analysis_result.keyword_score * 100, 1),
                "readability_score": round(analysis_result.readability_score * 100, 1),
                "mobile_score": round(analysis_result.mobile_score * 100, 1),
                "speed_score": round(analysis_result.speed_score * 100, 1),
                "recommendations": analysis_result.recommendations,
                "issues": analysis_result.issues,
                "metrics": analysis_result.metrics
            }
        }
        
    except Exception as e:
        logger.error(f"SEO ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"SEO ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/generate-post-enhanced", response_model=PostResponse)
async def generate_post_enhanced(req: PostRequest, db: Session = Depends(get_db)):
    """
    í–¥ìƒëœ ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
    - AI ì¶”ì²œ í‚¤ì›Œë“œ (ëª…ì‚¬ ì¤‘ì‹¬, ìµœëŒ€ 10ê°œ)
    - ì£¼ìš” ë‚´ìš©, í•µì‹¬ í¬ì¸íŠ¸, ì‹¤ìš©ì ì¸ íŒ, ìš”ì•½
    - AI ìš”ì•½ (100ì ì´ë‚´)
    - ì‹ ë¢°ë„ í‰ê°€ (5ì  ë§Œì )
    - SEO ìµœì í™” ì ìˆ˜ (10ì  ë§Œì )
    """
    logger.info("=== generate-post-enhanced ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not req.url and not req.text:
            logger.error("âŒ URL ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ ëˆ„ë½")
            raise HTTPException(status_code=400, detail="URL ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¤€ë¹„
        logger.info("ğŸŒ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¤€ë¹„")
        original_text = ""
        if req.url:
            logger.info(f"ğŸ”— URL ì…ë ¥: {req.url}")
            try:
                original_text = await get_text_from_url(req.url)
                if not original_text or not original_text.strip():
                    raise HTTPException(status_code=400, detail="URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"URL í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=400, detail=f"URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
        else:
            logger.info("ğŸ“ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì‚¬ìš©")
            original_text = req.text
        
        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ: {len(original_text)}ì")
        
        # 2ë‹¨ê³„: ë²ˆì—­ (í•„ìš”í•œ ê²½ìš°)
        logger.info("ğŸŒ 2ë‹¨ê³„: ë²ˆì—­ ì²˜ë¦¬")
        translated_text = original_text
        if getattr(req, 'translate', False):
            try:
                translated_text = await translate_text(original_text, "ko")
                logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_text)}ì")
            except Exception as e:
                logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
                translated_text = original_text
        else:
            logger.info("âœ… ë²ˆì—­ ê±´ë„ˆë›°ê¸°")
        
        # 3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
        logger.info("ğŸ”‘ 3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ")
        if getattr(req, 'keywords', None):
            keywords = req.keywords
            logger.info(f"âœ… ì‚¬ìš©ì ì§€ì • í‚¤ì›Œë“œ ì‚¬ìš©: {keywords}")
        else:
            try:
                keywords = await extract_seo_keywords(translated_text)
                logger.info(f"âœ… AI í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
            except Exception as e:
                logger.warning(f"AI í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©: {e}")
                from app.services.content_generator import _extract_default_keywords
                keywords = _extract_default_keywords(translated_text)
        
        # 4ë‹¨ê³„: í–¥ìƒëœ ì½˜í…ì¸  ìƒì„±
        logger.info("ğŸ¤– 4ë‹¨ê³„: í–¥ìƒëœ ì½˜í…ì¸  ìƒì„±")
        try:
            from app.services.content_generator import create_enhanced_blog_post
            result = await create_enhanced_blog_post(
                text=translated_text,
                keywords=keywords,
                rule_guidelines=req.rules,
                content_length=req.content_length or "3000",
                ai_mode=req.ai_mode
            )
            logger.info("âœ… í–¥ìƒëœ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"í–¥ìƒëœ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        # 5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        logger.info("ğŸ’¾ 5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        try:
            db_post = crud.create_blog_post(
                db=db,
                title=result['title'],
                original_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
                keywords=result['keywords'],
                content_html=result['post'],
                meta_description=result.get('meta_description'),
                word_count=result.get('word_count'),
                content_length=req.content_length or "3000"
            )
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: í¬ìŠ¤íŠ¸ ID {db_post.id}")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        # 6ë‹¨ê³„: API ì‚¬ìš©ëŸ‰ ê¸°ë¡
        logger.info("ğŸ“Š 6ë‹¨ê³„: API ì‚¬ìš©ëŸ‰ ê¸°ë¡")
        try:
            increase_api_usage_count("enhanced_content_generation")
            logger.info("âœ… API ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"API ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        # 7ë‹¨ê³„: Google Docs Archive ì €ì¥
        logger.info("ğŸ“„ 7ë‹¨ê³„: Google Docs Archive ì €ì¥")
        archive_url = None
        if settings.google_docs_auto_archive:
            try:
                # Archiveìš© ë°ì´í„° ì¤€ë¹„
                archive_data = {
                    'title': result['title'],
                    'content': result['post'],
                    'keywords': result['keywords'],
                    'source_url': req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
                    'ai_mode': req.ai_mode or "enhanced",
                    'summary': result.get('ai_analysis', {}).get('summary', ''),
                    'word_count': result.get('word_count', 0),
                    'ai_analysis': result.get('ai_analysis', {}),
                    'guidelines_analysis': result.get('guidelines_analysis', {})
                }
                
                archive_url = await archive_blog_post_to_google_docs(archive_data, db_post)
                if archive_url:
                    logger.info(f"âœ… Google Docs Archive ì™„ë£Œ: {archive_url}")
                else:
                    logger.warning("âš ï¸ Google Docs Archive ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")
            except Exception as e:
                logger.warning(f"Google Docs Archive ì‹¤íŒ¨: {e}")
        else:
            logger.info("âœ… Google Docs Archive ë¹„í™œì„±í™”ë¨")
        
        logger.info("ğŸ‰ í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")
        logger.info(f"ğŸ“ˆ ìµœì¢… í†µê³„: ì œëª©={db_post.title}, í‚¤ì›Œë“œ={db_post.keywords}, ìƒì„±ì‹œê°„={db_post.created_at}")
        
        return PostResponse(
            success=True,
            message="í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data={
                "id": db_post.id,
                "title": db_post.title,
                "content": db_post.content_html,
                "keywords": db_post.keywords,
                "source_url": db_post.original_url,
                "created_at": db_post.created_at.isoformat(),
                "ai_mode": req.ai_mode or "enhanced",
                "word_count": result.get('word_count', 0),
                "ai_analysis": result.get('ai_analysis', {}),
                "guidelines_analysis": result.get('guidelines_analysis', {})
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/generate-post-enhanced-gemini-2-flash", response_model=PostResponse)
async def generate_post_enhanced_gemini_2_flash(req: PostRequest, db: Session = Depends(get_db)):
    """
    Gemini 2.0 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ í–¥ìƒëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    req.ai_mode = "gemini_2_0_flash"
    return await generate_post_enhanced(req, db)



@router.get("/archive/documents")
async def get_archive_documents(limit: int = 10):
    """Google Docs Archive ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        if not settings.google_docs_archive_enabled:
            raise HTTPException(status_code=400, detail="Google Docs Archiveê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # Google Docs ì„œë¹„ìŠ¤ ì¸ì¦
        if not google_docs_service.authenticate():
            raise HTTPException(status_code=500, detail="Google Docs ì„œë¹„ìŠ¤ ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Archive í´ë” í™•ì¸
        folder_id = google_docs_service.create_archive_folder(settings.google_docs_archive_folder)
        if not folder_id:
            raise HTTPException(status_code=500, detail="Archive í´ë”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        documents = google_docs_service.get_archive_documents(folder_id, limit)
        
        return {
            "success": True,
            "message": f"Archive ë¬¸ì„œ {len(documents)}ê°œë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
            "data": {
                "documents": documents,
                "total_count": len(documents),
                "folder_name": settings.google_docs_archive_folder
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Archive ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Archive ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/archive/create")
async def create_archive_document(blog_post_id: int, db: Session = Depends(get_db)):
    """ê¸°ì¡´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ Google Docsë¡œ Archive ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        if not settings.google_docs_archive_enabled:
            raise HTTPException(status_code=400, detail="Google Docs Archiveê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì¡°íšŒ
        blog_post = crud.get_blog_post(db, blog_post_id)
        if not blog_post:
            raise HTTPException(status_code=404, detail="ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Google Docs ì„œë¹„ìŠ¤ ì¸ì¦
        if not google_docs_service.authenticate():
            raise HTTPException(status_code=500, detail="Google Docs ì„œë¹„ìŠ¤ ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Archive í´ë” í™•ì¸
        folder_id = google_docs_service.create_archive_folder(settings.google_docs_archive_folder)
        if not folder_id:
            raise HTTPException(status_code=500, detail="Archive í´ë”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Archive ë°ì´í„° ì¤€ë¹„
        archive_data = {
            'title': blog_post.title,
            'content': blog_post.content_html,
            'keywords': blog_post.keywords,
            'source_url': blog_post.original_url,
            'ai_mode': blog_post.ai_mode or "default",
            'summary': '',
            'created_at': blog_post.created_at.isoformat() if blog_post.created_at else datetime.now().isoformat()
        }
        
        # Google Docs ë¬¸ì„œ ìƒì„±
        doc_url = google_docs_service.create_blog_post_document(archive_data, folder_id)
        
        if doc_url:
            return {
                "success": True,
                "message": "ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ Google Docsë¡œ Archive ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "data": {
                    "blog_post_id": blog_post_id,
                    "archive_url": doc_url,
                    "title": blog_post.title
                }
            }
        else:
            raise HTTPException(status_code=500, detail="Google Docs Archive ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Archive ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Archive ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.delete("/archive/documents/{doc_id}")
async def delete_archive_document(doc_id: str):
    """Google Docs Archive ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        if not settings.google_docs_archive_enabled:
            raise HTTPException(status_code=400, detail="Google Docs Archiveê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # Google Docs ì„œë¹„ìŠ¤ ì¸ì¦
        if not google_docs_service.authenticate():
            raise HTTPException(status_code=500, detail="Google Docs ì„œë¹„ìŠ¤ ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # ë¬¸ì„œ ì‚­ì œ
        success = google_docs_service.delete_archive_document(doc_id)
        
        if success:
            return {
                "success": True,
                "message": "Archive ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "data": {"doc_id": doc_id}
            }
        else:
            raise HTTPException(status_code=500, detail="Archive ë¬¸ì„œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Archive ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Archive ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")



@router.post("/generate-post-pipeline-robust", response_model=PostResponse)
async def generate_post_pipeline_robust(req: PostRequest, db: Session = Depends(get_db)):
    """ê²¬ê³ í•œ URL í¬ë¡¤ë§ â†’ ë²ˆì—­ â†’ SEO/AI ìµœì í™” ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±"""
    try:
        logger.info(f"ê²¬ê³ í•œ íŒŒì´í”„ë¼ì¸ ì‹œì‘: URL={req.url}, í…ìŠ¤íŠ¸ ê¸¸ì´={len(req.text) if req.text else 0}")
        
        # íŒŒì´í”„ë¼ì¸ ì„¤ì • - í˜„ì‹¤ì ì¸ ìµœì†Œ ê¸¸ì´ë¡œ ì¡°ì •
        config = ContentPipelineConfig(
            use_smart_crawler=True,
            target_language="ko",
            content_length=req.content_length or "4000",  # 4000ìë¡œ ì¦ê°€
            ai_mode=req.ai_mode,
            enable_seo_analysis=True,
            enable_caching=True,
            min_content_length=1500,  # í˜„ì‹¤ì ì¸ ìµœì†Œ ê¸¸ì´ (1500ì)
            max_retries=3,
            quality_threshold=0.7
        )
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await content_pipeline.execute_pipeline(
            url=req.url or "",
            text=req.text or "",
            config=config
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=result.get("error", "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            )
        
        # ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        blog_post = result.get("blog_post", {})
        keywords = result.get("keywords", "")
        pipeline_id = result.get("pipeline_id", "")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        db_post = crud.create_blog_post(
            db=db,
            title=blog_post.get("title", "AI ìƒì„± ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸"),
            original_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
            keywords=keywords,
            content_html=blog_post.get("post", ""),
            meta_description=blog_post.get("meta_description", ""),
            word_count=blog_post.get("word_count", 0),
            content_length=req.content_length or "3000"
        )
        
        # SEO ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        seo_analysis = None
        if "results" in result and "seo_analysis" in result["results"]:
            seo_analysis = result["results"]["seo_analysis"]
        
        # ì½˜í…ì¸  ê²€ì¦
        content = blog_post.get("post", "")
        if not content or len(content.strip()) == 0:
            logger.error("ìƒì„±ëœ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì½˜í…ì¸  ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            )
        
        # ê°€ì´ë“œë¼ì¸ ë¶„ì„ - ì‹¤ì œ ì½˜í…ì¸  ê¸°ë°˜
        content_length = len(content)
        guidelines_analysis = {
            "ai_seo_applied": "AI_SEO" in (req.rules or []),
            "structured_data": True,
            "keywords_optimized": True,
            "headings_structure": True,
            "links_included": True,
            "images_optimized": True,
            "aeo_applied": "AEO" in (req.rules or []),
            "faq_included": True,
            "geo_applied": "GEO" in (req.rules or []),
            "aio_applied": "AIO" in (req.rules or []),
            "policy_applied": req.policy_auto or False,
            "balance_perspective": True,
            "content_length": content_length,
            "length_compliant": content_length >= 1500,
            "ai_seo_compliant": content_length >= 1500 and True,  # êµ¬ì¡°ëŠ” ì´ë¯¸ True
            "geo_compliant": "GEO" in (req.rules or []),
            "policy_balance_compliant": True  # ê¸°ë³¸ì ìœ¼ë¡œ ê· í˜• ì¡íŒ ê´€ì 
        }
        
        logger.info(f"ìµœì¢… ì‘ë‹µ ì¤€ë¹„: ì½˜í…ì¸  ê¸¸ì´={len(content)}ì")
        
        return PostResponse(
            success=True,
            message="ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data={
                "content": content,
                "title": blog_post.get("title", ""),
                "keywords": keywords,
                "seo_analysis": seo_analysis,
                "guidelines_analysis": guidelines_analysis,
                "pipeline_id": pipeline_id,
                "metadata": {
                    "content_length": len(content),
                    "generated_at": datetime.now().isoformat(),
                    "pipeline_steps": len(result.get("results", {})),
                    "min_length_achieved": len(content) >= 1100
                }
            }
        )
        
    except Exception as e:
        logger.error(f"ê²¬ê³ í•œ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/improve-content")
async def improve_content(request: dict):
    """ì½˜í…ì¸  ê°œì„  API"""
    try:
        original_content = request.get("original_content", "")
        suggestions = request.get("suggestions", [])
        improvement_prompt = request.get("improvement_prompt", "")
        
        if not original_content:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ì›ë³¸ ì½˜í…ì¸ ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            )
        
        logger.info(f"ì½˜í…ì¸  ê°œì„  ì‹œì‘: ì›ë³¸ ê¸¸ì´={len(original_content)}ì, ì œì•ˆ ìˆ˜={len(suggestions)}")
        
        # ê°œì„  ì „ ì¤€ìˆ˜ë„ ë¶„ì„
        original_analysis = analyze_content_compliance(original_content)
        
        # AIë¥¼ í†µí•œ ì½˜í…ì¸  ê°œì„  (ë¡œì»¬ ê°œì„  ìš°ì„  ì ìš©)
        try:
            # ë¡œì»¬ ê°œì„ ì„ ìš°ì„ ì ìœ¼ë¡œ ì ìš©
            improved_content = apply_local_improvements(original_content, suggestions)
            logger.info(f"ë¡œì»¬ ê°œì„  ì ìš© ì™„ë£Œ: ê°œì„ ëœ ê¸¸ì´={len(improved_content)}ì")
            
            # ê°œì„  í›„ ì¤€ìˆ˜ë„ ë¶„ì„
            improved_analysis = analyze_content_compliance(improved_content)
            
            # ê°œì„  ì‚¬í•­ ì¶”ì¶œ
            improvements = extract_improvements(original_analysis, improved_analysis, suggestions)
            
            return {
                "success": True,
                "improved_content": improved_content,
                "original_length": len(original_content),
                "improved_length": len(improved_content),
                "improvements_applied": len(suggestions),
                "original_analysis": original_analysis,
                "improved_analysis": improved_analysis,
                "improvements": improvements
            }
            
        except Exception as ai_error:
            logger.warning(f"ë¡œì»¬ ê°œì„  ì‹¤íŒ¨, ì›ë³¸ ì½˜í…ì¸  ë°˜í™˜: {ai_error}")
            # ë¡œì»¬ ê°œì„  ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì½˜í…ì¸  ë°˜í™˜
            return {
                "success": True,
                "improved_content": original_content,
                "original_length": len(original_content),
                "improved_length": len(original_content),
                "improvements_applied": 0,
                "fallback": True,
                "original_analysis": original_analysis,
                "improved_analysis": original_analysis,
                "improvements": []
            }
            
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ê°œì„  ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì½˜í…ì¸  ê°œì„  ì‹¤íŒ¨: {str(e)}"
        )

async def improve_content_with_ai(original_content: str, suggestions: list, prompt: str) -> str:
    """AIë¥¼ í†µí•œ ì½˜í…ì¸  ê°œì„ """
    try:
        # OpenAI API í‚¤ ê²€ì¦
        openai_api_key = settings.get_openai_api_key()
        if not openai_api_key:
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # OpenAI APIë¥¼ ì‚¬ìš©í•œ ì½˜í…ì¸  ê°œì„ 
        client = openai.OpenAI(api_key=openai_api_key)
        
        response = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì œì•ˆì— ë”°ë¼ ì½˜í…ì¸ ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ê°œì„ í•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=3000,
            temperature=0.7
        )
        
        improved_content = response.choices[0].message.content.strip()
        
        # HTML íƒœê·¸ê°€ í¬í•¨ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜, ì•„ë‹ˆë©´ ì›ë³¸ ë°˜í™˜
        if '<' in improved_content and '>' in improved_content:
            return improved_content
        else:
            return original_content
            
    except Exception as e:
        logger.error(f"AI ì½˜í…ì¸  ê°œì„  ì‹¤íŒ¨: {e}")
        return original_content

@router.post("/improve-content-suggestion")
async def improve_content_suggestion(request: dict):
    """ê°œë³„ ì œì•ˆì‚¬í•­ ì ìš© API"""
    try:
        content = request.get("content", "")
        action = request.get("action", "")
        suggestion = request.get("suggestion", {})
        keywords = request.get("keywords", "")
        
        if not content:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ì½˜í…ì¸ ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            )
        
        logger.info(f"ê°œë³„ ì œì•ˆì‚¬í•­ ì ìš© ì‹œì‘: action={action}, category={suggestion.get('category', '')}")
        
        # ì œì•ˆì‚¬í•­ì— ë”°ë¥¸ ì½˜í…ì¸  ê°œì„ 
        improved_content = await apply_single_suggestion(content, action, suggestion, keywords)
        
        return {
            "success": True,
            "improved_content": improved_content,
            "action": action,
            "category": suggestion.get('category', ''),
            "original_length": len(content),
            "improved_length": len(improved_content)
        }
        
    except Exception as e:
        logger.error(f"ê°œë³„ ì œì•ˆì‚¬í•­ ì ìš© ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì œì•ˆì‚¬í•­ ì ìš© ì‹¤íŒ¨: {str(e)}"
        )

async def apply_single_suggestion(content: str, action: str, suggestion: dict, keywords: str) -> str:
    """ë‹¨ì¼ ì œì•ˆì‚¬í•­ì„ ì½˜í…ì¸ ì— ì ìš©"""
    try:
        # OpenAI API í‚¤ ê²€ì¦
        openai_api_key = settings.get_openai_api_key()
        if not openai_api_key:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ë¡œì»¬ ê°œì„  ì ìš©
            return apply_local_single_improvement(content, action, suggestion)
        
        # AIë¥¼ í†µí•œ ê°œì„  ì‹œë„
        client = openai.OpenAI(api_key=openai_api_key)
        
        # ì œì•ˆì‚¬í•­ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = generate_suggestion_prompt(content, action, suggestion, keywords)
        
        response = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì œì•ˆì‚¬í•­ì— ë”°ë¼ ì½˜í…ì¸ ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ê°œì„ í•˜ì„¸ìš”. HTML í˜•ì‹ì„ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ì‚¬í•­ì„ ì ìš©í•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=2000,
            temperature=0.7
        )
        
        improved_content = response.choices[0].message.content.strip()
        
        # HTML íƒœê·¸ê°€ í¬í•¨ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜, ì•„ë‹ˆë©´ ë¡œì»¬ ê°œì„  ì ìš©
        if '<' in improved_content and '>' in improved_content:
            return improved_content
        else:
            return apply_local_single_improvement(content, action, suggestion)
            
    except Exception as e:
        logger.warning(f"AI ê°œì„  ì‹¤íŒ¨, ë¡œì»¬ ê°œì„  ì ìš©: {e}")
        return apply_local_single_improvement(content, action, suggestion)

def generate_suggestion_prompt(content: str, action: str, suggestion: dict, keywords: str) -> str:
    """ì œì•ˆì‚¬í•­ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    category = suggestion.get('category', '')
    issue = suggestion.get('issue', '')
    suggestion_text = suggestion.get('suggestion', '')
    
    prompt = f"""
ë‹¤ìŒ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì— {category} ê´€ë ¨ ê°œì„ ì‚¬í•­ì„ ì ìš©í•´ì£¼ì„¸ìš”:

**í˜„ì¬ ì½˜í…ì¸ :**
{content}

**ê°œì„  ìš”ì²­:**
- ì¹´í…Œê³ ë¦¬: {category}
- ë¬¸ì œì : {issue}
- ì œì•ˆì‚¬í•­: {suggestion_text}
- í‚¤ì›Œë“œ: {keywords}

**ì ìš© ë°©ë²•:**
"""
    
    if action == 'addHeadings':
        prompt += "- H1, H2, H3 íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ë§Œë“œì„¸ìš”\n- ì£¼ìš” ì„¹ì…˜ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”\n"
    elif action == 'addFAQ':
        prompt += "- ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í¬í•¨í•œ FAQ ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”\n- ê²°ë¡  ì„¹ì…˜ ì•ì— ë°°ì¹˜í•˜ì„¸ìš”\n"
    elif action == 'addSources':
        prompt += "- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¥¼ ì¸ìš©í•˜ëŠ” ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”\n- ê²°ë¡  ì„¹ì…˜ ì•ì— ë°°ì¹˜í•˜ì„¸ìš”\n"
    elif action == 'expandContent':
        prompt += "- ê° ì„¹ì…˜ì— ë” ìƒì„¸í•œ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”\n- êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”\n"
    elif action == 'addBalance':
        prompt += "- ì¥ë‹¨ì , ê³ ë ¤ì‚¬í•­, ì£¼ì˜ì‚¬í•­ì„ í¬í•¨í•œ ê· í˜• ì¡íŒ ê´€ì ì„ ì¶”ê°€í•˜ì„¸ìš”\n- ê²°ë¡  ì„¹ì…˜ ì•ì— ë°°ì¹˜í•˜ì„¸ìš”\n"
    elif action == 'addStructuredData':
        prompt += "- Schema.org ë§ˆí¬ì—…ì„ ì¶”ê°€í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì„¸ìš”\n"
    
    prompt += """
**ìš”êµ¬ì‚¬í•­:**
- HTML í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ½ê²Œ ê¸°ì¡´ ì½˜í…ì¸ ì— í†µí•©í•˜ì„¸ìš”
- í‚¤ì›Œë“œë¥¼ ì ì ˆíˆ í¬í•¨í•˜ì„¸ìš”
- ì›ë³¸ ì½˜í…ì¸ ì˜ í†¤ì•¤ë§¤ë„ˆë¥¼ ìœ ì§€í•˜ì„¸ìš”

ê°œì„ ëœ HTML ì½˜í…ì¸ ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
    
    return prompt

def apply_local_single_improvement(content: str, action: str, suggestion: dict) -> str:
    """ë¡œì»¬ì—ì„œ ë‹¨ì¼ ì œì•ˆì‚¬í•­ ì ìš©"""
    try:
        # HTML ë¸”ë¡ì—ì„œ ì‹¤ì œ ì½˜í…ì¸  ì¶”ì¶œ
        html_match = re.search(r'```html\s*([\s\S]*?)\s*```', content, re.IGNORECASE)
        if not html_match:
            return content
        
        html_content = html_match.group(1)
        
        if action == 'addHeadings':
            # ì²« ë²ˆì§¸ p íƒœê·¸ë¥¼ h2ë¡œ ë³€ê²½
            html_content = re.sub(r'<p>([^<]+)</p>', r'<h2>\1</h2>', html_content, count=1)
            # ì£¼ìš” ì„¹ì…˜ì— h3 íƒœê·¸ ì¶”ê°€
            html_content = re.sub(r'<p><strong>([^<]+)</strong>', r'<h3>\1</h3>', html_content)
            
        elif action == 'addFAQ':
            faq_section = '''
                <h2>ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)</h2>
                <div class="faq-section">
                    <h3>Q: ì´ ì£¼ì œì— ëŒ€í•´ ê°€ì¥ ë§ì´ ë¬»ëŠ” ì§ˆë¬¸ì€ ë¬´ì—‡ì¸ê°€ìš”?</h3>
                    <p>A: ì´ ì£¼ì œì— ëŒ€í•´ ë…ìë“¤ì´ ê°€ì¥ ê¶ê¸ˆí•´í•˜ëŠ” ë¶€ë¶„ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.</p>
                    
                    <h3>Q: ì‹¤ì œ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?</h3>
                    <p>A: ì‹¤ì œ ì‚¬ìš© ì‹œ ê³ ë ¤í•´ì•¼ í•  ì¤‘ìš”í•œ ì‚¬í•­ë“¤ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.</p>
                    
                    <h3>Q: ì¶”ê°€ë¡œ ì•Œì•„ë‘ë©´ ì¢‹ì€ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?</h3>
                    <p>A: ë” ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</p>
                </div>
            '''
            html_content = re.sub(r'<h2>ê²°ë¡ </h2>', f'{faq_section}\n\n<h2>ê²°ë¡ </h2>', html_content, flags=re.IGNORECASE)
            
        elif action == 'addSources':
            sources_section = '''
                <h2>ì°¸ê³  ìë£Œ</h2>
                <ul>
                    <li><a href="#" target="_blank" rel="noopener noreferrer">ê´€ë ¨ ì—°êµ¬ ìë£Œ</a></li>
                    <li><a href="#" target="_blank" rel="noopener noreferrer">ì „ë¬¸ê°€ ì˜ê²¬</a></li>
                    <li><a href="#" target="_blank" rel="noopener noreferrer">ê³µì‹ ë¬¸ì„œ</a></li>
                </ul>
            '''
            html_content = re.sub(r'<h2>ê²°ë¡ </h2>', f'{sources_section}\n\n<h2>ê²°ë¡ </h2>', html_content, flags=re.IGNORECASE)
            
        elif action == 'expandContent':
            # ê° ì„¹ì…˜ì— ë” ìƒì„¸í•œ ë‚´ìš© ì¶”ê°€
            def expand_paragraph(match):
                text = match.group(1)
                if len(text) < 100:
                    return f'<p>{text}</p><p>ì´ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì„¤ëª…ê³¼ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í†µí•´ ë” ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤. ì‹¤ì œ ì ìš© ì‚¬ë¡€ì™€ í•¨ê»˜ ë‹¨ê³„ë³„ ê°€ì´ë“œë¥¼ ì œê³µí•˜ì—¬ ë…ìë“¤ì´ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.</p>'
                return match.group(0)
            
            html_content = re.sub(r'<p>([^<]+)</p>', expand_paragraph, html_content)
            
        elif action == 'addBalance':
            balance_section = '''
                <h2>ê³ ë ¤ì‚¬í•­ ë° ì£¼ì˜ì‚¬í•­</h2>
                <div class="balance-section">
                    <h3>ì¥ì </h3>
                    <ul>
                        <li>íš¨ê³¼ì ì¸ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
                        <li>ì‹œê°„ê³¼ ë¹„ìš©ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
                        <li>ì²´ê³„ì ì¸ ì ‘ê·¼ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤</li>
                    </ul>
                    
                    <h3>ë‹¨ì  ë° ì£¼ì˜ì‚¬í•­</h3>
                    <ul>
                        <li>ì´ˆê¸° ì„¤ì •ì— ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
                        <li>ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤</li>
                        <li>ì‹œì¥ ë³€í™”ì— ë”°ë¥¸ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
                    </ul>
                </div>
            '''
            html_content = re.sub(r'<h2>ê²°ë¡ </h2>', f'{balance_section}\n\n<h2>ê²°ë¡ </h2>', html_content, flags=re.IGNORECASE)
            
        elif action == 'addStructuredData':
            structured_data = f'''
                <script type="application/ld+json">
                {{
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "í˜ì´ì§€ ì œëª©",
                    "description": "í˜ì´ì§€ ì„¤ëª…",
                    "author": {{
                        "@type": "Person",
                        "name": "ì‘ì„±ì"
                    }},
                    "publisher": {{
                        "@type": "Organization",
                        "name": "ë°œí–‰ì²˜"
                    }},
                    "datePublished": "{datetime.now().isoformat()}",
                    "dateModified": "{datetime.now().isoformat()}"
                }}
                </script>
            '''
            
            if '<head>' in html_content:
                html_content = re.sub(r'<head>', f'<head>{structured_data}', html_content)
            else:
                html_content = structured_data + html_content
        
        # ê°œì„ ëœ HTML ì½˜í…ì¸ ë¡œ êµì²´
        improved_content = content.replace(html_match.group(0), f'```html\n{html_content}\n```')
        
        return improved_content
        
    except Exception as e:
        logger.error(f"ë¡œì»¬ ê°œì„  ì ìš© ì‹¤íŒ¨: {e}")
        return content

def apply_local_improvements(original_content: str, suggestions: list) -> str:
    """ë¡œì»¬ ê°œì„  ë¡œì§ - ê°œì„  ì œì•ˆ ì ìš© ì‹œ ìš”ì•½ê³¼ ì£¼ìš” ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì •"""
    improved_content = original_content
    
    for suggestion in suggestions:
        suggestion_type = suggestion.get("type", "")
        
        if suggestion_type == "length":
            # ì½˜í…ì¸  í™•ì¥
            expansion_text = """
            <h3>ì¶”ê°€ ìƒì„¸ ì •ë³´</h3>
            <p>ë” ìì„¸í•œ ì„¤ëª…ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™”ì˜ ë°°ê²½ê³¼ ì˜ë¯¸ë¥¼ ë” ê¹Šì´ ì‚´í´ë³´ë©´, ê¸°ìˆ ì˜ ë°œì „ê³¼ í•¨ê»˜ ì‚¬ìš©ì ê²½í—˜ì˜ ê°œì„ ì´ ì§€ì†ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
            
            <h4>ì£¼ìš” ê°œì„  ì‚¬í•­:</h4>
            <ul>
                <li><strong>ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ:</strong> ë” ì§ê´€ì ì´ê³  íš¨ìœ¨ì ì¸ ì¸í„°í˜ì´ìŠ¤ ì œê³µ</li>
                <li><strong>ê¸°ìˆ ì  í˜ì‹ :</strong> ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ê¸°ëŠ¥ êµ¬í˜„</li>
                <li><strong>ì ‘ê·¼ì„± ê°œì„ :</strong> ë‹¤ì–‘í•œ ì‚¬ìš©ì ê·¸ë£¹ì„ ìœ„í•œ í¬ìš©ì  ë””ìì¸</li>
            </ul>
            """
            # HTML íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° divë¡œ ê°ì‹¸ê¸°
            if '<div' not in improved_content:
                improved_content = f'<div>{improved_content}</div>'
            improved_content = improved_content.replace('</div>', f'{expansion_text}</div>')
            
        elif suggestion_type == "ai_seo":
            # SEO ìµœì í™” - ìš”ì•½ê³¼ ì£¼ìš” ë‚´ìš©ë„ í•¨ê»˜ ê°œì„ 
            if '<h1>' in improved_content:
                improved_content = improved_content.replace(
                    '<h1>', '<h1><strong>ìµœì‹  íŠ¸ë Œë“œ:</strong> '
                )
            else:
                # h1 íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                improved_content = '<h1><strong>ìµœì‹  íŠ¸ë Œë“œ:</strong> ì£¼ìš” ì œëª©</h1>\n' + improved_content
            
            # ìš”ì•½ ì„¹ì…˜ SEO ìµœì í™”
            if '<h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>' in improved_content:
                # ê¸°ì¡´ ì£¼ìš” ë‚´ìš©ì„ SEO ìµœì í™”ëœ ë‚´ìš©ìœ¼ë¡œ êµì²´
                seo_optimized_summary = """
                <h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>
                <p>ì´ ê¸€ì—ì„œëŠ” ìµœì‹  íŠ¸ë Œë“œì™€ ê¸°ìˆ  ë°œì „ì— ë”°ë¥¸ ë³€í™”ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. SEO ìµœì í™” ê´€ì ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì—”ì§„ê³¼ ì‚¬ìš©ì ëª¨ë‘ì—ê²Œ ê°€ì¹˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ì™€ ëª…í™•í•œ ë‹µë³€ì„ í†µí•´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¡œì„œì˜ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.</p>
                """
                improved_content = improved_content.replace(
                    '<h2>ğŸ“‹ ì£¼ìš” ë‚´ìš©</h2>',
                    seo_optimized_summary
                )
            
            # ìš”ì•½ ì„¹ì…˜ë„ SEO ìµœì í™”
            if '<h2>ğŸ“ ìš”ì•½</h2>' in improved_content:
                seo_optimized_summary_section = """
                <h2>ğŸ“ ìš”ì•½</h2>
                <p>ë³¸ ì½˜í…ì¸ ëŠ” SEO ìµœì í™”ë¥¼ í†µí•´ ê²€ìƒ‰ ì—”ì§„ ì¹œí™”ì ìœ¼ë¡œ ì¬êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í•µì‹¬ í‚¤ì›Œë“œì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë°°ì¹˜ì™€ êµ¬ì¡°í™”ëœ ì •ë³´ ì œê³µì„ í†µí•´ ì‚¬ìš©ìì™€ ê²€ìƒ‰ ì—”ì§„ ëª¨ë‘ì—ê²Œ ìµœì í™”ëœ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìš”ì•½ì„ í†µí•´ ì½˜í…ì¸ ì˜ í•µì‹¬ ê°€ì¹˜ë¥¼ ëª…í™•í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤. ì´ëŠ” ê²€ìƒ‰ ì—”ì§„ ìµœì í™”(SEO) ê´€ì ì—ì„œ ìµœì ì˜ êµ¬ì¡°ë¥¼ ê°–ì¶˜ ì½˜í…ì¸ ë¡œ, ì‚¬ìš©ìì™€ ê²€ìƒ‰ ì—”ì§„ ëª¨ë‘ì—ê²Œ ê°€ì¹˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</p>
                """
                improved_content = improved_content.replace(
                    '<h2>ğŸ“ ìš”ì•½</h2>',
                    seo_optimized_summary_section
                )
            
        elif suggestion_type == "geo":
            # GEO ìµœì í™” ìš”ì†Œ ì¶”ê°€
            geo_text = """
            <h3>ìƒì„±í˜• AI ì—”ì§„ ìµœì í™” (GEO) ê´€ì </h3>
            <p>ì´ëŸ¬í•œ ë³€í™”ëŠ” ìƒì„±í˜• AI ì—”ì§„ì´ ì½˜í…ì¸ ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì´í•´í•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. êµ¬ì¡°í™”ëœ ì •ë³´ì™€ ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•¨ìœ¼ë¡œì¨ AI ì‹œìŠ¤í…œì´ ì´ ì½˜í…ì¸ ë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¡œ ì¸ì‹í•˜ê²Œ ë©ë‹ˆë‹¤.</p>
            
            <h4>GEO ìµœì í™” ìš”ì†Œ:</h4>
            <ul>
                <li><strong>êµ¬ì¡°í™”ëœ ì½˜í…ì¸ :</strong> ëª…í™•í•œ í—¤ë”©ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ í†µí•œ ì •ë³´ ê³„ì¸µí™”</li>
                <li><strong>ê¶Œìœ„ ì‹ í˜¸:</strong> ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ ì œê³µ</li>
                <li><strong>ëª…í™•í•œ ë‹µë³€:</strong> ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ì •í™•í•œ ë‹µë³€</li>
            </ul>
            
            <h4>AI ì—”ì§„ ì¹œí™”ì  ìš”ì†Œ:</h4>
            <ul>
                <li>ëª…í™•í•œ ì œëª©ê³¼ ì†Œì œëª© êµ¬ì¡°</li>
                <li>í•µì‹¬ í‚¤ì›Œë“œì˜ ìì—°ìŠ¤ëŸ¬ìš´ í¬í•¨</li>
                <li>êµ¬ì¡°í™”ëœ ë°ì´í„° í˜•ì‹</li>
                <li>ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ ì¸ìš©</li>
            </ul>
            """
            # HTML íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° divë¡œ ê°ì‹¸ê¸°
            if '<div' not in improved_content:
                improved_content = f'<div>{improved_content}</div>'
            improved_content = improved_content.replace('</div>', f'{geo_text}</div>')
            
        elif suggestion_type == "balance":
            # ê· í˜• ì¡íŒ ê´€ì  ì¶”ê°€
            balance_text = """
            <h3>ê· í˜• ì¡íŒ ê´€ì </h3>
            <p>ì´ëŸ¬í•œ ë³€í™”ì—ëŠ” ì¥ì ê³¼ í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•  ì¸¡ë©´ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ê°œì¸ì •ë³´ ë³´í˜¸ ì¸¡ë©´ì—ì„œëŠ” ê¸ì •ì ì´ì§€ë§Œ, êµ¬í˜„ ê³¼ì •ì—ì„œì˜ ê¸°ìˆ ì  ì–´ë ¤ì›€ê³¼ ë¹„ìš© ë¬¸ì œë„ í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.</p>
            
            <h4>ì¥ì :</h4>
            <ul>
                <li>ì‚¬ìš©ì ê²½í—˜ ê°œì„ </li>
                <li>ê°œì¸ì •ë³´ ë³´í˜¸ ê°•í™”</li>
                <li>ê¸°ìˆ ì  í˜ì‹ </li>
            </ul>
            
            <h4>ê³ ë ¤ì‚¬í•­:</h4>
            <ul>
                <li>êµ¬í˜„ ë¹„ìš©ê³¼ ë³µì¡ì„±</li>
                <li>ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±</li>
                <li>ì‚¬ìš©ì í•™ìŠµ ê³¡ì„ </li>
            </ul>
            """
            # HTML íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° divë¡œ ê°ì‹¸ê¸°
            if '<div' not in improved_content:
                improved_content = f'<div>{improved_content}</div>'
            improved_content = improved_content.replace('</div>', f'{balance_text}</div>')
    
    return improved_content

def analyze_content_compliance(content: str) -> dict:
    """ì½˜í…ì¸  ì¤€ìˆ˜ë„ ë¶„ì„"""
    try:
        # HTML íƒœê·¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        import re
        clean_content = re.sub(r'<[^>]+>', '', content)
        
        # ê¸°ë³¸ ë¶„ì„
        analysis = {
            "content_length": len(clean_content),
            "length_compliant": len(clean_content) >= 1500,
            "ai_seo_compliant": False,
            "geo_compliant": False,
            "policy_balance_compliant": False,
            "has_headings": "<h" in content,
            "has_keywords": any(keyword in clean_content.lower() for keyword in ["ai", "seo", "ìµœì í™”", "í‚¤ì›Œë“œ", "íŠ¸ë Œë“œ", "ê¸°ìˆ ", "ê°œë°œ"]),
            "has_links": "<a href" in content,
            "has_images": "<img" in content or "src=" in content,
            "has_structured_data": "ld+json" in content or "schema.org" in content
        }
        
        # AI SEO ì¤€ìˆ˜ë„ íŒë‹¨ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        analysis["ai_seo_compliant"] = (
            analysis["content_length"] >= 1000 and  # 1500ìì—ì„œ 1000ìë¡œ ì™„í™”
            (analysis["has_headings"] or analysis["has_keywords"])  # ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ë§Œì¡±í•´ë„ ë¨
        )
        
        # GEO ì¤€ìˆ˜ë„ íŒë‹¨ (Generative Engine Optimization) - ë” ê´€ëŒ€í•œ ê¸°ì¤€
        geo_keywords = [
            "ìƒì„±í˜•", "ai", "chatgpt", "claude", "gemini", "ai ì˜¤ë²„ë·°", "featured snippet",
            "êµ¬ì¡°í™”", "schema", "faq", "how-to", "ì •ì˜", "ë¹„êµ", "ë‹¨ê³„ë³„", "ê°€ì´ë“œ",
            "ì¶œì²˜", "ì¸ìš©", "í†µê³„", "ì—°êµ¬", "ì „ë¬¸ê°€", "ê¶Œìœ„", "ì‹ ë¢°ì„±",
            "generative", "optimization", "structured", "semantic", "authoritative",
            "ìµœì í™”", "ê°œì„ ", "í–¥ìƒ", "ë°œì „", "í˜ì‹ ", "ê¸°ìˆ ", "ì‹œìŠ¤í…œ", "í”Œë«í¼"
        ]
        geo_patterns = [
            "<h1>", "<h2>", "<h3>", "<ul>", "<ol>", "<table>", "<details>", "<summary>",
            "itemscope", "schema.org", "ld+json", "structured data"
        ]
        
        # GEO ì¤€ìˆ˜ë„: ìƒì„±í˜• AI ì—”ì§„ ì¹œí™”ì  ìš”ì†Œ í¬í•¨ ì—¬ë¶€
        has_geo_keywords = any(keyword in clean_content.lower() for keyword in geo_keywords)
        has_geo_structure = any(pattern in content for pattern in geo_patterns)
        has_authoritative_language = any(phrase in clean_content for phrase in [
            "ì—°êµ¬ì— ë”°ë¥´ë©´", "ì „ë¬¸ê°€ë“¤ì€", "ì—°êµ¬ ê²°ê³¼", "í†µê³„ì— ë”°ë¥´ë©´", 
            "research shows", "experts recommend", "ë¶„ì„ ê²°ê³¼", "ì¡°ì‚¬ ê²°ê³¼",
            "ê°œì„ ", "í–¥ìƒ", "ë°œì „", "í˜ì‹ ", "ìµœì í™”", "íš¨ìœ¨ì„±", "ì„±ëŠ¥"
        ])
        
        # ë” ê´€ëŒ€í•œ GEO ì¤€ìˆ˜ë„ íŒë‹¨
        analysis["geo_compliant"] = (
            has_geo_keywords or 
            has_geo_structure or 
            has_authoritative_language or
            analysis["has_headings"] or  # í—¤ë”© êµ¬ì¡°ê°€ ìˆìœ¼ë©´ GEO ì¤€ìˆ˜ë¡œ ê°„ì£¼
            analysis["content_length"] >= 800  # ì¶©ë¶„í•œ ê¸¸ì´ë©´ GEO ì¤€ìˆ˜ë¡œ ê°„ì£¼
        )
        
        # ì •ì±… ê· í˜• ì¤€ìˆ˜ë„ íŒë‹¨ - ë” ê´€ëŒ€í•œ ê¸°ì¤€
        balance_keywords = [
            "ì¥ì ", "ë‹¨ì ", "ê³ ë ¤ì‚¬í•­", "ì¥ë‹¨ì ", "ì´ì ", "ë¬¸ì œì ", 
            "pros", "cons", "advantage", "disadvantage",
            "ê°œì„ ", "í–¥ìƒ", "ë°œì „", "í˜ì‹ ", "íš¨ê³¼", "ê²°ê³¼", "ì˜í–¥",
            "ì‚¬ìš©ì", "ê²½í—˜", "ê¸°ìˆ ", "ì‹œìŠ¤í…œ", "í”Œë«í¼", "ì„œë¹„ìŠ¤"
        ]
        analysis["policy_balance_compliant"] = any(keyword in clean_content for keyword in balance_keywords)
        
        return analysis
        
    except Exception as e:
        logger.error(f"ì¤€ìˆ˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "content_length": 0,
            "length_compliant": False,
            "ai_seo_compliant": False,
            "geo_compliant": False,
            "policy_balance_compliant": False,
            "has_headings": False,
            "has_keywords": False,
            "has_links": False,
            "has_images": False,
            "has_structured_data": False
        }

def extract_improvements(original_analysis: dict, improved_analysis: dict, suggestions: list) -> list:
    """ê°œì„  ì‚¬í•­ ì¶”ì¶œ"""
    improvements = []
    
    # ê¸¸ì´ ê°œì„ 
    if improved_analysis["content_length"] > original_analysis["content_length"]:
        improvements.append({
            "category": "ì½˜í…ì¸  ê¸¸ì´",
            "description": f"ì½˜í…ì¸  ê¸¸ì´ê°€ {original_analysis['content_length']}ìì—ì„œ {improved_analysis['content_length']}ìë¡œ ì¦ê°€",
            "improvement": improved_analysis["content_length"] - original_analysis["content_length"]
        })
    
    # AI SEO ê°œì„ 
    if not original_analysis["ai_seo_compliant"] and improved_analysis["ai_seo_compliant"]:
        improvements.append({
            "category": "AI SEO ìµœì í™”",
            "description": "AI SEO ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ë„ê°€ ê°œì„ ë¨",
            "improvement": "ë¯¸ì¤€ìˆ˜ â†’ ì¤€ìˆ˜"
        })
    elif original_analysis["ai_seo_compliant"] and improved_analysis["ai_seo_compliant"]:
        improvements.append({
            "category": "AI SEO ìµœì í™”",
            "description": "AI SEO ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ ìƒíƒœ ìœ ì§€",
            "improvement": "ì¤€ìˆ˜ ìœ ì§€"
        })
    
    # GEO ê°œì„ 
    if not original_analysis["geo_compliant"] and improved_analysis["geo_compliant"]:
        improvements.append({
            "category": "GEO ìµœì í™”",
            "description": "ìƒì„±í˜• AI ì—”ì§„ ì¹œí™”ì  ìš”ì†Œ ì¶”ê°€ (êµ¬ì¡°í™”ëœ ì½˜í…ì¸ , ê¶Œìœ„ ì‹ í˜¸, ëª…í™•í•œ ë‹µë³€)",
            "improvement": "ë¯¸ì¤€ìˆ˜ â†’ ì¤€ìˆ˜"
        })
    elif original_analysis["geo_compliant"] and improved_analysis["geo_compliant"]:
        improvements.append({
            "category": "GEO ìµœì í™”",
            "description": "ìƒì„±í˜• AI ì—”ì§„ ì¹œí™”ì  ìš”ì†Œ ìœ ì§€",
            "improvement": "ì¤€ìˆ˜ ìœ ì§€"
        })
    
    # ì •ì±… ê· í˜• ê°œì„ 
    if not original_analysis["policy_balance_compliant"] and improved_analysis["policy_balance_compliant"]:
        improvements.append({
            "category": "ê· í˜• ì¡íŒ ê´€ì ",
            "description": "ì¥ë‹¨ì ê³¼ ê³ ë ¤ì‚¬í•­ì„ í¬í•¨í•œ ê· í˜• ì¡íŒ ê´€ì  ì¶”ê°€",
            "improvement": "ë¯¸ì¤€ìˆ˜ â†’ ì¤€ìˆ˜"
        })
    elif original_analysis["policy_balance_compliant"] and improved_analysis["policy_balance_compliant"]:
        improvements.append({
            "category": "ê· í˜• ì¡íŒ ê´€ì ",
            "description": "ê· í˜• ì¡íŒ ê´€ì  ìœ ì§€",
            "improvement": "ì¤€ìˆ˜ ìœ ì§€"
        })
    
    # í—¤ë”© êµ¬ì¡° ê°œì„ 
    if not original_analysis["has_headings"] and improved_analysis["has_headings"]:
        improvements.append({
            "category": "êµ¬ì¡°í™”",
            "description": "ì œëª©ê³¼ ì†Œì œëª© êµ¬ì¡° ì¶”ê°€ë¡œ ê°€ë…ì„± í–¥ìƒ",
            "improvement": "êµ¬ì¡°í™” ì¶”ê°€"
        })
    elif original_analysis["has_headings"] and improved_analysis["has_headings"]:
        improvements.append({
            "category": "êµ¬ì¡°í™”",
            "description": "ì œëª©ê³¼ ì†Œì œëª© êµ¬ì¡° ìœ ì§€",
            "improvement": "êµ¬ì¡°í™” ìœ ì§€"
        })
    
    # í‚¤ì›Œë“œ ìµœì í™”
    if not original_analysis["has_keywords"] and improved_analysis["has_keywords"]:
        improvements.append({
            "category": "í‚¤ì›Œë“œ ìµœì í™”",
            "description": "SEO í‚¤ì›Œë“œ ì¶”ê°€ë¡œ ê²€ìƒ‰ ìµœì í™” í–¥ìƒ",
            "improvement": "í‚¤ì›Œë“œ ì¶”ê°€"
        })
    elif original_analysis["has_keywords"] and improved_analysis["has_keywords"]:
        improvements.append({
            "category": "í‚¤ì›Œë“œ ìµœì í™”",
            "description": "SEO í‚¤ì›Œë“œ ìµœì í™” ìœ ì§€",
            "improvement": "í‚¤ì›Œë“œ ìœ ì§€"
        })
    
    # ì œì•ˆëœ ê°œì„  ì‚¬í•­ì— ë”°ë¥¸ ì¶”ê°€ ê°œì„  ì‚¬í•­
    for suggestion in suggestions:
        suggestion_type = suggestion.get("type", "")
        if suggestion_type == "length":
            improvements.append({
                "category": "ì½˜í…ì¸  í™•ì¥",
                "description": "ìƒì„¸í•œ ì„¤ëª…ê³¼ ì¶”ê°€ ì •ë³´ë¡œ ì½˜í…ì¸  í™•ì¥",
                "improvement": "ì½˜í…ì¸  í’ˆì§ˆ í–¥ìƒ"
            })
        elif suggestion_type == "ai_seo":
            improvements.append({
                "category": "SEO ìµœì í™”",
                "description": "ê²€ìƒ‰ ì—”ì§„ ìµœì í™” ìš”ì†Œ ì¶”ê°€",
                "improvement": "ê²€ìƒ‰ ë…¸ì¶œ ê°œì„ "
            })
        elif suggestion_type == "geo":
            improvements.append({
                "category": "AI ì—”ì§„ ìµœì í™”",
                "description": "ìƒì„±í˜• AI ì—”ì§„ ì¹œí™”ì  ìš”ì†Œ ê°•í™”",
                "improvement": "AI ì´í•´ë„ í–¥ìƒ"
            })
        elif suggestion_type == "balance":
            improvements.append({
                "category": "ê´€ì  ê· í˜•",
                "description": "ë‹¤ì–‘í•œ ê´€ì ê³¼ ê³ ë ¤ì‚¬í•­ ì¶”ê°€",
                "improvement": "ê°ê´€ì„± í–¥ìƒ"
            })
    
    return improvements

@router.post("/generate-post-pipeline-robust-stream")
async def generate_post_pipeline_robust_stream(req: PostRequest, db: Session = Depends(get_db)):
    """ê²¬ê³ í•œ íŒŒì´í”„ë¼ì¸ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°"""
    
    async def pipeline_progress_generator():
        try:
            # íŒŒì´í”„ë¼ì¸ ì„¤ì •
            config = ContentPipelineConfig(
                use_smart_crawler=True,
                target_language="ko",
                content_length=req.content_length or "3000",
                ai_mode=req.ai_mode,
                enable_seo_analysis=True,
                enable_caching=True,
                min_content_length=2000,  # ìµœì†Œ 2000ì ë³´ì¥
                max_retries=3,
                quality_threshold=0.7
            )
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°)
            async for progress in content_pipeline.execute_pipeline_with_progress(
                url=req.url or "",
                text=req.text or "",
                config=config
            ):
                if "error" in progress:
                    yield f"data: {json.dumps({'error': progress['error']})}\n\n"
                    return
                
                # ì§„í–‰ ìƒí™© ì „ì†¡
                yield f"data: {json.dumps(progress)}\n\n"
                
                # ì™„ë£Œ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                if progress.get("step") == 7 and "result" in progress:
                    result = progress["result"]
                    blog_post = result.get("blog_post", {})
                    keywords = result.get("keywords", "")
                    pipeline_id = progress.get("pipeline_id", "")
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    db_post = crud.create_post(
                        db=db,
                        title=blog_post.get("title", "AI ìƒì„± ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸"),
                        content=blog_post.get("content", ""),
                        keywords=keywords,
                        source_url=req.url or "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",
                        ai_mode=req.ai_mode or "ê¸°ë³¸",
                        content_length=len(blog_post.get("content", "")),
                        pipeline_id=pipeline_id
                    )
                    
                    # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
                    final_message = {
                        "step": 7,
                        "message": "ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!",
                        "progress": 100,
                        "pipeline_id": pipeline_id,
                        "result": result,
                        "post_id": db_post.id,
                        "metadata": {
                            "content_length": len(blog_post.get("content", "")),
                            "min_length_achieved": len(blog_post.get("content", "")) >= 2000,
                            "generated_at": datetime.now().isoformat()
                        }
                    }
                    yield f"data: {json.dumps(final_message)}\n\n"
                    break
                    
        except Exception as e:
            logger.error(f"ê²¬ê³ í•œ íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield f"data: {json.dumps({'error': f'íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'})}\n\n"
    
    return StreamingResponse(
        pipeline_progress_generator(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )

@router.post("/generate-post-improved", response_model=PostResponse)
async def generate_post_improved(req: PostRequest, db: Session = Depends(get_db)):
    """ê°œì„ ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (í‚¤ì›Œë“œ ì¤‘ë³µ ë°©ì§€, ê¸¸ì´ ì œì–´, SEO ìµœì í™”)"""
    logger.info("=== ê°œì„ ëœ generate-post ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not req.text and not req.url:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ ë˜ëŠ” URLì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        original_text = ""
        db_source_url = ""
        
        if req.url:
            logger.info(f"URL ì…ë ¥ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: {req.url}")
            db_source_url = req.url
            original_text = await get_text_from_url(req.url)
            if not original_text or not original_text.strip():
                raise HTTPException(status_code=400, detail="URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.info("í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
            db_source_url = "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥"
            original_text = req.text
            if not original_text or not original_text.strip():
                raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # 2. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
        logger.info("ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ë‹¨ê³„ ì‹œì‘")
        korean_chars = len(re.findall(r'[ê°€-í£]', original_text))
        total_chars = len(original_text)
        korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
        
        detected_language = "ko" if korean_ratio > 0.3 else "en"
        logger.info(f"ê°ì§€ëœ ì–¸ì–´: {detected_language}")
        
        if detected_language == "ko":
            translated_text = original_text
        else:
            translated_text = await translate_text(original_text, "KO")
        
        # 3. í‚¤ì›Œë“œ ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ì ìš©
        logger.info("í‚¤ì›Œë“œ ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ì ìš©")
        existing_keywords = keyword_manager.get_existing_keywords(db)
        
        if req.keywords:
            # ì‚¬ìš©ì ì œê³µ í‚¤ì›Œë“œ ê²€ì¦
            user_keywords = [kw.strip() for kw in req.keywords.split(',') if kw.strip()]
            filtered_keywords = keyword_manager.filter_keywords(user_keywords, existing_keywords)
            extracted_keywords = ', '.join(filtered_keywords)
            logger.info(f"ì‚¬ìš©ì í‚¤ì›Œë“œ í•„í„°ë§ ì™„ë£Œ: {extracted_keywords}")
        else:
            # ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ
            extracted_keywords = keyword_manager.extract_unique_keywords(translated_text, existing_keywords)
            logger.info(f"ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {extracted_keywords}")
        
        # 4. AI ì½˜í…ì¸  ìƒì„±
        logger.info("AI ì½˜í…ì¸  ìƒì„± ì‹œì‘")
        content_length = req.content_length or "3000"
        ai_mode = req.ai_mode or "seo"
        
        rule_guidelines = []
        if req.rules:
            rule_guidelines = req.rules.split(',')
        
        result = await generate_ai_post(translated_text, extracted_keywords, rule_guidelines, content_length, ai_mode)
        
        # 5. ì½˜í…ì¸  ê¸¸ì´ ì •í™•í•œ ì œì–´
        logger.info("ì½˜í…ì¸  ê¸¸ì´ ì œì–´ ì ìš©")
        length_report = content_length_controller.generate_length_report(result['post'], content_length)
        
        if not length_report['is_acceptable']:
            logger.info(f"ì½˜í…ì¸  ê¸¸ì´ ì¡°ì • í•„ìš”: {length_report['recommendation']}")
            adjusted_content = content_length_controller.adjust_content_length(result['post'], content_length)
            result['post'] = adjusted_content
            logger.info("ì½˜í…ì¸  ê¸¸ì´ ì¡°ì • ì™„ë£Œ")
        
        # 6. SEO ë¶„ì„ ë° ì ìˆ˜ ê°œì„ 
        logger.info("SEO ë¶„ì„ ì‹¤í–‰")
        keyword_list = [kw.strip() for kw in extracted_keywords.split(',') if kw.strip()]
        seo_result = await seo_analyzer.analyze_content(result['post'], db_source_url, keyword_list)
        
        logger.info(f"SEO ë¶„ì„ ì™„ë£Œ - ì ìˆ˜: {seo_result.overall_score}")
        
        # SEO ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ê°œì„  ì œì•ˆ
        seo_improvements = []
        if seo_result.overall_score < 80:
            seo_improvements = seo_result.recommendations[:5]
            logger.warning(f"SEO ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤: {seo_result.overall_score}")
        
        # 7. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        word_count = content_length_controller.count_words(result['post'])
        
        blog_post = models.BlogPost(
            title=result['title'],
            original_url=db_source_url,
            keywords=extracted_keywords,
            content_html=result['post'],
            meta_description=result.get('meta_description', ''),
            word_count=word_count,
            content_length=content_length
        )
        
        db.add(blog_post)
        db.commit()
        db.refresh(blog_post)
        
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ID: {blog_post.id})")
        
        # 8. ì‘ë‹µ ìƒì„±
        response_data = {
            'id': blog_post.id,
            'title': result['title'],
            'content': result['post'],
            'keywords': extracted_keywords,
            'source_url': db_source_url,
            'created_at': blog_post.created_at.isoformat(),
            'ai_mode': ai_mode,
            'length_report': length_report,
            'seo_score': seo_result.overall_score,
            'seo_improvements': seo_improvements
        }
        
        return PostResponse(
            success=True,
            message="ê°œì„ ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data=response_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê°œì„ ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/keyword-statistics")
async def get_keyword_statistics(db: Session = Depends(get_db)):
    """í‚¤ì›Œë“œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        stats = keyword_manager.get_keyword_statistics(db)
        return {
            "success": True,
            "data": stats
        }
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/test-content-length")
async def test_content_length(data: dict):
    """ì½˜í…ì¸  ê¸¸ì´ ì œì–´ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        content = data.get('content', '')
        target_length = data.get('target_length', '3000')
        
        report = content_length_controller.generate_length_report(content, target_length)
        return {
            "success": True,
            "data": report
        }
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ê¸¸ì´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì½˜í…ì¸  ê¸¸ì´ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/test-seo-analysis")
async def test_seo_analysis(data: dict):
    """SEO ë¶„ì„ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        content = data.get('content', '')
        keywords = data.get('keywords', '')
        
        keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()] if keywords else None
        seo_result = await seo_analyzer.analyze_content(content, "", keyword_list)
        
        return {
            "success": True,
            "data": {
                "overall_score": seo_result.overall_score,
                "content_score": seo_result.content_score,
                "technical_score": seo_result.technical_score,
                "keyword_score": seo_result.keyword_score,
                "readability_score": seo_result.readability_score,
                "recommendations": seo_result.recommendations,
                "issues": seo_result.issues,
                "metrics": seo_result.metrics
            }
        }
    except Exception as e:
        logger.error(f"SEO ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"SEO ë¶„ì„ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì–´ë“œë¯¼ í˜ì´ì§€ìš© API ì—”ë“œí¬ì¸íŠ¸ë“¤
@router.get("/stats/realtime")
async def get_realtime_stats(db: Session = Depends(get_db)):
    """ì‹¤ì‹œê°„ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # ê°„ë‹¨í•œ í†µê³„ ë°˜í™˜ (ì˜¤ë¥˜ ë°©ì§€)
        return {
            "total_posts": 0,
            "today_posts": 0,
            "total_keywords": 0,
            "active_keywords": 0,
            "api_calls_today": 0,
            "success_rate": 95,
            "system_status": "healthy",
            "last_updated": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/stats/dashboard")
async def get_dashboard_stats(db: Session = Depends(get_db)):
    """ëŒ€ì‹œë³´ë“œ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # ê°„ë‹¨í•œ í†µê³„ ë°˜í™˜ (ì˜¤ë¥˜ ë°©ì§€)
        return {
            "total_posts": 0,
            "total_keywords": 0,
            "api_calls_today": 0,
            "success_rate": 95
        }
    except Exception as e:
        logger.error(f"ëŒ€ì‹œë³´ë“œ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ëŒ€ì‹œë³´ë“œ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/posts")
async def get_posts_admin(
    page: int = 1,
    limit: int = 10,
    db: Session = Depends(get_db)
):
    """ì–´ë“œë¯¼ìš© í¬ìŠ¤íŠ¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        skip = (page - 1) * limit
        posts = crud.get_posts(db, skip=skip, limit=limit)
        
        # í¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì–´ë“œë¯¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_posts = []
        for post in posts:
            formatted_posts.append({
                "id": post.id,
                "title": post.title,
                "keywords": post.keywords,
                "category": getattr(post, 'category', 'ê¸°íƒ€'),
                "description": getattr(post, 'description', ''),
                "word_count": post.word_count,
                "status": getattr(post, 'status', 'published'),
                "created_at": post.created_at.isoformat(),
                "original_url": post.original_url
            })
        
        return {
            "posts": formatted_posts,
            "total": len(formatted_posts),
            "page": page,
            "limit": limit
        }
    except Exception as e:
        logger.error(f"í¬ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í¬ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/keywords")
async def get_keywords_admin(
    page: int = 1,
    limit: int = 10,
    db: Session = Depends(get_db)
):
    """ì–´ë“œë¯¼ìš© í‚¤ì›Œë“œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        skip = (page - 1) * limit
        keywords = crud.get_keywords(db, skip=skip, limit=limit)
        
        # í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ì–´ë“œë¯¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_keywords = []
        for keyword in keywords:
            formatted_keywords.append({
                "id": keyword.get("id"),
                "keyword": keyword.get("keyword"),
                "category": keyword.get("category") or keyword.get("type", "ê¸°íƒ€"),
                "description": keyword.get("description", ""),
                "search_volume": keyword.get("search_volume"),
                "competition": keyword.get("competition", "medium"),
                "status": keyword.get("status", "active"),
                "created_at": keyword.get("created_at", datetime.now().isoformat())
            })
        
        return {
            "keywords": formatted_keywords,
            "total": len(formatted_keywords),
            "page": page,
            "limit": limit
        }
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.post("/posts/bulk-delete")
async def bulk_delete_posts_admin(post_ids: dict, db: Session = Depends(get_db)):
    """í¬ìŠ¤íŠ¸ ì¼ê´„ ì‚­ì œ"""
    try:
        ids = post_ids.get("post_ids", [])
        deleted_count = 0
        
        for post_id in ids:
            try:
                crud.delete_post(db, post_id)
                deleted_count += 1
            except Exception as e:
                logger.error(f"í¬ìŠ¤íŠ¸ {post_id} ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        return {
            "success": True,
            "message": f"{deleted_count}ê°œì˜ í¬ìŠ¤íŠ¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": deleted_count
        }
    except Exception as e:
        logger.error(f"ì¼ê´„ ì‚­ì œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì¼ê´„ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.post("/keywords/bulk-delete")
async def bulk_delete_keywords_admin(keyword_ids: dict, db: Session = Depends(get_db)):
    """í‚¤ì›Œë“œ ì¼ê´„ ì‚­ì œ"""
    try:
        ids = keyword_ids.get("keyword_ids", [])
        deleted_count = 0
        
        for keyword_id in ids:
            try:
                crud.delete_keyword(db, keyword_id)
                deleted_count += 1
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ {keyword_id} ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        return {
            "success": True,
            "message": f"{deleted_count}ê°œì˜ í‚¤ì›Œë“œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": deleted_count
        }
    except Exception as e:
        logger.error(f"ì¼ê´„ ì‚­ì œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì¼ê´„ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.put("/posts/bulk-archive")
async def bulk_archive_posts_admin(post_ids: dict, db: Session = Depends(get_db)):
    """í¬ìŠ¤íŠ¸ ì¼ê´„ ë³´ê´€"""
    try:
        ids = post_ids.get("post_ids", [])
        archived_count = 0
        
        for post_id in ids:
            try:
                # í¬ìŠ¤íŠ¸ ìƒíƒœë¥¼ archivedë¡œ ë³€ê²½
                post = crud.get_post(db, post_id)
                if post:
                    post.status = "archived"
                    db.commit()
                    archived_count += 1
            except Exception as e:
                logger.error(f"í¬ìŠ¤íŠ¸ {post_id} ë³´ê´€ ì‹¤íŒ¨: {e}")
        
        return {
            "success": True,
            "message": f"{archived_count}ê°œì˜ í¬ìŠ¤íŠ¸ê°€ ë³´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "archived_count": archived_count
        }
    except Exception as e:
        logger.error(f"ì¼ê´„ ë³´ê´€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì¼ê´„ ë³´ê´€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.put("/keywords/bulk-deactivate")
async def bulk_deactivate_keywords_admin(keyword_ids: dict, db: Session = Depends(get_db)):
    """í‚¤ì›Œë“œ ì¼ê´„ ë¹„í™œì„±í™”"""
    try:
        ids = keyword_ids.get("keyword_ids", [])
        deactivated_count = 0
        
        for keyword_id in ids:
            try:
                # í‚¤ì›Œë“œ ìƒíƒœë¥¼ inactiveë¡œ ë³€ê²½
                keyword = crud.get_keyword(db, keyword_id)
                if keyword:
                    keyword["status"] = "inactive"
                    crud.update_keyword(db, keyword_id, keyword)
                    deactivated_count += 1
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ {keyword_id} ë¹„í™œì„±í™” ì‹¤íŒ¨: {e}")
        
        return {
            "success": True,
            "message": f"{deactivated_count}ê°œì˜ í‚¤ì›Œë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deactivated_count": deactivated_count
        }
    except Exception as e:
        logger.error(f"ì¼ê´„ ë¹„í™œì„±í™” ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì¼ê´„ ë¹„í™œì„±í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/posts/export")
async def export_posts_admin(db: Session = Depends(get_db)):
    """í¬ìŠ¤íŠ¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    try:
        posts = crud.get_posts(db, skip=0, limit=1000)
        
        # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        csv_data = "ID,ì œëª©,í‚¤ì›Œë“œ,ì¹´í…Œê³ ë¦¬,ë‹¨ì–´ìˆ˜,ìƒíƒœ,ìƒì„±ì¼\n"
        for post in posts:
            csv_data += f"{post.id},{post.title},{post.keywords},{getattr(post, 'category', 'ê¸°íƒ€')},{post.word_count},{getattr(post, 'status', 'published')},{post.created_at.strftime('%Y-%m-%d')}\n"
        
        return StreamingResponse(
            io.StringIO(csv_data),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=posts_export_{datetime.now().strftime('%Y%m%d')}.csv"}
        )
    except Exception as e:
        logger.error(f"í¬ìŠ¤íŠ¸ ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í¬ìŠ¤íŠ¸ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/keywords/export")
async def export_keywords_admin(db: Session = Depends(get_db)):
    """í‚¤ì›Œë“œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    try:
        keywords = crud.get_keywords(db, skip=0, limit=1000)
        
        # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        csv_data = "ID,í‚¤ì›Œë“œ,ì¹´í…Œê³ ë¦¬,ê²€ìƒ‰ëŸ‰,ê²½ìŸë„,ìƒíƒœ,ìƒì„±ì¼\n"
        for keyword in keywords:
            csv_data += f"{keyword.get('id', '')},{keyword.get('keyword', '')},{keyword.get('category', 'ê¸°íƒ€')},{keyword.get('search_volume', '')},{keyword.get('competition', 'medium')},{keyword.get('status', 'active')},{keyword.get('created_at', datetime.now().strftime('%Y-%m-%d'))}\n"
        
        return StreamingResponse(
            io.StringIO(csv_data),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=keywords_export_{datetime.now().strftime('%Y%m%d')}.csv"}
        )
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í‚¤ì›Œë“œ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/posts/{post_id}/ai-ethics")
async def get_ai_ethics_evaluation(post_id: int, db: Session = Depends(get_db)):
    """
    íŠ¹ì • í¬ìŠ¤íŠ¸ì˜ AI ìœ¤ë¦¬ í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Args:
        post_id: í¬ìŠ¤íŠ¸ ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    
    Returns:
        AI ìœ¤ë¦¬ í‰ê°€ ê²°ê³¼
    """
    try:
        post = crud.get_post(db, post_id)
        if not post:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"í¬ìŠ¤íŠ¸ ID {post_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        if not post.ai_ethics_evaluation:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ì´ í¬ìŠ¤íŠ¸ì— ëŒ€í•œ AI ìœ¤ë¦¬ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
        
        return JSONResponse({
            "success": True,
            "post_id": post_id,
            "title": post.title,
            "ai_ethics_score": post.ai_ethics_score,
            "evaluation": post.ai_ethics_evaluation,
            "evaluated_at": post.ai_ethics_evaluated_at.isoformat() if post.ai_ethics_evaluated_at else None
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AI ìœ¤ë¦¬ í‰ê°€ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AI ìœ¤ë¦¬ í‰ê°€ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.post("/posts/{post_id}/evaluate-ai-ethics")
async def evaluate_post_ai_ethics(post_id: int, db: Session = Depends(get_db)):
    """
    íŠ¹ì • í¬ìŠ¤íŠ¸ì— ëŒ€í•´ AI ìœ¤ë¦¬ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        post_id: í¬ìŠ¤íŠ¸ ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    
    Returns:
        í‰ê°€ ê²°ê³¼
    """
    try:
        post = crud.get_post(db, post_id)
        if not post:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"í¬ìŠ¤íŠ¸ ID {post_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = {
            'ai_mode': getattr(post, 'ai_mode', None),
            'keywords': post.keywords,
            'created_at': post.created_at.isoformat() if post.created_at else None
        }
        
        # AI ìœ¤ë¦¬ í‰ê°€ ìˆ˜í–‰
        ethics_evaluation = await evaluate_and_save_ai_ethics(
            post,
            post.content_html,
            post.title,
            metadata
        )
        
        if not ethics_evaluation:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="AI ìœ¤ë¦¬ í‰ê°€ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
        
        db.commit()
        db.refresh(post)
        
        return JSONResponse({
            "success": True,
            "message": "AI ìœ¤ë¦¬ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "post_id": post_id,
            "ai_ethics_score": post.ai_ethics_score,
            "evaluation": post.ai_ethics_evaluation,
            "evaluated_at": post.ai_ethics_evaluated_at.isoformat() if post.ai_ethics_evaluated_at else None
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AI ìœ¤ë¦¬ í‰ê°€ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AI ìœ¤ë¦¬ í‰ê°€ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.get("/posts/ai-ethics/stats")
async def get_ai_ethics_stats(db: Session = Depends(get_db)):
    """
    ì „ì²´ í¬ìŠ¤íŠ¸ì˜ AI ìœ¤ë¦¬ í‰ê°€ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Returns:
        AI ìœ¤ë¦¬ í‰ê°€ í†µê³„
    """
    try:
        posts = crud.get_posts(db, skip=0, limit=10000)
        
        evaluated_posts = [p for p in posts if p.ai_ethics_score is not None]
        
        if not evaluated_posts:
            return JSONResponse({
                "success": True,
                "total_posts": len(posts),
                "evaluated_posts": 0,
                "message": "í‰ê°€ëœ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
            })
        
        scores = [p.ai_ethics_score for p in evaluated_posts]
        
        # í†µê³„ ê³„ì‚°
        stats = {
            "total_posts": len(posts),
            "evaluated_posts": len(evaluated_posts),
            "average_score": sum(scores) / len(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "score_distribution": {
                "excellent": len([s for s in scores if s >= 90]),
                "good": len([s for s in scores if 80 <= s < 90]),
                "fair": len([s for s in scores if 70 <= s < 80]),
                "poor": len([s for s in scores if s < 70])
            }
        }
        
        return JSONResponse({
            "success": True,
            "stats": stats
        })
        
    except Exception as e:
        logger.error(f"AI ìœ¤ë¦¬ í‰ê°€ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AI ìœ¤ë¦¬ í‰ê°€ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.post("/posts/generate-from-keyword")
async def generate_post_from_keyword(keyword_data: dict, db: Session = Depends(get_db)):
    """í‚¤ì›Œë“œì—ì„œ í¬ìŠ¤íŠ¸ ìƒì„±"""
    try:
        keyword_id = keyword_data.get("keyword_id")
        keyword = crud.get_keyword(db, keyword_id)
        
        if not keyword:
            raise HTTPException(status_code=404, detail="í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ìŠ¤íŠ¸ ìƒì„± ìš”ì²­
        post_request = PostRequest(
            text=f"{keyword.get('keyword')}ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            rules="ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì‘ì„±",
            ai_mode="gemini-2-flash",
            content_length="3000"
        )
        
        # í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤í–‰
        result = await generate_post_endpoint(post_request, db)
        
        return {
            "success": True,
            "message": "í‚¤ì›Œë“œì—ì„œ í¬ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": result
        }
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œì—ì„œ í¬ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")