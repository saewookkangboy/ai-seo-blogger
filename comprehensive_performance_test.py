#!/usr/bin/env python3
"""
AI SEO Blogger ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import requests
import time
import json
import concurrent.futures
from datetime import datetime
import statistics

def test_endpoint(url, method="GET", timeout=10, name=None):
    """ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
    start_time = time.time()
    try:
        if method.upper() == "GET":
            response = requests.get(url, timeout=timeout)
        elif method.upper() == "POST":
            response = requests.post(url, timeout=timeout)
        
        end_time = time.time()
        response_time = (end_time - start_time) * 1000
        
        return {
            "name": name or url,
            "url": url,
            "method": method,
            "status_code": response.status_code,
            "response_time_ms": round(response_time, 2),
            "success": response.status_code == 200,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        end_time = time.time()
        response_time = (end_time - start_time) * 1000
        
        return {
            "name": name or url,
            "url": url,
            "method": method,
            "status_code": None,
            "response_time_ms": round(response_time, 2),
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def load_test_endpoint(url, method="GET", num_requests=10, concurrent_requests=5, timeout=10, name=None):
    """ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ”¥ {name or url} ë¶€í•˜ í…ŒìŠ¤íŠ¸ ({num_requests} ìš”ì²­, {concurrent_requests} ë™ì‹œ)")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
        futures = []
        for i in range(num_requests):
            future = executor.submit(test_endpoint, url, method, timeout, f"{name or url} #{i+1}")
            futures.append(future)
        
        results = []
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())
    
    return results

def comprehensive_performance_test():
    """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    base_url = "http://localhost:8000"
    
    print("ğŸš€ AI SEO Blogger ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
    basic_endpoints = [
        {"url": f"{base_url}/health", "name": "í—¬ìŠ¤ì²´í¬"},
        {"url": f"{base_url}/", "name": "ë©”ì¸ í˜ì´ì§€"},
        {"url": f"{base_url}/docs", "name": "API ë¬¸ì„œ"},
        {"url": f"{base_url}/admin", "name": "ê´€ë¦¬ì í˜ì´ì§€"},
        {"url": f"{base_url}/api/v1/admin/test-integration", "name": "í†µí•© í…ŒìŠ¤íŠ¸"},
        {"url": f"{base_url}/api/v1/admin/posts/stats", "name": "í¬ìŠ¤íŠ¸ í†µê³„"},
        {"url": f"{base_url}/admin/session-status", "name": "ì„¸ì…˜ ìƒíƒœ"},
        {"url": f"{base_url}/admin/test-session", "name": "í…ŒìŠ¤íŠ¸ ì„¸ì…˜"},
    ]
    
    print("ğŸ“Š ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    basic_results = []
    for endpoint in basic_endpoints:
        result = test_endpoint(endpoint["url"], name=endpoint["name"])
        basic_results.append(result)
        
        status_icon = "âœ…" if result['success'] else "âŒ"
        print(f"   {status_icon} {result['name']}: {result['response_time_ms']:.2f}ms")
        time.sleep(0.2)
    
    # ë¶€í•˜ í…ŒìŠ¤íŠ¸
    print("\nğŸ”¥ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì¤‘...")
    load_test_endpoints = [
        {"url": f"{base_url}/health", "name": "í—¬ìŠ¤ì²´í¬ ë¶€í•˜í…ŒìŠ¤íŠ¸"},
        {"url": f"{base_url}/admin/session-status", "name": "ì„¸ì…˜ ìƒíƒœ ë¶€í•˜í…ŒìŠ¤íŠ¸"},
    ]
    
    load_results = []
    for endpoint in load_test_endpoints:
        results = load_test_endpoint(
            endpoint["url"], 
            num_requests=20, 
            concurrent_requests=5, 
            name=endpoint["name"]
        )
        load_results.extend(results)
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        successful_results = [r for r in results if r['success']]
        if successful_results:
            response_times = [r['response_time_ms'] for r in successful_results]
            avg_time = statistics.mean(response_times)
            min_time = min(response_times)
            max_time = max(response_times)
            std_dev = statistics.stdev(response_times) if len(response_times) > 1 else 0
            
            print(f"   ğŸ“Š {endpoint['name']}:")
            print(f"      í‰ê· : {avg_time:.2f}ms, ìµœì†Œ: {min_time:.2f}ms, ìµœëŒ€: {max_time:.2f}ms")
            print(f"      í‘œì¤€í¸ì°¨: {std_dev:.2f}ms, ì„±ê³µë¥ : {len(successful_results)}/{len(results)}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\nğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
    db_endpoints = [
        {"url": f"{base_url}/api/v1/admin/posts/stats", "name": "í¬ìŠ¤íŠ¸ í†µê³„"},
        {"url": f"{base_url}/api/v1/admin/keywords", "name": "í‚¤ì›Œë“œ ëª©ë¡"},
    ]
    
    db_results = []
    for endpoint in db_endpoints:
        # ì—¬ëŸ¬ ë²ˆ í…ŒìŠ¤íŠ¸í•˜ì—¬ í‰ê·  ê³„ì‚°
        times = []
        for i in range(5):
            result = test_endpoint(endpoint["url"], name=f"{endpoint['name']} #{i+1}")
            if result['success']:
                times.append(result['response_time_ms'])
            time.sleep(0.5)
        
        if times:
            avg_time = statistics.mean(times)
            min_time = min(times)
            max_time = max(times)
            
            db_result = {
                "name": endpoint["name"],
                "avg_response_time_ms": round(avg_time, 2),
                "min_response_time_ms": round(min_time, 2),
                "max_response_time_ms": round(max_time, 2),
                "test_count": len(times)
            }
            db_results.append(db_result)
            
            print(f"   ğŸ“Š {endpoint['name']}: í‰ê·  {avg_time:.2f}ms (ìµœì†Œ: {min_time:.2f}ms, ìµœëŒ€: {max_time:.2f}ms)")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    all_results = basic_results + load_results
    
    successful_tests = [r for r in all_results if r['success']]
    failed_tests = [r for r in all_results if not r['success']]
    
    if successful_tests:
        response_times = [r['response_time_ms'] for r in successful_tests]
        avg_response_time = statistics.mean(response_times)
        min_response_time = min(response_times)
        max_response_time = max(response_times)
        median_response_time = statistics.median(response_times)
        std_dev_response_time = statistics.stdev(response_times) if len(response_times) > 1 else 0
        
        print(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(successful_tests)}/{len(all_results)} ({len(successful_tests)/len(all_results)*100:.1f}%)")
        print(f"ğŸ“Š í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.2f}ms")
        print(f"ğŸ“Š ì¤‘ê°„ê°’ ì‘ë‹µ ì‹œê°„: {median_response_time:.2f}ms")
        print(f"âš¡ ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min_response_time:.2f}ms")
        print(f"ğŸŒ ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max_response_time:.2f}ms")
        print(f"ğŸ“Š í‘œì¤€í¸ì°¨: {std_dev_response_time:.2f}ms")
        
        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        if avg_response_time < 50:
            performance_grade = "ğŸŸ¢ ìš°ìˆ˜"
        elif avg_response_time < 100:
            performance_grade = "ğŸŸ¡ ì–‘í˜¸"
        elif avg_response_time < 200:
            performance_grade = "ğŸŸ  ë³´í†µ"
        else:
            performance_grade = "ğŸ”´ ê°œì„  í•„ìš”"
        
        print(f"ğŸ† ì„±ëŠ¥ ë“±ê¸‰: {performance_grade}")
    
    if failed_tests:
        print(f"âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {len(failed_tests)}/{len(all_results)}")
        for test in failed_tests[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            print(f"   - {test['name']}: {test.get('error', f'HTTP {test.get('status_code', 'Unknown')}')}")
        if len(failed_tests) > 5:
            print(f"   ... ë° {len(failed_tests) - 5}ê°œ ë”")
    
    # ìƒì„¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    comprehensive_results = {
        "test_timestamp": datetime.now().isoformat(),
        "test_summary": {
            "total_tests": len(all_results),
            "successful_tests": len(successful_tests),
            "failed_tests": len(failed_tests),
            "success_rate": len(successful_tests)/len(all_results)*100 if all_results else 0
        },
        "performance_metrics": {
            "avg_response_time_ms": round(statistics.mean([r['response_time_ms'] for r in successful_tests]), 2) if successful_tests else 0,
            "min_response_time_ms": min([r['response_time_ms'] for r in successful_tests]) if successful_tests else 0,
            "max_response_time_ms": max([r['response_time_ms'] for r in successful_tests]) if successful_tests else 0,
            "median_response_time_ms": round(statistics.median([r['response_time_ms'] for r in successful_tests]), 2) if successful_tests else 0,
            "std_dev_response_time_ms": round(statistics.stdev([r['response_time_ms'] for r in successful_tests]), 2) if len(successful_tests) > 1 else 0
        },
        "database_performance": db_results,
        "detailed_results": all_results
    }
    
    with open('comprehensive_performance_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(comprehensive_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ 'comprehensive_performance_test_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return comprehensive_results

if __name__ == "__main__":
    comprehensive_performance_test()
